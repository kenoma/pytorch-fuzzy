{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детектор аномалии на принципе многократной прогонки реконструкции входного образца до сходимости латентного вектора.\n",
    "Критерий аномальности - расстояние от первоначального латентного вектора до сошедшего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "from torchfuzzy import FuzzyLayer, DefuzzyLinearLayer, FuzzyBellLayer\n",
    "import piqa\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 2e-3\n",
    "num_epochs = 300\n",
    "latent_dim = 5\n",
    "mnist_class_anomaly = 4\n",
    "kernels = 2\n",
    "fuzzy_rules_count = 100\n",
    "beta = 1e-3\n",
    "gamma = 1\n",
    "\n",
    "prefix = f\"fuzzy_cvae_mamdani_anomaly\"\n",
    "writer = SummaryWriter(f'runs/mnist/{prefix}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ssim = piqa.SSIM(window_size = 11, n_channels=1, reduction='none').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет\n",
    "\n",
    "1. Исключаем класс аномалии `mnist_class_anomaly` из общей выборк\n",
    "2. Убираем метки с остальных классов\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_and_transform(x):\n",
    "    nimg = 2.0*(x.view(-1, 28, 28) - 0.5)\n",
    "    nimg = torch.clamp(nimg, -1, 1)\n",
    "    return nimg\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.RandomCrop(size=26),\n",
    "    transforms.Resize(size=(28, 28)),\n",
    "    transforms.Lambda(norm_and_transform)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_and_mask(target_label):\n",
    "    t = target_label\n",
    "    return t \n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=True, \n",
    "    transform = transform,\n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "\n",
    "idx = (train_data.targets != mnist_class_anomaly)\n",
    "train_data.targets = train_data.targets[idx]\n",
    "train_data.data = train_data.data[idx]\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "загружаем тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=transform, \n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем итераторы датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    \n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data,_ in iter(train_loader):\n",
    "    plt.imshow(data[0].squeeze())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermAverageActivationStatsLayer(torch.nn.Module):\n",
    "    def __init__(self, terms_count, alpha=1e-2):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.dummy = torch.nn.Parameter(torch.empty(0))\n",
    "        self.terms_count = terms_count\n",
    "        self.alpha = alpha\n",
    "        self.accumulated_average_activation = None\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        ava = x.mean(0)\n",
    "        if self.accumulated_average_activation is None:\n",
    "            self.accumulated_average_activation = ava\n",
    "        else:\n",
    "            self.accumulated_average_activation = (1 - self.alpha) * self.accumulated_average_activation + self.alpha * ava\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_norm_stats(self, eps=1e-7):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        s = self.accumulated_average_activation.sum() + eps\n",
    "        return (self.accumulated_average_activation / s).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermLatentSpaceCentroidTrackingLayer(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, alpha=1e-3):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.dummy = torch.nn.Parameter(torch.empty(0))\n",
    "        self.latent_dim = latent_dim\n",
    "        self.alpha = alpha\n",
    "        self.aver_centroid = None\n",
    "\n",
    "    def forward(self, latent_vectors):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        avc = latent_vectors.mean(0)\n",
    "        if self.aver_centroid is None:\n",
    "            self.aver_centroid = avc\n",
    "        else:\n",
    "            self.aver_centroid = (1 - self.alpha) * self.aver_centroid + self.alpha * avc\n",
    "        return latent_vectors\n",
    "\n",
    "    def get_average_centroid(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.aver_centroid.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Компонент энкодера для VAE\n",
    "    \n",
    "    Args:\n",
    "        latent_dim (int): Размер латентного вектора.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "                \n",
    "        self.input = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels, kernel_size=3, padding=2, stride=1),\n",
    "            nn.BatchNorm2d(kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "        )\n",
    "\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(kernels, 2*kernels, kernel_size=2, stride=2, padding = 2),\n",
    "            nn.BatchNorm2d(2*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(2*kernels, 2*kernels, kernel_size=3, stride=2, padding = 2),\n",
    "            nn.BatchNorm2d(2*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "        )\n",
    "\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv2d(2*kernels, 4*kernels, kernel_size = 3, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(4*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(4*kernels, 4*kernels, kernel_size = 3, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(4*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "        )\n",
    "\n",
    "        self.block_4 = nn.Sequential(\n",
    "            nn.Conv2d(4*kernels, 8*kernels, kernel_size = 4, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(8*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(8*kernels, 8*kernels, kernel_size = 3, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(8*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "        )\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*kernels, 2 * latent_dim), # mean + variance.\n",
    "        )\n",
    "\n",
    "        self.downscale_1 = nn.Conv2d(kernels, 8*kernels, kernel_size=30)\n",
    "        self.downscale_2 = nn.Conv2d(2*kernels, 8*kernels, kernel_size=10)\n",
    "        self.downscale_3 = nn.Conv2d(4*kernels, 8*kernels, kernel_size=6)\n",
    "        self.after_sum = nn.SiLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "         \n",
    "    def forward(self, x, eps: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Выход энкодера для чистого VAE.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Входной вектор.\n",
    "            eps (float): Небольшая поправка к скейлу для лучшей сходимости и устойчивости.\n",
    "        \n",
    "        Returns:\n",
    "            mu, logvar, z, dist\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.input(x)\n",
    "        #print(x.shape)\n",
    "        res_1 = self.downscale_1(x)\n",
    "        \n",
    "\n",
    "        x = self.block_2(x)\n",
    "        #print(x.shape)\n",
    "        res_2 = self.downscale_2(x)\n",
    "        \n",
    "        x = self.block_3(x)\n",
    "        #print(x.shape)\n",
    "        res_3 = self.downscale_3(x)\n",
    "        \n",
    "        x = self.block_4(x)\n",
    "\n",
    "        x = self.after_sum(x + res_1 + res_2 + res_3)\n",
    "        x = self.out(x)\n",
    "\n",
    "        mu, logvar = torch.chunk(x, 2, dim=-1)\n",
    "        scale = self.softplus(logvar) + eps\n",
    "        scale_tril = torch.diag_embed(scale)\n",
    "        dist = torch.distributions.MultivariateNormal(mu, scale_tril=scale_tril)\n",
    "        z = dist.rsample()\n",
    "        \n",
    "        return mu, logvar, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(10, 1, 28, 28)\n",
    "m = Encoder(latent_dim)\n",
    "mu = m.forward(inp)\n",
    "mu[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Компонент декодера для VAE\n",
    "    \n",
    "    Args:\n",
    "        latent_dim (int): Размер латентного вектора.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, fuzzy_rules_count):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        initial_centroids = 2 * (0.5 - np.random.rand(fuzzy_rules_count, latent_dim))\n",
    "        initial_scales = np.ones((fuzzy_rules_count, latent_dim))       \n",
    "        self.fuzzy = nn.Sequential(\n",
    "            TermLatentSpaceCentroidTrackingLayer(latent_dim),\n",
    "            FuzzyLayer.from_centers_and_scales(initial_centroids, initial_scales, trainable=True),\n",
    "            TermAverageActivationStatsLayer(fuzzy_rules_count)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(fuzzy_rules_count, 16*kernels),\n",
    "            nn.SiLU(), \n",
    "            nn.BatchNorm1d(16*kernels, track_running_stats=False),\n",
    "            nn.Unflatten(1, (16*kernels, 1, 1)),\n",
    "            nn.ConvTranspose2d(16*kernels, 8*kernels, 12),\n",
    "            nn.SiLU(), \n",
    "            nn.BatchNorm2d(8*kernels, track_running_stats=False),\n",
    "            nn.ConvTranspose2d(8*kernels, 4*kernels, 5),\n",
    "            nn.SiLU(), \n",
    "            nn.BatchNorm2d(4*kernels, track_running_stats=False),\n",
    "            nn.ConvTranspose2d(4*kernels, 2*kernels, 5),\n",
    "            nn.SiLU(), \n",
    "            nn.BatchNorm2d(2*kernels, track_running_stats=False),\n",
    "            nn.ConvTranspose2d(2*kernels, kernels, 5),\n",
    "            nn.SiLU(), \n",
    "            nn.BatchNorm2d(kernels, track_running_stats=False),\n",
    "            nn.ConvTranspose2d(kernels, 1, 5),\n",
    "            nn.Tanh() \n",
    "        )\n",
    "         \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Декодирует латентный вектор в исходное представление\n",
    "        \n",
    "        Args:\n",
    "            z (torch.Tensor): Латентный вектор.\n",
    "        \n",
    "        Returns:\n",
    "            x\n",
    "        \"\"\"\n",
    "        fz = self.fuzzy(z)\n",
    "        x = self.decoder(fz)\n",
    "        return x, fz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(10, latent_dim)\n",
    "m = Decoder(latent_dim, fuzzy_rules_count=fuzzy_rules_count)\n",
    "mu, fz = m.forward(inp)\n",
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        latent_dim (int): Размер латентного вектора.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, fuzzy_rules_count):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(latent_dim)        \n",
    "        self.decoder = Decoder(latent_dim, fuzzy_rules_count)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        mu, _,  _, = self.encoder(x)\n",
    "        x_recon, fz = self.decoder(mu)\n",
    "        return mu, x_recon, fz\n",
    "    \n",
    "    def half_pass(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        mu, logvar, z = self.encoder(x)\n",
    "        return mu, logvar, z\n",
    "    \n",
    "    def decoder_pass(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def fuzzy_pass(self, x):\n",
    "        return self.decoder.fuzzy(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_dim=latent_dim, fuzzy_rules_count=fuzzy_rules_count).to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of parameters: {num_params:,}')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer_d = torch.optim.Adam(model.decoder.fuzzy[1].parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fz = torch.rand(batch_size, latent_dim)\n",
    "max_entropy = np.log(1.0/latent_dim)\n",
    "p = nn.functional.normalize(fz.sum(0), p=1, dim=0) + 1e-7\n",
    "(p*p.log()).sum() - max_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vae_loss(x, recon_x, mu, logvar):\n",
    "    \n",
    "    diff = ssim((x + 1)/2, (recon_x+1)/2)\n",
    "    \n",
    "    loss_recon = (1 - diff).abs().mean() #F.binary_cross_entropy((recon_x+1)/2, (x + 1)/2, reduction='none').sum(-1).mean()#\n",
    "    \n",
    "    tsquare = torch.square(mu)\n",
    "    tlogvar = torch.exp(logvar)\n",
    "    kl_loss = -0.5 * (1 + logvar - tsquare - tlogvar)\n",
    "    loss_kl = kl_loss.sum(-1).mean()\n",
    "    \n",
    "    loss = loss_recon + beta * loss_kl\n",
    "\n",
    "    return loss, loss_recon, loss_kl\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.rand(10, latent_dim).to(device)\n",
    "# eigensum = (100 - fuzzy_layer.get_transformation_matrix_eigenvals().real.mean(-1)).square()\n",
    "# (model.decoder_pass(inp)[1]* eigensum).sum(-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids_loss(fuzzy_layer):\n",
    "    \n",
    "    c = fuzzy_layer[1].get_centroids()\n",
    "    c = c.mean(0)\n",
    "    ac = fuzzy_layer[0].get_average_centroid()\n",
    "\n",
    "    return (c-ac).square().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fz_inference = torch.rand(3, fuzzy_rules_count)\n",
    "# winners = fz_inference.argmax(-1)\n",
    "eigens = model.decoder.fuzzy[1].get_transformation_matrix_eigenvals().real\n",
    "\n",
    "\n",
    "# eigens[winners]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fuzzy_loss(fz_inference, fuzzy_layer):\n",
    "    \n",
    "    fz_loss = (0.8 - torch.topk(fz_inference, 3).values.sum(-1).clamp(max=0.8)).square().mean() #fz_inference.min(-1).values.square().mean()  #(1 - fz.sum(-1)).square() + (1 - fz.max(-1).values).square() .mean() #fz.quantile(0.5, dim=-1).square().mean() # + (fz.min(-1).values).square().mean()\n",
    "    \n",
    "    eigens = fuzzy_layer[1].get_transformation_matrix_eigenvals().real\n",
    "\n",
    "    #winners = fz_inference.argmax(-1)\n",
    "    fz_volume = (0.8 - (eigens.min(-1).values/eigens.max(-1).values).clamp(max=0.8)).square()\n",
    "    fz_volume = fz_volume.mean()\n",
    "    return fz_loss, fz_volume\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_eigenvals_positive_loss(layer, eps = 1e-15):\n",
    "    ev = layer[1].get_transformation_matrix_eigenvals().real.min()\n",
    "    ev = torch.clamp(ev, max=eps)\n",
    "    return -ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arate(inp):\n",
    "    _, _, fz = model.forward(inp)\n",
    "    # prev_mu = mu\n",
    "    # sum = torch.zeros_like(mu)\n",
    "    \n",
    "    # for i in range(20):\n",
    "    #     recon_x = model.decoder_pass(mu)\n",
    "    #     mu, _, _ = model.half_pass(recon_x)\n",
    "    #     sum += (mu - prev_mu).abs()\n",
    "    #     prev_mu = mu\n",
    "    # +\n",
    "    return fz.sum(-1).cpu().numpy()#ssim((inp + 1)/2, (recon_x+1)/2).cpu().numpy() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer_e, prev_updates, writer=None):\n",
    "    model.train()  \n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(tqdm(dataloader, disable=True)):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer_e.zero_grad()  \n",
    "        \n",
    "        mu, logvar, z = model.half_pass(data)  \n",
    "        recon_x, fz = model.decoder_pass(z)\n",
    "        loss, _, _ = compute_vae_loss(data, recon_x, mu, logvar)\n",
    "        fz_loss, fz_volume = compute_fuzzy_loss(fz, model.decoder.fuzzy)                \n",
    "        ev_loss = keep_eigenvals_positive_loss(model.decoder.fuzzy)\n",
    "        #c_loss = compute_centroids_loss(model.decoder.fuzzy)\n",
    "\n",
    "        if ev_loss.item() > 0:\n",
    "            ev_loss.backward(retain_graph=True)\n",
    "        fz_loss.backward(retain_graph=True)\n",
    "        fz_volume.backward(retain_graph=True)\n",
    "        #c_loss.backward(retain_graph=True)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_e.step()  \n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        \n",
    "        \n",
    "    return prev_updates + len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(fuzzy):\n",
    "    distr = fuzzy[2].get_norm_stats()\n",
    "    dim = distr.shape[0]\n",
    "    return np.log(dim) + (distr*distr.log()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_random_z = torch.randn(16, latent_dim).to(device)\n",
    "\n",
    "def test(model, dataloader, cur_step, epoch, writer=None):\n",
    "    model.eval() \n",
    "    test_recon_loss = 0\n",
    "    test_kl_loss = 0\n",
    "    test_fz_loss = 0\n",
    "    test_fzvol_loss = 0\n",
    "    test_c_loss = 0\n",
    "    \n",
    "    lab_true = []\n",
    "    lab_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, lab in tqdm(test_loader, desc='Test MNIST', disable=True):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            rates = get_arate(data)\n",
    "            \n",
    "            for f, l in  zip(rates, lab):\n",
    "                lab_pred.append(f)        \n",
    "                if l == mnist_class_anomaly:\n",
    "                    lab_true.append(1)\n",
    "                else:\n",
    "                    lab_true.append(0)\n",
    "                        \n",
    "    fpr, tpr, _ = metrics.roc_curve(lab_true, lab_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    embedings = []\n",
    "    labels_expected = []\n",
    "    centroids = model.decoder.fuzzy[1].get_centroids().detach().cpu().numpy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc='Testing', disable=True):\n",
    "            data = data.to(device)\n",
    "            mu, logvar, z = model.half_pass(data)  \n",
    "            recon_x, fz = model.decoder_pass(z)\n",
    "            \n",
    "            embedings.append(mu.cpu().numpy())\n",
    "            labels_expected.append(target.cpu().numpy())\n",
    "\n",
    "            _, loss_recon, loss_kl = compute_vae_loss(data, recon_x, mu, logvar)\n",
    "            fz_loss, fz_volume = compute_fuzzy_loss(fz, model.decoder.fuzzy)    \n",
    "            c_loss = compute_centroids_loss(model.decoder.fuzzy)    \n",
    "                    \n",
    "            test_recon_loss += loss_recon.item()\n",
    "            test_kl_loss += loss_kl.item()\n",
    "            test_fz_loss += fz_loss.item()\n",
    "            test_fzvol_loss += fz_volume.item()\n",
    "            test_c_loss += c_loss.item()\n",
    "\n",
    "    embedings = np.concatenate(embedings, axis=0)\n",
    "    labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "\n",
    "    if epoch % 20 == 1:\n",
    "        plt.figure(figsize=(18, 6))\n",
    "\n",
    "        R, C = 1, 3\n",
    "\n",
    "        plt.subplot(R, C, 1)\n",
    "        plt.title(\"MNIST XY\")\n",
    "        plt.scatter(embedings[:, 0],      embedings[:,  1], c=labels_expected, cmap='tab10', s=2)\n",
    "        plt.scatter(centroids[:, 0],      centroids[:, 1], marker='1', c='black', s= 50)\n",
    "        # plt.xlim((-1, 1))\n",
    "        # plt.ylim((-1, 1))\n",
    "\n",
    "        plt.subplot(R, C, 2)\n",
    "        plt.title(\"MNIST XZ\")\n",
    "        plt.scatter(embedings[:, 0],      embedings[:,  2], c=labels_expected, cmap='tab10', s=2)\n",
    "        plt.scatter(centroids[:, 0],      centroids[:, 2], marker='1', c='black', s= 50)\n",
    "        # plt.xlim((-1, 1))\n",
    "        # plt.ylim((-1, 1))\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    test_recon_loss /= len(dataloader)\n",
    "    test_kl_loss /= len(dataloader)\n",
    "    test_fz_loss /= len(dataloader)\n",
    "    test_fzvol_loss /= len(dataloader)\n",
    "    test_c_loss /= len(dataloader)\n",
    "    print(f'[{cur_step}] Reconstruction loss: {test_recon_loss:.4f}, KLD: {test_kl_loss:.4f} AUC {roc_auc:.4f} FZ {test_fz_loss:.4f} FZVOL {test_fzvol_loss:.4f} CENTROIDS DISCREPANCY {test_c_loss:.4f} SHANNON {shannon_entropy(model.decoder.fuzzy)}')\n",
    "    #print(f'Average activation stats: {model.decoder.fuzzy[2].get_norm_stats()}')\n",
    "    #print(f'Average centroid stats: {model.decoder.fuzzy[0].get_average_centroid()}')\n",
    "    if writer is not None:\n",
    "        \n",
    "        writer.add_scalar('ADFVAE/AUC', roc_auc, global_step=cur_step)\n",
    "        writer.add_scalar('ADFVAE/Reconstruction', test_recon_loss, global_step=cur_step)\n",
    "        writer.add_scalar('ADFVAE/KLD', test_kl_loss, global_step=cur_step)\n",
    "        writer.add_scalar('ADFVAE/Fuzzy', test_fz_loss, global_step=cur_step)\n",
    "        writer.add_scalar('ADFVAE/Fuzzy/Vol', test_fzvol_loss, global_step=cur_step)\n",
    "        writer.add_scalar('ADFVAE/Fuzzy/CDist', test_c_loss, global_step=cur_step)\n",
    "        \n",
    "        samples, _ = model.decoder_pass(fixed_random_z)\n",
    "        writer.add_images('ADFVAE/Samples', samples.view(-1, 1, 28, 28), global_step=cur_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_updates = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    prev_updates = train(model, train_loader, optimizer, prev_updates, writer=writer)\n",
    "    test(model, test_loader, prev_updates, epoch, writer=writer)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_eigenvals_positive_loss(model.decoder.fuzzy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализируем результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_activation_stats(model, dataloader):\n",
    "    rulestat = {}\n",
    "    with torch.no_grad():\n",
    "        for _, (data, _) in enumerate(tqdm(dataloader)):\n",
    "            data = data.to(device)\n",
    "            _, _, fz = model.forward(data)\n",
    "            act_fz = fz.max(-1).indices.cpu().numpy()\n",
    "            for ind in act_fz:\n",
    "                rulestat[ind] = rulestat.get(ind, 0) + 1\n",
    "    return rulestat\n",
    "\n",
    "train_stat = get_activation_stats(model, train_loader)\n",
    "test_stat = get_activation_stats(model, test_loader)\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.bar(list(train_stat.keys()), train_stat.values(), 0.5, color='g')\n",
    "plt.bar(list(test_stat.keys()), test_stat.values(), 0.5, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_stats_by_digit(digit, model, dataloader):\n",
    "    rulestat = {}\n",
    "    with torch.no_grad():\n",
    "        for _, (data, target) in enumerate(tqdm(dataloader)):\n",
    "            data = data.to(device)\n",
    "            _, _, fz = model.forward(data)\n",
    "            act_fz = fz.max(-1).indices.cpu().numpy()\n",
    "            for ind, lab in zip(act_fz, target.cpu().numpy()):\n",
    "                if lab == digit:\n",
    "                    rulestat[ind] = rulestat.get(ind, 0) + 1\n",
    "    return rulestat\n",
    "\n",
    "\n",
    "test_stat_by_digit_0 = get_activation_stats_by_digit(0, model, test_loader)\n",
    "test_stat_by_digit_1 = get_activation_stats_by_digit(1, model, test_loader)\n",
    "test_stat_by_digit_2 = get_activation_stats_by_digit(2, model, test_loader)\n",
    "test_stat_by_digit_3 = get_activation_stats_by_digit(3, model, test_loader)\n",
    "test_stat_by_digit_4 = get_activation_stats_by_digit(4, model, test_loader)\n",
    "test_stat_by_digit_5 = get_activation_stats_by_digit(5, model, test_loader)\n",
    "test_stat_by_digit_6 = get_activation_stats_by_digit(6, model, test_loader)\n",
    "test_stat_by_digit_7 = get_activation_stats_by_digit(7, model, test_loader)\n",
    "test_stat_by_digit_8 = get_activation_stats_by_digit(8, model, test_loader)\n",
    "test_stat_by_digit_9 = get_activation_stats_by_digit(9, model, test_loader)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.bar(list(test_stat_by_digit_0.keys()), test_stat_by_digit_0.values(), 1)\n",
    "plt.bar(list(test_stat_by_digit_1.keys()), test_stat_by_digit_1.values(), 1)\n",
    "plt.bar(list(test_stat_by_digit_2.keys()), test_stat_by_digit_2.values(), 1)\n",
    "plt.bar(list(test_stat_by_digit_3.keys()), test_stat_by_digit_3.values(), 1)\n",
    "plt.bar(list(test_stat_by_digit_4.keys()), test_stat_by_digit_4.values(), 1)\n",
    "plt.bar(list(test_stat_by_digit_5.keys()), test_stat_by_digit_5.values(), 1)\n",
    "plt.bar(list(test_stat_by_digit_6.keys()), test_stat_by_digit_6.values(), 1)\n",
    "plt.bar(list(test_stat_by_digit_7.keys()), test_stat_by_digit_7.values(), 1)\n",
    "plt.bar(list(test_stat_by_digit_8.keys()), test_stat_by_digit_8.values(), 1)\n",
    "plt.bar(list(test_stat_by_digit_9.keys()), test_stat_by_digit_9.values(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fz_activations(model, dataloader):\n",
    "    embedding = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for _, (data, target) in enumerate(tqdm(dataloader)):\n",
    "            data = data.to(device)\n",
    "            _, _, fz = model.forward(data)\n",
    "            fz_inf = fz.cpu().numpy()\n",
    "            for f,l in zip(fz_inf, target.cpu().numpy()):\n",
    "                embedding.append(f)\n",
    "                labels.append(l)\n",
    "                \n",
    "    return embedding, labels\n",
    "\n",
    "train_fz = get_fz_activations(model, train_loader)\n",
    "test_fz = get_fz_activations(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "X = train_fz[0]\n",
    "\n",
    "clf = IsolationForest(random_state=0,n_estimators=1000).fit(X)\n",
    "\n",
    "clf.predict(test_fz[0]).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(64, latent_dim).to(device)\n",
    "samples,_ = model.decoder_pass(z)\n",
    "\n",
    "# Plot the generated images\n",
    "fig, ax = plt.subplots(8, 8, figsize=(8, 8))\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        ax[i, j].imshow(samples[i*8+j].view(28, 28).cpu().detach().numpy(), cmap='gray')\n",
    "        ax[i, j].axis('off')\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig('cvae_mnist.webp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arate(inp):\n",
    "    _, _, fz = model.forward(inp)\n",
    "    # +\n",
    "    return (1-fz.topk(k=1).values.min(-1).values).cpu().numpy() #(fz.quantile(0.9, dim=-1)).cpu().numpy()#ssim((inp + 1)/2, (recon_x+1)/2).cpu().numpy() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firings_mnist = {}\n",
    "firings_mnist['MNIST'] = []\n",
    "firings_mnist['DISSIDENT'] = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader, desc='MNIST HIST'):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        rates = get_arate(data)\n",
    "        for f, l in  zip(rates, target):\n",
    "            if l != mnist_class_anomaly:\n",
    "                firings_mnist['MNIST'].append(f)\n",
    "            else:\n",
    "                firings_mnist['DISSIDENT'].append(f)\n",
    "        \n",
    "\n",
    "labels, data = firings_mnist.keys(), firings_mnist.values()\n",
    "\n",
    "fig = plt.figure(figsize =(12, 2))\n",
    "plt.boxplot(data, notch=True, showfliers=False)\n",
    "plt.xticks(range(1, len(labels) + 1), labels)\n",
    "plt.show()\n",
    "\n",
    "writer.add_figure('Anomaly Detection', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    firing_levels = []\n",
    "    lab_true = []\n",
    "    lab_pred = []\n",
    "\n",
    "    for data, lab in tqdm(test_loader, desc='Test MNIST', disable=True):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        rates = get_arate(data)\n",
    "        \n",
    "        for f, l in  zip(rates, lab):\n",
    "            firing_levels.append(f)\n",
    "            lab_pred.append(f)        \n",
    "            if l == mnist_class_anomaly:\n",
    "                lab_true.append(1)\n",
    "            else:\n",
    "                lab_true.append(0)\n",
    "                    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(lab_true, lab_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    fig = plt.figure(figsize =(4, 4))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    writer.add_figure('ROC', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot():\n",
    "    centroids = model.decoder.fuzzy[1].get_centroids().detach().cpu().numpy()\n",
    "    embedings = []\n",
    "    labels_expected = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc='Encoding'):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            embeding,_,_ = model.forward(data)\n",
    "            embedings.append(embeding.cpu().numpy())\n",
    "            labels_expected.append(target.cpu().numpy())\n",
    "    embedings = np.concatenate(embedings, axis=0)\n",
    "    labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    R, C = 1, 3\n",
    "\n",
    "    plt.subplot(R, C, 1)\n",
    "    plt.title(\"MNIST XY\")\n",
    "    plt.scatter(embedings[:, 0],      embedings[:,  1], c=labels_expected, cmap='tab10', s=2)\n",
    "    plt.scatter(centroids[:, 0],      centroids[:, 1], marker='1', c='black', s= 50)\n",
    "\n",
    "    plt.subplot(R, C, 2)\n",
    "    plt.title(\"MNIST XZ\")\n",
    "    plt.scatter(embedings[:, 0],      embedings[:,  2], c=labels_expected, cmap='tab10', s=2)\n",
    "    plt.scatter(centroids[:, 0],      centroids[:, 2], marker='1', c='black', s= 50)\n",
    "\n",
    "    \n",
    "show_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
