{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детектор аномалии на принципе многократной прогонки реконструкции входного образца до сходимости латентного вектора.\n",
    "Критерий аномальности - расстояние от первоначального латентного вектора до сошедшего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "from torchfuzzy import FuzzyLayer, DefuzzyLinearLayer, FuzzyBellLayer\n",
    "import piqa\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 2e-3\n",
    "num_epochs = 25\n",
    "latent_dim = 7\n",
    "mnist_class_anomaly = 4\n",
    "kernels = 16\n",
    "fuzzy_rules_count = 8\n",
    "\n",
    "prefix = f\"fuzzy_cvae_mamdani_anomaly\"\n",
    "writer = SummaryWriter(f'runs/mnist/{prefix}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ssim = piqa.SSIM(window_size = 11, n_channels=1, reduction='none').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет\n",
    "\n",
    "1. Исключаем класс аномалии `mnist_class_anomaly` из общей выборк\n",
    "2. Убираем метки с остальных классов\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_and_transform(x):\n",
    "    nimg = 2.0*(x.view(-1, 28, 28) - 0.5)\n",
    "    nimg = torch.clamp(nimg, -1, 1)\n",
    "    return nimg\n",
    "\n",
    "def clamp(x):\n",
    "    #nimg = 2.0*(x.view(-1, 28, 28) - 0.5)\n",
    "    nimg = torch.clamp(x, -1, 1)\n",
    "    return nimg\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(norm_and_transform)\n",
    "])\n",
    "\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomRotation(15, fill=-1), \n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), fill=-1), \n",
    "    #transforms.RandomCrop(size=26),\n",
    "    #transforms.Resize(size=(28, 28)),\n",
    "    transforms.Lambda(clamp)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54158"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target_and_mask(target_label):\n",
    "    t = target_label\n",
    "    return t \n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=True, \n",
    "    transform = transform,\n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "\n",
    "idx = (train_data.targets != mnist_class_anomaly)\n",
    "train_data.targets = train_data.targets[idx]\n",
    "train_data.data = train_data.data[idx]\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "загружаем тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=transform, \n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем итераторы датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    \n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data,_ in iter(train_loader):\n",
    "    R, C = 1, 2\n",
    "    plt.subplot(R, C, 1)\n",
    "    plt.imshow(data[0].squeeze())\n",
    "    plt.subplot(R, C, 2)\n",
    "    plt.imshow(augmentation(data)[0].squeeze())\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermAverageActivationStatsLayer(torch.nn.Module):\n",
    "    def __init__(self, terms_count, alpha=1e-2):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.dummy = torch.nn.Parameter(torch.empty(0))\n",
    "        self.terms_count = terms_count\n",
    "        self.alpha = alpha\n",
    "        self.accumulated_average_activation = None\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        ava = x.mean(0)\n",
    "        if self.accumulated_average_activation is None:\n",
    "            self.accumulated_average_activation = ava\n",
    "        else:\n",
    "            self.accumulated_average_activation = (1 - self.alpha) * self.accumulated_average_activation + self.alpha * ava\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_norm_stats(self, eps=1e-7):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        s = self.accumulated_average_activation.sum() + eps\n",
    "        return (self.accumulated_average_activation / s).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermLatentSpaceCentroidTrackingLayer(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, alpha=1e-3):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        #self.dummy = torch.nn.Parameter(torch.empty(0))\n",
    "        self.latent_dim = latent_dim\n",
    "        self.alpha = alpha\n",
    "        self.aver_centroid = None\n",
    "\n",
    "    def forward(self, latent_vectors):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        avc = latent_vectors.mean(0)\n",
    "        if self.aver_centroid is None:\n",
    "            self.aver_centroid = avc\n",
    "        else:\n",
    "            self.aver_centroid = (1 - self.alpha) * self.aver_centroid + self.alpha * avc\n",
    "        return latent_vectors\n",
    "\n",
    "    def get_average_centroid(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.aver_centroid.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.SiLU()]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Компонент энкодера для VAE\n",
    "    \n",
    "    Args:\n",
    "        latent_dim (int): Размер латентного вектора.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, kernels):\n",
    "        super(Encoder, self).__init__()\n",
    "                \n",
    "        self.conv1 = conv_block(1, 2*kernels)\n",
    "        self.conv2 = conv_block(2*kernels, 4*kernels, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(4*kernels, 4*kernels), conv_block(4*kernels, 4*kernels))\n",
    "        \n",
    "        self.conv3 = conv_block(4*kernels, 8*kernels, pool=True)\n",
    "        self.conv4 = conv_block(8*kernels, 16*kernels, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(16*kernels, 16*kernels), conv_block(16*kernels, 16*kernels))\n",
    "\n",
    "        #self.after_sum = nn.SiLU()\n",
    "        #self.softplus = nn.Softplus()\n",
    "        self.latent = nn.Sequential(\n",
    "            nn.Conv2d(16*kernels, latent_dim, 3),\n",
    "            nn.BatchNorm2d(latent_dim, track_running_stats=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Flatten(),\n",
    "            #nn.Linear(16*kernels*9, latent_dim), # mean + variance.\n",
    "        )\n",
    "\n",
    "         \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Выход энкодера для чистого VAE.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Входной вектор.\n",
    "            eps (float): Небольшая поправка к скейлу для лучшей сходимости и устойчивости.\n",
    "        \n",
    "        Returns:\n",
    "            mu, logvar, z, dist\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "\n",
    "        out = self.latent(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.rand(10, 1, 28, 28)\n",
    "m = Encoder(latent_dim, 2)\n",
    "mu = m.forward(inp)\n",
    "mu[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Компонент декодера для VAE\n",
    "    \n",
    "    Args:\n",
    "        latent_dim (int): Размер латентного вектора.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, fuzzy_rules_count, kernels):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        scale = 2\n",
    "        initial_centroids = 2 * scale * (0.5 - np.random.rand(fuzzy_rules_count, latent_dim))\n",
    "        initial_scales = scale * np.ones((fuzzy_rules_count, latent_dim))\n",
    "        self.fuzzy = nn.Sequential(\n",
    "            TermLatentSpaceCentroidTrackingLayer(latent_dim),\n",
    "            FuzzyLayer.from_centers_and_scales(initial_centroids, initial_scales, trainable=True),\n",
    "            TermAverageActivationStatsLayer(fuzzy_rules_count),\n",
    "            #nn.BatchNorm1d(fuzzy_rules_count, track_running_stats=False),\n",
    "            #nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.input = nn.Sequential(\n",
    "            nn.Unflatten(1, (fuzzy_rules_count, 1, 1)),\n",
    "            nn.BatchNorm2d(fuzzy_rules_count, track_running_stats=True),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(fuzzy_rules_count, 16*kernels, 4),\n",
    "            nn.BatchNorm2d(16*kernels, track_running_stats=True),\n",
    "            nn.SiLU(), \n",
    "            nn.ConvTranspose2d(16*kernels, 16*kernels, 4),\n",
    "            nn.BatchNorm2d(16*kernels, track_running_stats=True),\n",
    "            nn.SiLU(), \n",
    "        )\n",
    "\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16*kernels, 8*kernels, 4),\n",
    "            nn.BatchNorm2d(8*kernels, track_running_stats=True),\n",
    "            nn.SiLU(), \n",
    "            nn.ConvTranspose2d(8*kernels, 8*kernels, 4),\n",
    "            nn.BatchNorm2d(8*kernels, track_running_stats=True),\n",
    "            nn.SiLU(), \n",
    "        )\n",
    "\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8*kernels, 4*kernels, 4),\n",
    "            nn.BatchNorm2d(4*kernels, track_running_stats=True),\n",
    "            nn.SiLU(), \n",
    "            nn.ConvTranspose2d(4*kernels, 4*kernels, 4),\n",
    "            nn.BatchNorm2d(4*kernels, track_running_stats=True),\n",
    "            nn.SiLU(), \n",
    "        )\n",
    "\n",
    "        self.block_4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4*kernels, 2*kernels, 4),\n",
    "            nn.BatchNorm2d(2*kernels, track_running_stats=True),\n",
    "            nn.SiLU(), \n",
    "            nn.ConvTranspose2d(2*kernels, 2*kernels, 4),\n",
    "            nn.BatchNorm2d(2*kernels, track_running_stats=True),\n",
    "            nn.SiLU(), \n",
    "        )\n",
    "\n",
    "        self.block_5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*kernels, kernels, 2),\n",
    "            nn.BatchNorm2d(kernels, track_running_stats=True),\n",
    "            nn.SiLU(), \n",
    "            nn.ConvTranspose2d(kernels, kernels, 2),\n",
    "            nn.BatchNorm2d(kernels, track_running_stats=True),\n",
    "            nn.SiLU(), \n",
    "        )\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.ConvTranspose2d(kernels, 1, 2),\n",
    "            nn.Tanh() \n",
    "        )\n",
    "        self.upscale_1 = nn.Sequential(nn.ConvTranspose2d(fuzzy_rules_count, 16*kernels, kernel_size=7))\n",
    "        self.upscale_2 = nn.Sequential(nn.ConvTranspose2d(16*kernels, 8*kernels, kernel_size=7))\n",
    "        self.upscale_3 = nn.Sequential(nn.ConvTranspose2d( 8*kernels, 4*kernels, kernel_size=7))\n",
    "        self.upscale_4 = nn.Sequential(nn.ConvTranspose2d( 4*kernels, 2*kernels, kernel_size=7))\n",
    "        self.upscale_5 = nn.Sequential(nn.ConvTranspose2d( 2*kernels, kernels,   kernel_size=3))\n",
    "        self.after_sum = nn.SiLU()\n",
    "         \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Декодирует латентный вектор в исходное представление\n",
    "        \n",
    "        Args:\n",
    "            z (torch.Tensor): Латентный вектор.\n",
    "        \n",
    "        Returns:\n",
    "            x\n",
    "        \"\"\"\n",
    "        fz = self.fuzzy(z)\n",
    "        mapx = self.input(fz)\n",
    "\n",
    "        x = self.decode_from_mp(mapx)\n",
    "        return x, fz, mapx\n",
    "    \n",
    "    def decode_from_fz(self, fz):\n",
    "        mapx = self.input(fz)\n",
    "        x = self.decode_from_mp(mapx)\n",
    "        return x\n",
    "\n",
    "    def decode_from_mp(self, x):\n",
    "        res = self.upscale_1(x)\n",
    "        #print(res.shape)\n",
    "        x = self.block_1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.after_sum(x+res)\n",
    "\n",
    "        res = self.upscale_2(x)\n",
    "        #print(res.shape)\n",
    "        x = self.block_2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.after_sum(x+res)\n",
    "\n",
    "        res = self.upscale_3(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.after_sum(x+res)\n",
    "\n",
    "        res = self.upscale_4(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.after_sum(x+res)\n",
    "\n",
    "        res = self.upscale_5(x)\n",
    "        x = self.block_5(x)\n",
    "        x = self.after_sum(x+res)\n",
    "        \n",
    "        return self.output(x)\n",
    "    \n",
    "inp = torch.rand(10, latent_dim)\n",
    "m = Decoder(latent_dim, fuzzy_rules_count, 8)\n",
    "mu = m.forward(inp)\n",
    "mu[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        latent_dim (int): Размер латентного вектора.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, fuzzy_rules_count, kernels):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(latent_dim, kernels)        \n",
    "        self.decoder = Decoder(latent_dim, fuzzy_rules_count, 2*kernels)\n",
    "        #self.resd = Decoder(latent_dim, fuzzy_rules_count)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        mu = self.encoder(x)\n",
    "        x_recon, fz, _ = self.decoder(mu)\n",
    "        \n",
    "        return mu, x_recon, fz\n",
    "    \n",
    "    def half_pass(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        mu = self.encoder(x)\n",
    "        return mu\n",
    "    \n",
    "    def decoder_pass(self, x):\n",
    "        r, f, _ = self.decoder(x)\n",
    "        return r, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_dim=latent_dim, fuzzy_rules_count=fuzzy_rules_count, kernels=kernels).to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of parameters: {num_params:,}')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam(list(model.encoder.parameters()) + list(model.decoder.parameters()), lr=learning_rate)\n",
    "#optimizer_d = torch.optim.Adam(model.delta_decoder.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, learning_rate, epochs=num_epochs, steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tarctanh(x):\n",
    "    return 0.5 * torch.log((1+x)/(1-x))\n",
    "\n",
    "def log_norm_const(x, eps = 1e-6):\n",
    "    # numerically stable computation\n",
    "    x = torch.clamp(x, eps, 1 - eps)\n",
    "    x = torch.where((x < 0.49) | (x > 0.51), x, 0.49 *\n",
    "            torch.ones_like(x))\n",
    "    return torch.log((2 * tarctanh(1 - 2 * x)) /\n",
    "                    (1 - 2 * x) + eps)\n",
    "\n",
    "def xent_continuous_ber(recon_x, x, pixelwise=False):\n",
    "    ''' p(x_i|z_i) a continuous bernoulli '''\n",
    "    eps = 1e-6\n",
    "    if pixelwise:\n",
    "        return (x * torch.log(recon_x + eps) +\n",
    "                        (1 - x) * torch.log(1 - recon_x + eps) +\n",
    "                        log_norm_const(recon_x, eps))\n",
    "    else:\n",
    "        return torch.mean(torch.sum(x * torch.log(recon_x + eps) +\n",
    "                        (1 - x) * torch.log(1 - recon_x + eps) +\n",
    "                        log_norm_const(recon_x, eps), dim=(1, )), dim=(1, 2))\n",
    "    \n",
    "\n",
    "a = -torch.ones(3, 1, 28, 28)\n",
    "b = -torch.ones(3, 1, 28, 28)\n",
    "\n",
    "xent_continuous_ber((a + 1)/2, (b + 1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vae_loss(x, recon_x):\n",
    "    \n",
    "    loss_recon = (x - recon_x).square().sum(-1).sum(-1).mean()\n",
    "    #loss_recon = -xent_continuous_ber((recon_x + 1)/2, (x + 1)/2)\n",
    "    #loss_recon = loss_recon.mean()\n",
    "    #loss_recon = (1 - ssim((recon_x + 1)/2, (x + 1)/2))\n",
    "    #loss_recon = loss_recon.mean()\n",
    "    \n",
    "    #raw_loss_recon = (1 - diff).square() #F.binary_cross_entropy((recon_x+1)/2, (x + 1)/2, reduction='none').sum(-1).mean()#\n",
    "    #loss_recon = raw_loss_recon.mean()\n",
    "    \n",
    "    return loss_recon #(x-recon_x).square().sum(-1).sum(-1).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fuzzy_loss(fuzzy, z, fz):\n",
    "    \n",
    "    iid = torch.cdist(z, fuzzy.get_centroids()).min(-1).indices.reshape(-1, 1)\n",
    "    winners = torch.gather(fz, 1, iid).squeeze()\n",
    "    \n",
    "    return (1 - winners).clamp(min=0.5).square().mean() + (1 - fz.sum(-1)).square().mean() #(0.999 - (tops[:,0]+tops[:,1]).clamp(max=0.999)).mean() + tops[:, 2].clamp(min=0.001).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids_loss(fuzzy_layer):\n",
    "    \n",
    "    c = fuzzy_layer[1].get_centroids()\n",
    "    c = c.mean(0)\n",
    "    ac = fuzzy_layer[0].get_average_centroid()\n",
    "\n",
    "    return (c-ac).square().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigens = model.decoder.fuzzy[1].get_transformation_matrix_eigenvals().real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fshape_loss(fuzzy_layer):\n",
    "    \n",
    "    eigens = fuzzy_layer.get_transformation_matrix_eigenvals().real\n",
    "    fz_volume = (0.1 - eigens).square()\n",
    "    fz_volume = fz_volume.mean()\n",
    "    \n",
    "    #fz_scales = (1 - eigens).square().mean()\n",
    "\n",
    "    return fz_volume\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_eigenvals_positive_loss(layer, eps = 1e-15):\n",
    "    ev = layer.get_transformation_matrix_eigenvals().real.min()\n",
    "    ev = torch.clamp(ev, max=eps)\n",
    "    return -ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arate(inp):\n",
    "    _, x_recon, _ = model.forward(inp)\n",
    "    \n",
    "    # prev_mu = mu\n",
    "    # sum = torch.zeros_like(mu)\n",
    "    \n",
    "    # for i in range(20):\n",
    "    #     recon_x = model.decoder_pass(mu)\n",
    "    #     mu, _, _ = model.half_pass(recon_x)\n",
    "    #     sum += (mu - prev_mu).abs()\n",
    "    #     prev_mu = mu\n",
    "    # +\n",
    "    return (1 - ssim((x_recon + 1)/2, (inp + 1)/2)).cpu().numpy()#xent_continuous_ber((x_recon + 1)/2, (inp + 1)/2).cpu().numpy()# #ssim((inp + 1)/2, (recon_x+1)/2).cpu().numpy() #fz.sum(-1).cpu().numpy()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def train(model, dataloader, optimizer, sched, prev_updates, epoch, writer=None):\n",
    "    model.train()  \n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(tqdm(dataloader, disable=True)):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        \n",
    "        data = data.to(device)\n",
    "        adata = augmentation(data)\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        z = model.half_pass(adata)  \n",
    "        x_reconstruct, _ = model.decoder_pass(z)\n",
    "        #x_resd, _ = model.resd_pass(z)\n",
    "        loss_reconstruct = compute_vae_loss(data, x_reconstruct)\n",
    "        #loss_resd = compute_vae_loss((data - x_reconstruct).abs(), x_resd)\n",
    "        #fz_loss = compute_fuzzy_loss(model.decoder.fuzzy[1], z, fz)\n",
    "        #fz_volume = compute_fshape_loss(model.decoder.fuzzy[1])\n",
    "        \n",
    "        ev_loss = keep_eigenvals_positive_loss(model.decoder.fuzzy[1])\n",
    "        if ev_loss.item() > 0:\n",
    "            ev_loss.backward(retain_graph=True)\n",
    "        \n",
    "        #fz_loss.backward(retain_graph=True)\n",
    "        #fz_volume.backward(retain_graph=True)\n",
    "        (loss_reconstruct).backward()\n",
    "        #loss_resd.backward() \n",
    "        \n",
    "        optimizer.step()  \n",
    "        \n",
    "        sched.step()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        \n",
    "    if writer is not None:\n",
    "        writer.add_scalar('ADFVAE/LR', get_lr(optimizer), global_step=epoch)\n",
    "        \n",
    "    return prev_updates + len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(fuzzy):\n",
    "    distr = fuzzy[2].get_norm_stats()\n",
    "    dim = distr.shape[0]\n",
    "    return np.log(dim) + (distr*distr.log()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_random_z = torch.randn(16, latent_dim).to(device)\n",
    "\n",
    "def test(model, dataloader, cur_step, epoch, writer=None):\n",
    "    model.eval() \n",
    "    test_recon_loss = 0\n",
    "    test_kl_loss = 0\n",
    "    test_fz_loss = 0\n",
    "    test_fzvol_loss = 0\n",
    "    test_c_loss = 0\n",
    "    \n",
    "    lab_true = []\n",
    "    lab_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, lab in tqdm(test_loader, desc='Test MNIST', disable=True):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            rates = get_arate(data)\n",
    "            \n",
    "            for f, l in  zip(rates, lab):\n",
    "                lab_pred.append(f)        \n",
    "                if l == mnist_class_anomaly:\n",
    "                    lab_true.append(1)\n",
    "                else:\n",
    "                    lab_true.append(0)\n",
    "                        \n",
    "    fpr, tpr, _ = metrics.roc_curve(lab_true, lab_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    embedings = []\n",
    "    labels_expected = []\n",
    "    centroids = model.decoder.fuzzy[1].get_centroids().detach().cpu().numpy()\n",
    "    #centroids_resd = model.resd.fuzzy[1].get_centroids().detach().cpu().numpy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc='Testing', disable=True):\n",
    "            data = data.to(device)\n",
    "            z = model.half_pass(data)  \n",
    "            x_reconstruct, fz = model.decoder_pass(z)\n",
    "            #x_resd, _ = model.resd_pass(z)\n",
    "            \n",
    "            embedings.append(z.cpu().numpy())\n",
    "            labels_expected.append(target.cpu().numpy())\n",
    "\n",
    "            loss_recon = compute_vae_loss(data, x_reconstruct)\n",
    "            #loss_resd = compute_vae_loss((data - x_reconstruct).abs(), x_resd)\n",
    "            fz_volume = compute_fshape_loss(model.decoder.fuzzy[1])\n",
    "            c_loss = compute_centroids_loss(model.decoder.fuzzy)    \n",
    "            fz_loss = compute_fuzzy_loss(model.decoder.fuzzy[1], z, fz)\n",
    "\n",
    "            test_recon_loss += loss_recon.item()\n",
    "            #test_kl_loss += loss_resd.item()\n",
    "            test_fz_loss += fz_loss.item()\n",
    "            test_fzvol_loss += fz_volume.item()\n",
    "            test_c_loss += c_loss.item()\n",
    "\n",
    "    embedings = np.concatenate(embedings, axis=0)\n",
    "    labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "\n",
    "    if epoch % 5 == 1:\n",
    "        plt.figure(figsize=(18, 6))\n",
    "\n",
    "        R, C = 1, 3\n",
    "\n",
    "        plt.subplot(R, C, 1)\n",
    "        plt.title(\"MNIST XY\")\n",
    "        plt.scatter(embedings[:, 0],      embedings[:,  1], c=labels_expected, cmap='tab10', s=2)\n",
    "        plt.scatter(centroids[:, 0],      centroids[:, 1], marker='1', c='black', s= 50)\n",
    "        #plt.scatter(centroids_resd[:, 0],      centroids_resd[:, 1], marker='2', c='red', s= 20)\n",
    "        \n",
    "        # plt.xlim((-1, 1))\n",
    "        # plt.ylim((-1, 1))\n",
    "\n",
    "        plt.subplot(R, C, 2)\n",
    "        plt.title(\"MNIST XZ\")\n",
    "        plt.scatter(embedings[:, 0],      embedings[:,  2], c=labels_expected, cmap='tab10', s=2)\n",
    "        plt.scatter(centroids[:, 0],      centroids[:, 2], marker='1', c='black', s= 50)\n",
    "        #plt.scatter(centroids_resd[:, 0], centroids_resd[:, 2], marker='2', c='red', s= 20)\n",
    "        \n",
    "        # plt.xlim((-1, 1))\n",
    "        # plt.ylim((-1, 1))\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    test_recon_loss /= len(dataloader)\n",
    "    test_kl_loss /= len(dataloader)\n",
    "    test_fz_loss /= len(dataloader)\n",
    "    test_fzvol_loss /= len(dataloader)\n",
    "    test_c_loss /= len(dataloader)\n",
    "    print(f'[{cur_step}] Reconstruction loss: {test_recon_loss:.4f}, KLD: {test_kl_loss:.4f} AUC {roc_auc:.4f} FZ {test_fz_loss:.4f} FZVOL {test_fzvol_loss:.4f} CENTROIDS DISCREPANCY {test_c_loss:.4f} SHANNON {shannon_entropy(model.decoder.fuzzy)}')\n",
    "    #print(f'Average activation stats: {model.decoder.fuzzy[2].get_norm_stats()}')\n",
    "    #print(f'Average centroid stats: {model.decoder.fuzzy[0].get_average_centroid()}')\n",
    "    if writer is not None:\n",
    "        writer.add_histogram('ADFVAE/TermActivation', model.decoder.fuzzy[2].get_norm_stats(), cur_step)\n",
    "        writer.add_scalar('ADFVAE/AUC', roc_auc, global_step=cur_step)\n",
    "        writer.add_scalar('ADFVAE/Reconstruction', test_recon_loss, global_step=cur_step)\n",
    "        writer.add_scalar('ADFVAE/KLD', test_kl_loss, global_step=cur_step)\n",
    "        writer.add_scalar('ADFVAE/Fuzzy', test_fz_loss, global_step=cur_step)\n",
    "        writer.add_scalar('ADFVAE/Fuzzy/Vol', test_fzvol_loss, global_step=cur_step)\n",
    "        writer.add_scalar('ADFVAE/Fuzzy/CDist', test_c_loss, global_step=cur_step)\n",
    "        \n",
    "        samples, _ = model.decoder_pass(fixed_random_z)\n",
    "        #samples_resd, _ = model.resd_pass(fixed_random_z)\n",
    "        writer.add_images('ADFVAE/Samples', samples.view(-1, 1, 28, 28), global_step=cur_step)\n",
    "        #writer.add_images('ADFVAE/ResdSamples', samples_resd.view(-1, 1, 28, 28), global_step=cur_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_updates = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    prev_updates = train(model, train_loader, optimizer, sched, prev_updates, epoch, writer=writer)\n",
    "    test(model, test_loader, prev_updates, epoch, writer=writer)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_eigenvals_positive_loss(model.decoder.fuzzy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализируем результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_activation_stats(model, dataloader):\n",
    "    rulestat = {}\n",
    "    with torch.no_grad():\n",
    "        for _, (data, _) in enumerate(tqdm(dataloader)):\n",
    "            data = data.to(device)\n",
    "            _, _, fz = model.forward(data)\n",
    "            act_fz = fz.max(-1).indices.cpu().numpy()\n",
    "            for ind in act_fz:\n",
    "                rulestat[ind] = rulestat.get(ind, 0) + 1\n",
    "    return rulestat\n",
    "\n",
    "train_stat = get_activation_stats(model, train_loader)\n",
    "test_stat = get_activation_stats(model, test_loader)\n",
    "\n",
    "plt.bar(list(train_stat.keys()), train_stat.values(), 0.5, color='g')\n",
    "plt.bar(list(test_stat.keys()), test_stat.values(), 0.5, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import IsolationForest\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# train_x = []\n",
    "# test_x = []\n",
    "# test_y = []\n",
    "# with torch.no_grad():\n",
    "#     for _, (data, _) in enumerate(tqdm(train_loader)):\n",
    "#         data = data.to(device)\n",
    "#         fz, _, _ = model.forward(data)\n",
    "#         for f in fz:\n",
    "#             train_x.append(f.cpu().numpy())\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for _, (data, target) in enumerate(tqdm(test_loader)):\n",
    "#         data = data.to(device)\n",
    "#         fz, _, _  = model.forward(data)\n",
    "#         for f, trg in zip(fz, target):\n",
    "#             test_x.append(f.cpu().numpy())\n",
    "#             if trg == mnist_class_anomaly:\n",
    "#                 test_y.append(-1)\n",
    "#             else:\n",
    "#                 test_y.append(1)\n",
    "    \n",
    "# clf = IsolationForest(random_state=0, n_estimators=1000).fit(train_x)\n",
    "# y_pred =clf.predict(test_x)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix(test_y, y_pred))\n",
    "# disp.plot()\n",
    "# accuracy_score(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_activation_stats_by_digit(digit, model, dataloader):\n",
    "    rulestat = {}\n",
    "    with torch.no_grad():\n",
    "        for _, (data, target) in enumerate(tqdm(dataloader)):\n",
    "        \n",
    "            data = data.to(device)\n",
    "            _, _, fz = model.forward(data)\n",
    "            act_fz = fz.max(-1).indices.cpu().numpy()\n",
    "            for ind, trg in zip(act_fz, target):\n",
    "                if trg == digit:\n",
    "                    rulestat[ind] = rulestat.get(ind, 0) + 1\n",
    "    return rulestat\n",
    "\n",
    "\n",
    "test_stat_by_digit = get_activation_stats_by_digit(9, model, test_loader)\n",
    "\n",
    "plt.bar(list(test_stat_by_digit.keys()), test_stat_by_digit.values(), 0.5, color='r')\n",
    "plt.xlim((-1, fuzzy_rules_count + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stat_by_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(64, latent_dim).to(device)\n",
    "samples, _ = model.decoder_pass(z)\n",
    "\n",
    "# Plot the generated images\n",
    "fig, ax = plt.subplots(8, 8, figsize=(8, 8))\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        ax[i, j].imshow(samples[i*8+j].view(28, 28).cpu().detach().numpy(), cmap='gray')\n",
    "        ax[i, j].axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fz = torch.zeros(64, fuzzy_rules_count).to(device)\n",
    "# for i in range(64):\n",
    "#     fz[i][5] = 1.0/(i+1)\n",
    "\n",
    "# samples = model.decoder.decode_from_fz(fz)\n",
    "\n",
    "# # Plot the generated images\n",
    "# fig, ax = plt.subplots(8, 8, figsize=(8, 8))\n",
    "# for i in range(8):\n",
    "#     for j in range(8):\n",
    "#         ax[i, j].imshow(samples[i*8+j].view(28, 28).cpu().detach().numpy(), cmap='gray')\n",
    "#         ax[i, j].axis('off')\n",
    "\n",
    "# # plt.show()\n",
    "# plt.savefig('cvae_mnist.webp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim = piqa.SSIM(window_size = 9, n_channels=1, reduction='none').to(device)\n",
    "def get_arate(inp):\n",
    "    _, x_recon, fz_1 = model.forward(inp)\n",
    "    _, x_recon, fz_2 = model.forward(x_recon)\n",
    "    \n",
    "    return (fz_1 - fz_2).abs().sum(-1).cpu().numpy() #(1 - ssim((x_recon + 1)/2, (inp + 1)/2)).cpu().numpy()#xent_continuous_ber((x_recon + 1)/2, (inp + 1)/2).cpu().numpy()# (1 - ssim((x_recon + 1)/2, (inp + 1)/2)).cpu().numpy() #ssim((inp + 1)/2, (recon_x+1)/2).cpu().numpy() #fz.sum(-1).cpu().numpy()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firings_mnist = {}\n",
    "firings_mnist['MNIST'] = []\n",
    "firings_mnist['DISSIDENT'] = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader, desc='MNIST HIST'):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        rates = get_arate(data)\n",
    "        for f, l in  zip(rates, target):\n",
    "            if l != mnist_class_anomaly:\n",
    "                firings_mnist['MNIST'].append(f)\n",
    "            else:\n",
    "                firings_mnist['DISSIDENT'].append(f)\n",
    "        \n",
    "\n",
    "labels, data = firings_mnist.keys(), firings_mnist.values()\n",
    "\n",
    "fig = plt.figure(figsize =(12, 2))\n",
    "plt.boxplot(data, notch=True, showfliers=False)\n",
    "plt.xticks(range(1, len(labels) + 1), labels)\n",
    "plt.show()\n",
    "\n",
    "writer.add_figure('Anomaly Detection', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    firing_levels = []\n",
    "    lab_true = []\n",
    "    lab_pred = []\n",
    "\n",
    "    for data, lab in tqdm(test_loader, desc='Test MNIST', disable=True):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        rates = get_arate(data)\n",
    "        \n",
    "        for f, l in  zip(rates, lab):\n",
    "            firing_levels.append(f)\n",
    "            lab_pred.append(f)        \n",
    "            if l == mnist_class_anomaly:\n",
    "                lab_true.append(1)\n",
    "            else:\n",
    "                lab_true.append(0)\n",
    "                    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(lab_true, lab_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    fig = plt.figure(figsize =(4, 4))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    writer.add_figure('ROC', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot():\n",
    "    centroids = model.decoder.fuzzy[1].get_centroids().detach().cpu().numpy()\n",
    "    embedings = []\n",
    "    labels_expected = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc='Encoding'):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            embeding,_,_ = model.forward(data)\n",
    "            embedings.append(embeding.cpu().numpy())\n",
    "            labels_expected.append(target.cpu().numpy())\n",
    "    embedings = np.concatenate(embedings, axis=0)\n",
    "    labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    R, C = 1, 3\n",
    "\n",
    "    plt.subplot(R, C, 1)\n",
    "    plt.title(\"MNIST XY\")\n",
    "    plt.scatter(embedings[:, 0],      embedings[:,  1], c=labels_expected, cmap='tab10', s=2)\n",
    "    #plt.scatter(centroids[:, 0],      centroids[:, 1], marker='1', c='black', s= 50)\n",
    "\n",
    "    plt.subplot(R, C, 2)\n",
    "    plt.title(\"MNIST XZ\")\n",
    "    plt.scatter(embedings[:, 0],      embedings[:,  2], c=labels_expected, cmap='tab10', s=2)\n",
    "    #plt.scatter(centroids[:, 0],      centroids[:, 2], marker='1', c='black', s= 50)\n",
    "\n",
    "    \n",
    "show_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_item_reconstructio(ind):\n",
    "    for data, trg in iter(test_loader):\n",
    "        data = data.to(device)\n",
    "        _, x_rec, fz = model.forward(data)\n",
    "        #_, x_rec_2, fz_2 = model.forward(x_rec)\n",
    "\n",
    "        plt.figure(figsize=(24, 6))\n",
    "\n",
    "        R, C = 1, 6\n",
    "\n",
    "        #plt.subplot(R, C, 1)\n",
    "        #plt.imshow(fz[ind].detach().cpu().squeeze())\n",
    "        #plt.bar(range(fuzzy_rules_count), fz[ind].detach().cpu().numpy(), color='g')\n",
    "        #plt.bar(range(fuzzy_rules_count), fz_2[ind].detach().cpu().numpy(), color='r')\n",
    "        plt.subplot(R, C, 1)\n",
    "        plt.imshow(data[ind].cpu().squeeze())\n",
    "        plt.subplot(R, C, 2)\n",
    "        plt.imshow(x_rec[ind].detach().cpu().squeeze())\n",
    "        #plt.subplot(R, C, 3)\n",
    "        #plt.imshow(x_rec_2[ind].detach().cpu().squeeze())\n",
    "        plt.subplot(R, C, 3)\n",
    "        plt.imshow((x_rec[ind] - data[ind]).abs().detach().cpu().squeeze())\n",
    "        #plt.subplot(R, C, 4)\n",
    "        #plt.imshow(((data[ind]-x_rec[ind]).abs() - x_resd[ind]).abs().clamp(min=0.2).abs().detach().cpu().squeeze())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_item_reconstructio(4)\n",
    "show_item_reconstructio(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
