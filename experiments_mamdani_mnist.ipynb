{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moister/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "from torchfuzzy import FuzzyLayer, DefuzzyLinearLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-2\n",
    "labels_count = 10\n",
    "num_epochs = 5\n",
    "fuzzy_dim = 2\n",
    "fuzzy_rules_count = 10\n",
    "\n",
    "prefix = \"mamdani_mnist\"\n",
    "writer = SummaryWriter(f'runs/mnist/{prefix}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = torch.rand((10,3))\n",
    "# df = DefuzzyLinearLayer.from_dimensions(3, 2)\n",
    "# df.forward(inp),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(lambda x: x.view(-1, 28, 28) - 0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(target_label):\n",
    "    \"\"\"\n",
    "    Возвращает вектор целевого значения\n",
    "\n",
    "    Args:\n",
    "        target_label (int): Метка класса\n",
    "    \n",
    "    Returns:\n",
    "        tensor (1, 10)\n",
    "    \"\"\"\n",
    "    t = F.one_hot(torch.LongTensor([target_label]), labels_count)\n",
    "    return t.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем обучающую выборку\n",
    "train_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=True, \n",
    "    transform = transform,\n",
    "    target_transform = transforms.Lambda(lambda x: get_target(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем тестовую выборку\n",
    "test_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=transform, \n",
    "    target_transform = transforms.Lambda(lambda x: get_target(x))\n",
    ")\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем итераторы датасетов\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    \n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Компонент энкодера\n",
    "    \n",
    "    Args:\n",
    "        fuzzy_dim (int): Размер латентного вектора.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fuzzy_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "                \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(8, 16, kernel_size=5),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(16, 32, kernel_size=5),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(32, 64, kernel_size=5),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(),  \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(9216, 625),\n",
    "            nn.BatchNorm1d(625),\n",
    "            nn.SiLU(),  \n",
    "            nn.Linear(625, fuzzy_dim),\n",
    "        )\n",
    "         \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Выход энкодера\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Входной вектор.\n",
    "        \n",
    "        Returns:\n",
    "            encoded input\n",
    "        \"\"\"\n",
    "\n",
    "        ex = self.encoder(x)\n",
    "        \n",
    "        return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MamdaniFIS(nn.Module):\n",
    "    \"\"\"\n",
    "    MamdaniFIS\n",
    "    \n",
    "    Args:\n",
    "        fuzzy_dim (int): Размер латентного вектора.\n",
    "        labels_count (int): Количество выходов классификатора\n",
    "    \"\"\"\n",
    "    def __init__(self, fuzzy_dim, fuzzy_rules_count, labels_count):\n",
    "        super(MamdaniFIS, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(fuzzy_dim)        \n",
    "        \n",
    "        self.fuzzy = nn.Sequential(\n",
    "            FuzzyLayer.from_dimensions(fuzzy_dim, fuzzy_rules_count, trainable=True),\n",
    "            DefuzzyLinearLayer.from_dimensions(fuzzy_rules_count, labels_count)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Входной вектор.\n",
    "        \n",
    "        Returns:\n",
    "            labels\n",
    "        \"\"\"\n",
    "\n",
    "        ex = self.encoder(x)\n",
    "        labels = self.fuzzy(ex)\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(target_labels, predicted_labels):\n",
    "    \n",
    "    #print(torch.squeeze(target_labels,1))\n",
    "    #print(predicted_labels)\n",
    "    ceLoss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    loss_fuzzy = ceLoss.forward(predicted_labels, torch.squeeze(target_labels,1).float())\n",
    "\n",
    "    return loss_fuzzy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5,830,935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MamdaniFIS(\n",
       "  (encoder): Encoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): SiLU()\n",
       "      (2): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (3): SiLU()\n",
       "      (4): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (5): SiLU()\n",
       "      (6): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): SiLU()\n",
       "      (9): Flatten(start_dim=1, end_dim=-1)\n",
       "      (10): Linear(in_features=9216, out_features=625, bias=True)\n",
       "      (11): BatchNorm1d(625, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): SiLU()\n",
       "      (13): Linear(in_features=625, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fuzzy): Sequential(\n",
       "    (0): FuzzyLayer()\n",
       "    (1): DefuzzyLinearLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MamdaniFIS(fuzzy_dim, fuzzy_rules_count, labels_count).to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of parameters: {num_params:,}')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, prev_updates, writer=None):\n",
    "    model.train()  \n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(tqdm(dataloader)):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        labels = model.forward(data)  \n",
    "        \n",
    "        loss = compute_loss(target, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if n_upd % 100 == 0:\n",
    "            total_norm = 0.0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "            print(f'Step {n_upd:,} (N samples: {n_upd*batch_size:,}), Loss: {loss.item():.4f} Grad: {total_norm:.4f}')\n",
    "\n",
    "            if writer is not None:\n",
    "                global_step = n_upd\n",
    "                writer.add_scalar('Loss/Train', loss.item(), global_step)\n",
    "                writer.add_scalar('GradNorm/Train', total_norm, global_step)\n",
    "            \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        \n",
    "        optimizer.step()  \n",
    "        \n",
    "    return prev_updates + len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, cur_step, writer=None):\n",
    "    model.eval() \n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc='Testing'):\n",
    "            data = data.to(device)\n",
    "            \n",
    "            labels = model.forward(data)  \n",
    "            \n",
    "            loss = compute_loss(target, labels)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            pred_target = np.argmax(labels.cpu().numpy(), axis=1)\n",
    "            target_labels =  np.argmax(target.cpu().numpy(), axis=1)\n",
    "            test_accuracy += np.sum(target_labels==pred_target)\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    test_accuracy /= len(dataloader)\n",
    "\n",
    "    print(f'====> Test set loss: {test_loss:.4f} (Accuracy {test_accuracy:.4f})')\n",
    "    \n",
    "    if writer is not None:\n",
    "        writer.add_scalar('Loss/Test', test_loss, global_step=cur_step)\n",
    "        writer.add_scalar('Fuzzy/Test/Accuracy', test_accuracy, global_step=cur_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/235 [00:01<04:26,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (N samples: 0), Loss: 2.3078 Grad: 0.8045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 74/235 [00:34<00:09, 17.76it/s]"
     ]
    }
   ],
   "source": [
    "prev_updates = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    prev_updates = train(model, train_loader, optimizer, prev_updates, writer=writer)\n",
    "    test(model, test_loader, prev_updates, writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
