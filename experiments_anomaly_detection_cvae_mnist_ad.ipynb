{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детектор аномалии на принципе многократной прогонки реконструкции входного образца до сходимости латентного вектора.\n",
    "Критерий аномальности - расстояние от первоначального латентного вектора до сошедшего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "from torchfuzzy import FuzzyLayer, DefuzzyLinearLayer, FuzzyBellLayer\n",
    "import piqa\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision.transforms import v2\n",
    "from torchinfo import summary\n",
    "from matplotlib.colors import ListedColormap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 2e-3\n",
    "num_epochs_ae = 10\n",
    "num_epochs_ad = 100\n",
    "latent_dim = 21\n",
    "mnist_class_anomaly = 4\n",
    "kernels = 8\n",
    "fuzzy_rules_count = 64\n",
    "\n",
    "prefix = f\"fuzzy_ad\"\n",
    "writer = SummaryWriter(f'runs/mnist/{prefix}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "binary_cmap = ListedColormap(['yellow', 'red'], N=2)\n",
    "ssim = piqa.SSIM(window_size = 11, n_channels=1, reduction='none').to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет\n",
    "\n",
    "1. Исключаем класс аномалии `mnist_class_anomaly` из общей выборк\n",
    "2. Убираем метки с остальных классов\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_and_transform(x):\n",
    "    nimg = x.view(-1, 28, 28)\n",
    "    nimg = torch.clamp(nimg, 0, 1)\n",
    "    return nimg\n",
    "\n",
    "def clamp(x):\n",
    "    #nimg = 2.0*(x.view(-1, 28, 28) - 0.5)\n",
    "    nimg = torch.clamp(x, 0, 1)\n",
    "    return nimg\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(norm_and_transform)\n",
    "])\n",
    "\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomRotation(15, fill=0), \n",
    "    transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), fill=0), \n",
    "    #transforms.RandomCrop(size=26),\n",
    "    #transforms.Resize(size=(28, 28)),\n",
    "    transforms.Lambda(clamp)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54158"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target_and_mask(target_label):\n",
    "    t = target_label\n",
    "    return t \n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=True, \n",
    "    transform = transform,\n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "\n",
    "idx = (train_data.targets != mnist_class_anomaly)\n",
    "train_data.targets = train_data.targets[idx]\n",
    "train_data.data = train_data.data[idx]\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "загружаем тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=transform, \n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем итераторы датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    \n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data,_ in iter(train_loader):\n",
    "    R, C = 1, 2\n",
    "    plt.subplot(R, C, 1)\n",
    "    plt.imshow(data[0].squeeze())\n",
    "    plt.subplot(R, C, 2)\n",
    "    plt.imshow(augmentation(data)[0].squeeze())\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Encoder                                  [256, 64]                 --\n",
       "├─Sequential: 1-1                        [256, 128, 1, 1]          --\n",
       "│    └─Conv2d: 2-1                       [256, 16, 24, 24]         416\n",
       "│    └─Conv2d: 2-2                       [256, 16, 20, 20]         6,416\n",
       "│    └─BatchNorm2d: 2-3                  [256, 16, 20, 20]         32\n",
       "│    └─SiLU: 2-4                         [256, 16, 20, 20]         --\n",
       "│    └─Conv2d: 2-5                       [256, 32, 16, 16]         12,832\n",
       "│    └─Conv2d: 2-6                       [256, 32, 12, 12]         25,632\n",
       "│    └─BatchNorm2d: 2-7                  [256, 32, 12, 12]         64\n",
       "│    └─SiLU: 2-8                         [256, 32, 12, 12]         --\n",
       "│    └─Conv2d: 2-9                       [256, 64, 8, 8]           51,264\n",
       "│    └─Conv2d: 2-10                      [256, 64, 4, 4]           102,464\n",
       "│    └─BatchNorm2d: 2-11                 [256, 64, 4, 4]           128\n",
       "│    └─SiLU: 2-12                        [256, 64, 4, 4]           --\n",
       "│    └─Conv2d: 2-13                      [256, 128, 1, 1]          131,200\n",
       "│    └─BatchNorm2d: 2-14                 [256, 128, 1, 1]          256\n",
       "│    └─SiLU: 2-15                        [256, 128, 1, 1]          --\n",
       "├─Sequential: 1-2                        [256, 21]                 --\n",
       "│    └─Flatten: 2-16                     [256, 128]                --\n",
       "│    └─Linear: 2-17                      [256, 21]                 2,709\n",
       "│    └─BatchNorm1d: 2-18                 [256, 21]                 42\n",
       "│    └─Tanh: 2-19                        [256, 21]                 --\n",
       "├─Sequential: 1-3                        [256, 64]                 --\n",
       "│    └─FuzzyLayer: 2-20                  [256, 64]                 17,537\n",
       "├─Sequential: 1-4                        [256, 784]                --\n",
       "│    └─DefuzzyLinearLayer: 2-21          [256, 784]                50,176\n",
       "==========================================================================================\n",
       "Total params: 401,168\n",
       "Trainable params: 399,759\n",
       "Non-trainable params: 1,409\n",
       "Total mult-adds (G): 3.80\n",
       "==========================================================================================\n",
       "Input size (MB): 0.80\n",
       "Forward/backward pass size (MB): 95.54\n",
       "Params size (MB): 1.53\n",
       "Estimated Total Size (MB): 97.88\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Компонент энкодера для VAE\n",
    "    \n",
    "    Args:\n",
    "        latent_dim (int): Размер латентного вектора.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, kernels):\n",
    "        super(Encoder, self).__init__()\n",
    "                \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels, kernel_size = 5), \n",
    "            nn.Conv2d(kernels, kernels, kernel_size = 5), \n",
    "            nn.BatchNorm2d(kernels), \n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Conv2d(kernels, 2*kernels, kernel_size = 5), \n",
    "            nn.Conv2d(2*kernels, 2*kernels, kernel_size = 5), \n",
    "            nn.BatchNorm2d(2*kernels), \n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Conv2d(2*kernels, 4*kernels, kernel_size = 5), \n",
    "            nn.Conv2d(4*kernels, 4*kernels, kernel_size = 5), \n",
    "            nn.BatchNorm2d(4*kernels), \n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Conv2d(4*kernels, 8*kernels, kernel_size = 4), \n",
    "            nn.BatchNorm2d(8*kernels), \n",
    "            nn.SiLU(),            \n",
    "        )\n",
    "\n",
    "        self.latent = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(8*kernels, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        initial_centroids = (0.5-np.random.rand(fuzzy_rules_count, latent_dim))\n",
    "        initial_scales = np.ones((fuzzy_rules_count, latent_dim))\n",
    "\n",
    "        self.fuzzy_a = nn.Sequential(\n",
    "            FuzzyLayer.from_centers_and_scales(initial_centroids, initial_scales)\n",
    "        )\n",
    "\n",
    "        self.defuzzy = nn.Sequential(\n",
    "            #DefuzzyLinearLayer.from_array(np.repeat(1e-1, fuzzy_rules_count).reshape(1, -1), with_norm=False, trainable=True)\n",
    "            DefuzzyLinearLayer.from_dimensions(fuzzy_rules_count, 28*28, with_norm=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Выход энкодера для чистого VAE.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Входной вектор.\n",
    "            eps (float): Небольшая поправка к скейлу для лучшей сходимости и устойчивости.\n",
    "        \n",
    "        Returns:\n",
    "            mu, logvar, z, dist\n",
    "        \"\"\"\n",
    "        enc = self.encoder(x)\n",
    "        latent = self.latent(enc)\n",
    "        #pt_a, pt_b = torch.chunk(latent, 2, dim=-1)\n",
    "        fz = self.fuzzy_a(latent)\n",
    "        dfz = self.defuzzy(fz).reshape(-1, 1, 28, 28)\n",
    "        #fz = (fz.clamp(min=0.1) - 0.1)/0.9\n",
    "        #fz_b = self.fuzzy_b(pt_b)\n",
    "        #fz = torch.cat((fz_a, fz_b),1)\n",
    "        #fz = 1 - fz\n",
    "        return fz, latent, dfz\n",
    "\n",
    "#inp = torch.rand(10, 1, 28, 28)\n",
    "m = Encoder(latent_dim, 16)\n",
    "#fz, mu, dfz = m.forward(inp)\n",
    "\n",
    "summary(m, input_size=(batch_size, 1, 28, 28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Decoder                                  [256, 1, 28, 28]          --\n",
       "├─Sequential: 1-1                        [256, 1, 28, 28]          --\n",
       "│    └─Linear: 2-1                       [256, 128]                2,816\n",
       "│    └─BatchNorm1d: 2-2                  [256, 128]                256\n",
       "│    └─Unflatten: 2-3                    [256, 128, 1, 1]          --\n",
       "│    └─ConvTranspose2d: 2-4              [256, 64, 4, 4]           131,136\n",
       "│    └─BatchNorm2d: 2-5                  [256, 64, 4, 4]           128\n",
       "│    └─SiLU: 2-6                         [256, 64, 4, 4]           --\n",
       "│    └─ConvTranspose2d: 2-7              [256, 32, 8, 8]           51,232\n",
       "│    └─ConvTranspose2d: 2-8              [256, 32, 12, 12]         25,632\n",
       "│    └─BatchNorm2d: 2-9                  [256, 32, 12, 12]         64\n",
       "│    └─SiLU: 2-10                        [256, 32, 12, 12]         --\n",
       "│    └─ConvTranspose2d: 2-11             [256, 16, 16, 16]         12,816\n",
       "│    └─ConvTranspose2d: 2-12             [256, 16, 20, 20]         6,416\n",
       "│    └─BatchNorm2d: 2-13                 [256, 16, 20, 20]         32\n",
       "│    └─SiLU: 2-14                        [256, 16, 20, 20]         --\n",
       "│    └─ConvTranspose2d: 2-15             [256, 16, 24, 24]         6,416\n",
       "│    └─ConvTranspose2d: 2-16             [256, 1, 28, 28]          401\n",
       "│    └─BatchNorm2d: 2-17                 [256, 1, 28, 28]          2\n",
       "│    └─SiLU: 2-18                        [256, 1, 28, 28]          --\n",
       "==========================================================================================\n",
       "Total params: 237,347\n",
       "Trainable params: 237,347\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.85\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 84.48\n",
       "Params size (MB): 0.95\n",
       "Estimated Total Size (MB): 85.45\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Компонент декодера для VAE\n",
    "    \n",
    "    Args:\n",
    "        latent_dim (int): Размер латентного вектора.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, kernels):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 8*kernels),\n",
    "            nn.BatchNorm1d(8*kernels),\n",
    "            nn.Unflatten(1, (8*kernels, 1, 1)),\n",
    "            \n",
    "            nn.ConvTranspose2d(8*kernels, 4*kernels, 4),\n",
    "            nn.BatchNorm2d(4*kernels),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(4*kernels, 2*kernels, 5),\n",
    "            nn.ConvTranspose2d(2*kernels, 2*kernels, 5),\n",
    "            nn.BatchNorm2d(2*kernels),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(2*kernels, kernels, 5),\n",
    "            nn.ConvTranspose2d(kernels, kernels, 5),\n",
    "            nn.BatchNorm2d(kernels),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(kernels, kernels, 5),\n",
    "            nn.ConvTranspose2d(kernels, 1, 5),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        \n",
    "         \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Декодирует латентный вектор в исходное представление\n",
    "        \n",
    "        Args:\n",
    "            z (torch.Tensor): Латентный вектор.\n",
    "        \n",
    "        Returns:\n",
    "            x\n",
    "        \"\"\"\n",
    "        out = self.decode(z)\n",
    "        out = out.clamp(0, 1)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "# inp = torch.rand(batch_size, latent_dim)\n",
    "m = Decoder(latent_dim, 16)\n",
    "# mu = m.forward(inp)\n",
    "# mu[0].shape\n",
    "\n",
    "summary(m, input_size=(batch_size, latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        latent_dim (int): Размер латентного вектора.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim, fuzzy_rules_count, kernels):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(latent_dim, kernels)        \n",
    "        self.decoder = Decoder(latent_dim, kernels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        fz, mu, dfz = self.encoder(x)\n",
    "        x_recon = self.decoder(mu)\n",
    "        \n",
    "        return fz, mu, x_recon, dfz\n",
    "    \n",
    "    def half_pass(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        fz, mu, dfz = self.encoder(x)\n",
    "        return fz, mu, dfz\n",
    "    \n",
    "    def decoder_pass(self, x):\n",
    "        r = self.decoder(x)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 211,002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VAE                                      [256, 64]                 --\n",
       "├─Encoder: 1-1                           [256, 64]                 --\n",
       "│    └─Sequential: 2-1                   [256, 64, 1, 1]           --\n",
       "│    │    └─Conv2d: 3-1                  [256, 8, 24, 24]          208\n",
       "│    │    └─Conv2d: 3-2                  [256, 8, 20, 20]          1,608\n",
       "│    │    └─BatchNorm2d: 3-3             [256, 8, 20, 20]          16\n",
       "│    │    └─SiLU: 3-4                    [256, 8, 20, 20]          --\n",
       "│    │    └─Conv2d: 3-5                  [256, 16, 16, 16]         3,216\n",
       "│    │    └─Conv2d: 3-6                  [256, 16, 12, 12]         6,416\n",
       "│    │    └─BatchNorm2d: 3-7             [256, 16, 12, 12]         32\n",
       "│    │    └─SiLU: 3-8                    [256, 16, 12, 12]         --\n",
       "│    │    └─Conv2d: 3-9                  [256, 32, 8, 8]           12,832\n",
       "│    │    └─Conv2d: 3-10                 [256, 32, 4, 4]           25,632\n",
       "│    │    └─BatchNorm2d: 3-11            [256, 32, 4, 4]           64\n",
       "│    │    └─SiLU: 3-12                   [256, 32, 4, 4]           --\n",
       "│    │    └─Conv2d: 3-13                 [256, 64, 1, 1]           32,832\n",
       "│    │    └─BatchNorm2d: 3-14            [256, 64, 1, 1]           128\n",
       "│    │    └─SiLU: 3-15                   [256, 64, 1, 1]           --\n",
       "│    └─Sequential: 2-2                   [256, 21]                 --\n",
       "│    │    └─Flatten: 3-16                [256, 64]                 --\n",
       "│    │    └─Linear: 3-17                 [256, 21]                 1,365\n",
       "│    │    └─BatchNorm1d: 3-18            [256, 21]                 42\n",
       "│    │    └─Tanh: 3-19                   [256, 21]                 --\n",
       "│    └─Sequential: 2-3                   [256, 64]                 --\n",
       "│    │    └─FuzzyLayer: 3-20             [256, 64]                 17,537\n",
       "│    └─Sequential: 2-4                   [256, 784]                --\n",
       "│    │    └─DefuzzyLinearLayer: 3-21     [256, 784]                50,176\n",
       "├─Decoder: 1-2                           [256, 1, 28, 28]          --\n",
       "│    └─Sequential: 2-5                   [256, 1, 28, 28]          --\n",
       "│    │    └─Linear: 3-22                 [256, 64]                 1,408\n",
       "│    │    └─BatchNorm1d: 3-23            [256, 64]                 128\n",
       "│    │    └─Unflatten: 3-24              [256, 64, 1, 1]           --\n",
       "│    │    └─ConvTranspose2d: 3-25        [256, 32, 4, 4]           32,800\n",
       "│    │    └─BatchNorm2d: 3-26            [256, 32, 4, 4]           64\n",
       "│    │    └─SiLU: 3-27                   [256, 32, 4, 4]           --\n",
       "│    │    └─ConvTranspose2d: 3-28        [256, 16, 8, 8]           12,816\n",
       "│    │    └─ConvTranspose2d: 3-29        [256, 16, 12, 12]         6,416\n",
       "│    │    └─BatchNorm2d: 3-30            [256, 16, 12, 12]         32\n",
       "│    │    └─SiLU: 3-31                   [256, 16, 12, 12]         --\n",
       "│    │    └─ConvTranspose2d: 3-32        [256, 8, 16, 16]          3,208\n",
       "│    │    └─ConvTranspose2d: 3-33        [256, 8, 20, 20]          1,608\n",
       "│    │    └─BatchNorm2d: 3-34            [256, 8, 20, 20]          16\n",
       "│    │    └─SiLU: 3-35                   [256, 8, 20, 20]          --\n",
       "│    │    └─ConvTranspose2d: 3-36        [256, 8, 24, 24]          1,608\n",
       "│    │    └─ConvTranspose2d: 3-37        [256, 1, 28, 28]          201\n",
       "│    │    └─BatchNorm2d: 3-38            [256, 1, 28, 28]          2\n",
       "│    │    └─SiLU: 3-39                   [256, 1, 28, 28]          --\n",
       "==========================================================================================\n",
       "Total params: 212,411\n",
       "Trainable params: 211,002\n",
       "Non-trainable params: 1,409\n",
       "Total mult-adds (G): 2.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.80\n",
       "Forward/backward pass size (MB): 92.46\n",
       "Params size (MB): 0.78\n",
       "Estimated Total Size (MB): 94.04\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAE(latent_dim=latent_dim, fuzzy_rules_count=fuzzy_rules_count, kernels=kernels).to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of parameters: {num_params:,}')\n",
    "\n",
    "summary(model, input_size=(batch_size, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ae_loss(x, recon_x):\n",
    "    #loss_c = ssim(x, centroid).clamp(0, 1).detach()\n",
    "    loss_recon = (x-recon_x).square().sum(-1).sum(-1)#(1 - ssim(recon_x, x))#\n",
    "    loss_recon = loss_recon.mean()\n",
    "    return loss_recon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0741, 0.8645, 0.1468, 0.7890, 0.9569, 0.5522, 0.0978, 0.6596, 0.7066,\n",
       "         0.7062],\n",
       "        [0.8700, 0.2555, 0.8433, 0.2046, 0.0582, 0.9643, 0.7489, 0.9231, 0.0502,\n",
       "         0.2417]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fuzzy_loss(fz):\n",
    "    return  fz.topk(2).values.diff().mean() # +  (1 - fz.qu.max(-1)).mean() #(0.999 - (tops[:,0]+tops[:,1]).clamp(max=0.999)).mean() + tops[:, 2].clamp(min=0.001).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_eigenvals_positive_loss(layer, eps = 1e-15):\n",
    "    ev = layer.get_transformation_matrix_eigenvals().real.min()\n",
    "    ev = torch.clamp(ev, max=eps)\n",
    "    return -ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_term_volume_loss(layer, fz):\n",
    "    dfz = fz.detach().mean(0)\n",
    "    \n",
    "    ev = layer.get_transformation_matrix_eigenvals().real.mean(-1)\n",
    "    ev = 1/ev.mean()\n",
    "    \n",
    "    return ev#(1 - fz.max(-1).values).abs().mean()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arate(inp):\n",
    "    fz, mu, x_recon, dfz = model.forward(inp)\n",
    "    \n",
    "    return fz.sum(-1).cpu().numpy() #(1 - ssim(x_recon.clamp(0, 1), inp)).cpu().numpy()#xent_continuous_ber((x_recon + 1)/2, (inp + 1)/2).cpu().numpy()# #ssim((inp + 1)/2, (recon_x+1)/2).cpu().numpy() #fz.sum(-1).cpu().numpy()#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ae = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer_ae, learning_rate, epochs=num_epochs_ae, steps_per_epoch=len(train_loader))\n",
    "#sched = torch.optim.lr_scheduler.ConstantLR(optimizer_ae, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def train(model, dataloader, optimizer, sched, prev_updates, epoch, writer=None):\n",
    "    model.train()  \n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(tqdm(dataloader, disable=True)):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        \n",
    "        data = data.to(device)\n",
    "        adata = augmentation(data)\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        _, mu, _ = model.half_pass(adata)  \n",
    "        x_reconstruct = model.decoder_pass(mu)\n",
    "        \n",
    "        loss_reconstruct = compute_ae_loss(data, x_reconstruct)\n",
    "        \n",
    "        loss_reconstruct.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1e-2)\n",
    "        optimizer.step()  \n",
    "\n",
    "        if sched is not None:\n",
    "            sched.step()\n",
    "        \n",
    "    if writer is not None:\n",
    "        writer.add_scalar('FAD/LR', get_lr(optimizer), global_step=epoch)\n",
    "        \n",
    "    return prev_updates + len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed_random_z = torch.randn(16, fuzzy_rules_count).to(device)\n",
    "\n",
    "def test(model, dataloader, cur_step, epoch, writer=None):\n",
    "    model.eval() \n",
    "\n",
    "    test_recon_loss = 0\n",
    "    test_fz_loss = 0\n",
    "    test_fz_sum_loss = 0\n",
    "    \n",
    "    lab_true = []\n",
    "    lab_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, lab in tqdm(test_loader, desc='Test MNIST', disable=True):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            rates = get_arate(data)\n",
    "            \n",
    "            for f, l in  zip(rates, lab):\n",
    "                lab_pred.append(f)        \n",
    "                if l == mnist_class_anomaly:\n",
    "                    lab_true.append(1)\n",
    "                else:\n",
    "                    lab_true.append(0)\n",
    "                        \n",
    "    fpr, tpr, _ = metrics.roc_curve(lab_true, lab_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    embedings = []\n",
    "    labels_expected = []\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc='Testing', disable=True):\n",
    "            data = data.to(device)\n",
    "\n",
    "            fz, mu, dfz = model.half_pass(data)  \n",
    "            x_reconstruct = model.decoder_pass(mu)\n",
    "            #x_centroidal = model.decoder(F.one_hot(fz.argmax(-1), fuzzy_rules_count).float())\n",
    "\n",
    "            embedings.append(mu.cpu().numpy())\n",
    "            labels_expected.append((target == mnist_class_anomaly).cpu().numpy())\n",
    "\n",
    "            loss_recon = compute_ae_loss(data, x_reconstruct)\n",
    "            fz_loss = fuzzy_term_volume_loss(model.encoder.fuzzy_a[0], fz)\n",
    "            \n",
    "            test_recon_loss += loss_recon.item()\n",
    "            test_fz_loss += fz_loss.item()\n",
    "            test_fz_sum_loss += fz.sum(-1).mean()\n",
    "            \n",
    "\n",
    "    embedings = np.concatenate(embedings, axis=0)\n",
    "    labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "   \n",
    "    test_recon_loss /= len(dataloader)\n",
    "    test_fz_loss /= len(dataloader)\n",
    "    test_fz_sum_loss /= len(dataloader)\n",
    "    \n",
    "    print(f'[{cur_step}] Reconstruction loss: {test_recon_loss:.4f}, VOL: {test_fz_loss:.4f} SUM: {test_fz_sum_loss:.2f} AUC: {roc_auc:.4f}')\n",
    "    #print(f'Average activation stats: {model.decoder.fuzzy[2].get_norm_stats()}')\n",
    "    #print(f'Average centroid stats: {model.decoder.fuzzy[0].get_average_centroid()}')\n",
    "    if writer is not None:\n",
    "        writer.add_scalar('FAD/AUC', roc_auc, global_step=cur_step)\n",
    "        writer.add_scalar('FAD/Reconstruction', test_recon_loss, global_step=cur_step)\n",
    "        writer.add_scalar('FAD/Fz', test_fz_loss, global_step=cur_step)\n",
    "        writer.add_scalar('FAD/Sum', test_fz_sum_loss, global_step=cur_step)\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "        centroids_a = model.encoder.fuzzy_a[0].get_centroids().detach().cpu().numpy()\n",
    "        #centroids_b = model.encoder.fuzzy_b[0].get_centroids().detach().cpu().numpy()\n",
    "        ax[0].scatter(embedings[:, 0],      embedings[:,  1], c=labels_expected, cmap=binary_cmap, s=2)\n",
    "        ax[0].scatter(centroids_a[:, 0],      centroids_a[:, 1], marker='1', c='black', s= 50)\n",
    "        ax[0].set_xlim(-1, 1)\n",
    "        ax[0].set_ylim(-1, 1)\n",
    "        #ax[0].scatter(centroids_b[:, 0],      centroids_b[:, 1], marker='2', c='black', s= 50)\n",
    "        \n",
    "        ax[1].scatter(embedings[:, 0],      embedings[:,  2], c=labels_expected, cmap=binary_cmap, s=2)\n",
    "        ax[1].scatter(centroids_a[:, 1],      centroids_a[:, 2], marker='1', c='black', s= 50)\n",
    "        ax[1].set_xlim(-1, 1)\n",
    "        ax[1].set_ylim(-1, 1)\n",
    "        #ax[1].scatter(centroids_b[:, 1],      centroids_b[:, 2], marker='2', c='black', s= 50)\n",
    "    \n",
    "        act_fz = torch.diag(torch.ones(fuzzy_rules_count)).to(device)\n",
    "        samples = model.encoder.defuzzy(act_fz)\n",
    "        img_idx = 0\n",
    "        fign, axn = plt.subplots(8, 1 + fuzzy_rules_count//8, figsize=(1 + fuzzy_rules_count//8, 8), squeeze=False)\n",
    "        for i in range(8):\n",
    "            if img_idx >= fuzzy_rules_count:\n",
    "                continue\n",
    "            for j in range(fuzzy_rules_count//8):\n",
    "                axn[i, j].imshow(samples[img_idx].view(28, 28).cpu().detach().numpy(), cmap='gray')\n",
    "                axn[i, j].axis('off')\n",
    "                img_idx += 1\n",
    "\n",
    "        writer.add_figure('FAD/Emedding', fig, global_step=cur_step)\n",
    "        writer.add_figure('FAD/Samples', fign, global_step=cur_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_updates = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[212] Reconstruction loss: 39.6550, VOL: 1.0000 SUM: 4.24 AUC: 0.6336\n",
      "[424] Reconstruction loss: 34.9533, VOL: 1.0000 SUM: 2.73 AUC: 0.6773\n",
      "[636] Reconstruction loss: 29.5047, VOL: 1.0000 SUM: 3.17 AUC: 0.6058\n",
      "[848] Reconstruction loss: 26.7710, VOL: 1.0000 SUM: 4.63 AUC: 0.5735\n",
      "[1060] Reconstruction loss: 22.0772, VOL: 1.0000 SUM: 5.45 AUC: 0.5934\n",
      "[1272] Reconstruction loss: 20.2984, VOL: 1.0000 SUM: 5.84 AUC: 0.5671\n",
      "[1484] Reconstruction loss: 17.9633, VOL: 1.0000 SUM: 6.40 AUC: 0.6335\n",
      "[1696] Reconstruction loss: 17.8070, VOL: 1.0000 SUM: 6.58 AUC: 0.5888\n",
      "[1908] Reconstruction loss: 18.9903, VOL: 1.0000 SUM: 6.32 AUC: 0.6517\n",
      "[2120] Reconstruction loss: 14.5652, VOL: 1.0000 SUM: 6.38 AUC: 0.6084\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs_ae):\n",
    "    prev_updates = train(model, train_loader, optimizer_ae, sched, prev_updates, epoch, writer=writer)\n",
    "    test(model, test_loader, prev_updates, epoch, writer=writer)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.encoder.fuzzy_a.parameters()) + list(model.encoder.latent.parameters()) + list(model.encoder.defuzzy.parameters())\n",
    "optimizer_ad = torch.optim.Adam(params, lr = learning_rate)\n",
    "sched_ad = torch.optim.lr_scheduler.ConstantLR(optimizer_ad, learning_rate)\n",
    "#sched_ad = torch.optim.lr_scheduler.OneCycleLR(optimizer_ad, learning_rate, epochs=num_epochs_ad, steps_per_epoch=len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ad(model, dataloader, optimizer, sched, prev_updates, epoch, writer=None):\n",
    "    model.train()  \n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(tqdm(dataloader, disable=True)):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        \n",
    "        data = data.to(device)\n",
    "        adata = augmentation(data)\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "        \n",
    "        fz, mu, dfz = model.half_pass(adata)  \n",
    "        x_reconstruct = model.decoder_pass(mu)\n",
    "        #fz_loss = compute_fuzzy_loss(fz)\n",
    "        loss_reconstruct_ae = compute_ae_loss(data, x_reconstruct)\n",
    "        loss_reconstruct_ad = compute_ae_loss(data, dfz) #(loss_reconstruct_ae - dfz).square().mean() #(data - dfz).square().sum(-1).sum(-1).mean() #+ compute_ae_loss(data, x_reconstruct)\n",
    "\n",
    "        ev_loss = keep_eigenvals_positive_loss(model.encoder.fuzzy_a[0])\n",
    "        if ev_loss.item() > 0:\n",
    "            ev_loss.backward(retain_graph=True)\n",
    "        \n",
    "        (loss_reconstruct_ad + 1e-3 * loss_reconstruct_ae).backward()\n",
    "        #fz_loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1e-2)\n",
    "        optimizer.step()  \n",
    "\n",
    "        if sched is not None:\n",
    "            sched.step()\n",
    "        \n",
    "    if writer is not None:\n",
    "        writer.add_scalar('FAD/LR', get_lr(optimizer), global_step=epoch)\n",
    "        \n",
    "    return prev_updates + len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8268] Reconstruction loss: 37.8334, VOL: 0.9203 SUM: 6.10 AUC: 0.4569\n",
      "[8480] Reconstruction loss: 42.4272, VOL: 0.9203 SUM: 6.09 AUC: 0.4524\n",
      "[8692] Reconstruction loss: 37.6675, VOL: 0.9192 SUM: 6.11 AUC: 0.4482\n",
      "[8904] Reconstruction loss: 38.0011, VOL: 0.9185 SUM: 6.50 AUC: 0.4496\n",
      "[9116] Reconstruction loss: 38.9362, VOL: 0.9185 SUM: 6.40 AUC: 0.4345\n",
      "[9328] Reconstruction loss: 41.0714, VOL: 0.9168 SUM: 6.41 AUC: 0.4522\n",
      "[9540] Reconstruction loss: 39.0208, VOL: 0.9171 SUM: 6.53 AUC: 0.4649\n",
      "[9752] Reconstruction loss: 42.1513, VOL: 0.9158 SUM: 6.58 AUC: 0.4791\n",
      "[9964] Reconstruction loss: 40.7680, VOL: 0.9161 SUM: 6.54 AUC: 0.4355\n",
      "[10176] Reconstruction loss: 37.1325, VOL: 0.9151 SUM: 6.80 AUC: 0.4448\n",
      "[10388] Reconstruction loss: 38.3357, VOL: 0.9142 SUM: 6.80 AUC: 0.4364\n",
      "[10600] Reconstruction loss: 37.3624, VOL: 0.9134 SUM: 6.87 AUC: 0.4444\n",
      "[10812] Reconstruction loss: 38.9513, VOL: 0.9127 SUM: 7.00 AUC: 0.4499\n",
      "[11024] Reconstruction loss: 37.9554, VOL: 0.9121 SUM: 7.04 AUC: 0.4545\n",
      "[11236] Reconstruction loss: 38.6181, VOL: 0.9108 SUM: 7.08 AUC: 0.4466\n",
      "[11448] Reconstruction loss: 37.9946, VOL: 0.9098 SUM: 7.22 AUC: 0.4595\n",
      "[11660] Reconstruction loss: 37.6396, VOL: 0.9083 SUM: 7.28 AUC: 0.4649\n",
      "[11872] Reconstruction loss: 36.9875, VOL: 0.9072 SUM: 7.33 AUC: 0.4541\n",
      "[12084] Reconstruction loss: 39.5049, VOL: 0.9068 SUM: 7.32 AUC: 0.4874\n",
      "[12296] Reconstruction loss: 44.4569, VOL: 0.9067 SUM: 7.10 AUC: 0.4445\n",
      "[12508] Reconstruction loss: 37.1724, VOL: 0.9043 SUM: 7.68 AUC: 0.4566\n",
      "[12720] Reconstruction loss: 42.0806, VOL: 0.9037 SUM: 7.41 AUC: 0.4361\n",
      "[12932] Reconstruction loss: 38.5872, VOL: 0.9028 SUM: 7.34 AUC: 0.4701\n",
      "[13144] Reconstruction loss: 38.3234, VOL: 0.9018 SUM: 7.53 AUC: 0.4367\n",
      "[13356] Reconstruction loss: 41.5278, VOL: 0.9003 SUM: 7.56 AUC: 0.4464\n",
      "[13568] Reconstruction loss: 37.2627, VOL: 0.8991 SUM: 7.44 AUC: 0.4425\n",
      "[13780] Reconstruction loss: 40.5942, VOL: 0.8981 SUM: 7.48 AUC: 0.4370\n",
      "[13992] Reconstruction loss: 37.4043, VOL: 0.8967 SUM: 7.66 AUC: 0.4243\n",
      "[14204] Reconstruction loss: 38.9107, VOL: 0.8953 SUM: 7.66 AUC: 0.4347\n",
      "[14416] Reconstruction loss: 39.5867, VOL: 0.8950 SUM: 7.69 AUC: 0.4777\n",
      "[14628] Reconstruction loss: 37.7299, VOL: 0.8942 SUM: 7.67 AUC: 0.4342\n",
      "[14840] Reconstruction loss: 43.4126, VOL: 0.8923 SUM: 7.69 AUC: 0.4405\n",
      "[15052] Reconstruction loss: 38.2858, VOL: 0.8912 SUM: 7.72 AUC: 0.4439\n",
      "[15264] Reconstruction loss: 37.2572, VOL: 0.8901 SUM: 7.83 AUC: 0.4344\n",
      "[15476] Reconstruction loss: 38.1623, VOL: 0.8894 SUM: 7.72 AUC: 0.4574\n",
      "[15688] Reconstruction loss: 36.5861, VOL: 0.8882 SUM: 7.92 AUC: 0.4314\n",
      "[15900] Reconstruction loss: 38.7027, VOL: 0.8871 SUM: 7.99 AUC: 0.4394\n",
      "[16112] Reconstruction loss: 37.4841, VOL: 0.8866 SUM: 7.79 AUC: 0.4245\n",
      "[16324] Reconstruction loss: 37.2718, VOL: 0.8846 SUM: 8.07 AUC: 0.4353\n",
      "[16536] Reconstruction loss: 37.6783, VOL: 0.8838 SUM: 8.02 AUC: 0.4164\n",
      "[16748] Reconstruction loss: 40.3252, VOL: 0.8823 SUM: 7.97 AUC: 0.4304\n",
      "[16960] Reconstruction loss: 37.4999, VOL: 0.8815 SUM: 7.89 AUC: 0.4311\n",
      "[17172] Reconstruction loss: 38.5406, VOL: 0.8801 SUM: 7.85 AUC: 0.4563\n",
      "[17384] Reconstruction loss: 46.2959, VOL: 0.8783 SUM: 7.64 AUC: 0.4244\n",
      "[17596] Reconstruction loss: 38.1989, VOL: 0.8774 SUM: 8.02 AUC: 0.4273\n",
      "[17808] Reconstruction loss: 40.9724, VOL: 0.8756 SUM: 7.88 AUC: 0.4267\n",
      "[18020] Reconstruction loss: 48.6275, VOL: 0.8750 SUM: 7.58 AUC: 0.4200\n",
      "[18232] Reconstruction loss: 37.1958, VOL: 0.8738 SUM: 8.08 AUC: 0.4399\n",
      "[18444] Reconstruction loss: 38.2020, VOL: 0.8722 SUM: 8.22 AUC: 0.4299\n",
      "[18656] Reconstruction loss: 37.3508, VOL: 0.8705 SUM: 8.04 AUC: 0.4164\n",
      "[18868] Reconstruction loss: 42.4085, VOL: 0.8690 SUM: 7.85 AUC: 0.4591\n",
      "[19080] Reconstruction loss: 40.3274, VOL: 0.8685 SUM: 8.03 AUC: 0.4547\n",
      "[19292] Reconstruction loss: 38.9216, VOL: 0.8668 SUM: 8.20 AUC: 0.4379\n",
      "[19504] Reconstruction loss: 40.1262, VOL: 0.8654 SUM: 8.14 AUC: 0.4442\n",
      "[19716] Reconstruction loss: 37.1629, VOL: 0.8641 SUM: 8.25 AUC: 0.4286\n",
      "[19928] Reconstruction loss: 38.5227, VOL: 0.8631 SUM: 8.21 AUC: 0.4193\n",
      "[20140] Reconstruction loss: 39.5438, VOL: 0.8618 SUM: 7.83 AUC: 0.4045\n",
      "[20352] Reconstruction loss: 38.7846, VOL: 0.8603 SUM: 8.12 AUC: 0.4326\n",
      "[20564] Reconstruction loss: 39.5096, VOL: 0.8596 SUM: 8.13 AUC: 0.4575\n",
      "[20776] Reconstruction loss: 38.6920, VOL: 0.8580 SUM: 8.20 AUC: 0.4274\n",
      "[20988] Reconstruction loss: 37.4178, VOL: 0.8566 SUM: 8.23 AUC: 0.4212\n",
      "[21200] Reconstruction loss: 37.3194, VOL: 0.8559 SUM: 8.18 AUC: 0.4129\n",
      "[21412] Reconstruction loss: 37.4396, VOL: 0.8549 SUM: 8.21 AUC: 0.4193\n",
      "[21624] Reconstruction loss: 39.2919, VOL: 0.8534 SUM: 8.33 AUC: 0.4325\n",
      "[21836] Reconstruction loss: 39.0795, VOL: 0.8519 SUM: 8.33 AUC: 0.4193\n",
      "[22048] Reconstruction loss: 40.6923, VOL: 0.8506 SUM: 8.22 AUC: 0.4418\n",
      "[22260] Reconstruction loss: 39.7256, VOL: 0.8496 SUM: 8.01 AUC: 0.4144\n",
      "[22472] Reconstruction loss: 39.2364, VOL: 0.8488 SUM: 8.35 AUC: 0.4321\n",
      "[22684] Reconstruction loss: 42.6358, VOL: 0.8467 SUM: 8.14 AUC: 0.4244\n",
      "[22896] Reconstruction loss: 37.6912, VOL: 0.8452 SUM: 8.24 AUC: 0.4213\n",
      "[23108] Reconstruction loss: 36.7292, VOL: 0.8438 SUM: 8.25 AUC: 0.4196\n",
      "[23320] Reconstruction loss: 40.1444, VOL: 0.8429 SUM: 8.12 AUC: 0.4308\n",
      "[23532] Reconstruction loss: 41.0183, VOL: 0.8417 SUM: 8.16 AUC: 0.4378\n",
      "[23744] Reconstruction loss: 38.1888, VOL: 0.8403 SUM: 8.28 AUC: 0.4261\n",
      "[23956] Reconstruction loss: 41.0695, VOL: 0.8390 SUM: 8.21 AUC: 0.4494\n",
      "[24168] Reconstruction loss: 37.0357, VOL: 0.8371 SUM: 8.24 AUC: 0.4266\n",
      "[24380] Reconstruction loss: 38.9355, VOL: 0.8359 SUM: 8.37 AUC: 0.4438\n",
      "[24592] Reconstruction loss: 39.4021, VOL: 0.8351 SUM: 8.22 AUC: 0.4476\n",
      "[24804] Reconstruction loss: 37.9641, VOL: 0.8340 SUM: 8.11 AUC: 0.3968\n",
      "[25016] Reconstruction loss: 39.2810, VOL: 0.8328 SUM: 8.28 AUC: 0.4242\n",
      "[25228] Reconstruction loss: 38.0299, VOL: 0.8314 SUM: 8.29 AUC: 0.4389\n",
      "[25440] Reconstruction loss: 39.0388, VOL: 0.8305 SUM: 8.28 AUC: 0.4236\n",
      "[25652] Reconstruction loss: 39.4919, VOL: 0.8297 SUM: 8.29 AUC: 0.4223\n",
      "[25864] Reconstruction loss: 37.9426, VOL: 0.8283 SUM: 8.21 AUC: 0.4074\n",
      "[26076] Reconstruction loss: 37.7076, VOL: 0.8270 SUM: 8.47 AUC: 0.4340\n",
      "[26288] Reconstruction loss: 38.3551, VOL: 0.8256 SUM: 8.34 AUC: 0.4292\n",
      "[26500] Reconstruction loss: 46.1259, VOL: 0.8247 SUM: 7.94 AUC: 0.4398\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs_ad):\n",
    "    prev_updates = train_ad(model, train_loader, optimizer_ad, sched_ad, prev_updates, epoch, writer=writer)\n",
    "    test(model, test_loader, prev_updates, epoch, writer=writer)\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "data = test_data[1][0].reshape(1,1,28,28).to(device)\n",
    "fz, mu, x_recon, dfz = model.forward(data)\n",
    "dfz - compute_ae_loss(data, x_recon) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.defuzzy[0].Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_eigenvals_positive_loss(model.encoder.fuzzy_a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализируем результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_activation_stats(model, dataloader):\n",
    "    rulestat = {}\n",
    "    with torch.no_grad():\n",
    "        for _, (data, _) in enumerate(tqdm(dataloader)):\n",
    "            data = data.to(device)\n",
    "            fz, mu, rec_x, dfz = model.forward(data)\n",
    "            act_fz = fz.max(-1).indices.cpu().numpy()\n",
    "            for ind in act_fz:\n",
    "                rulestat[ind] = rulestat.get(ind, 0) + 1\n",
    "    return rulestat\n",
    "\n",
    "train_stat = get_activation_stats(model, train_loader)\n",
    "test_stat = get_activation_stats(model, test_loader)\n",
    "\n",
    "plt.bar(list(train_stat.keys()), train_stat.values(), 0.5, color='g')\n",
    "plt.bar(list(test_stat.keys()), test_stat.values(), 0.5, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_activation_stats_by_digit(digit, model, dataloader):\n",
    "    rulestat = {}\n",
    "    with torch.no_grad():\n",
    "        for _, (data, target) in enumerate(tqdm(dataloader)):\n",
    "        \n",
    "            data = data.to(device)\n",
    "            fz, mu, rec_x, dfz = model.forward(data)\n",
    "            act_fz = fz.max(-1).indices.cpu().numpy()\n",
    "            for ind, trg in zip(act_fz, target):\n",
    "                if trg == digit:\n",
    "                    rulestat[ind] = rulestat.get(ind, 0) + 1\n",
    "    return rulestat\n",
    "\n",
    "\n",
    "test_stat_by_digit = get_activation_stats_by_digit(8, model, test_loader)\n",
    "\n",
    "plt.bar(list(test_stat_by_digit.keys()), test_stat_by_digit.values(), 0.5, color='r')\n",
    "plt.xlim((-1, fuzzy_rules_count + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stat_by_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    act_fz = torch.diag(torch.ones(fuzzy_rules_count)).to(device)\n",
    "    samples = model.encoder.defuzzy(act_fz)\n",
    "    img_idx = 0\n",
    "    fig, ax = plt.subplots(8, fuzzy_rules_count//8, figsize=(fuzzy_rules_count//8, 8))\n",
    "    for i in range(8):\n",
    "        for j in range(fuzzy_rules_count//8):\n",
    "            ax[i, j].imshow(samples[img_idx].view(28, 28).cpu().detach().numpy(), cmap='gray')\n",
    "            ax[i, j].axis('off')\n",
    "            img_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     z = torch.randn(64, fuzzy_rules_count).to(device)\n",
    "#     samples = model.decoder_pass(z)\n",
    "\n",
    "#     # Plot the generated images\n",
    "#     fig, ax = plt.subplots(8, 8, figsize=(8, 8))\n",
    "#     for i in range(8):\n",
    "#         for j in range(8):\n",
    "#             ax[i, j].imshow(samples[i*8+j].view(28, 28).cpu().detach().numpy(), cmap='gray')\n",
    "#             ax[i, j].axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arate(inp):\n",
    "    fz, mu, x_recon, dfz = model.forward(inp)\n",
    "    return (1 - ssim(x_recon, inp)).cpu().numpy()#fz.sum(-1).cpu().numpy() #xent_continuous_ber((x_recon + 1)/2, (inp + 1)/2).cpu().numpy()# #ssim((inp + 1)/2, (recon_x+1)/2).cpu().numpy() #fz.sum(-1).cpu().numpy()#(inp - x_recon).abs().sum(-1).sum(-1).mean(-1).cpu().numpy()#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firings_mnist = {}\n",
    "firings_mnist['MNIST'] = []\n",
    "firings_mnist['DISSIDENT'] = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader, desc='MNIST HIST'):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        rates = get_arate(data)\n",
    "        for f, l in  zip(rates, target):\n",
    "            if l != mnist_class_anomaly:\n",
    "                firings_mnist['MNIST'].append(f)\n",
    "            else:\n",
    "                firings_mnist['DISSIDENT'].append(f)\n",
    "        \n",
    "\n",
    "labels, data = firings_mnist.keys(), firings_mnist.values()\n",
    "\n",
    "fig = plt.figure(figsize =(12, 2))\n",
    "plt.boxplot(data, notch=True, showfliers=False)\n",
    "plt.xticks(range(1, len(labels) + 1), labels)\n",
    "plt.show()\n",
    "\n",
    "writer.add_figure('Anomaly Detection', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    firing_levels = []\n",
    "    lab_true = []\n",
    "    lab_pred = []\n",
    "\n",
    "    for data, lab in tqdm(test_loader, desc='Test MNIST', disable=True):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        rates = get_arate(data)\n",
    "        \n",
    "        for f, l in  zip(rates, lab):\n",
    "            firing_levels.append(f)\n",
    "            lab_pred.append(f)        \n",
    "            if l == mnist_class_anomaly:\n",
    "                lab_true.append(1)\n",
    "            else:\n",
    "                lab_true.append(0)\n",
    "                    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(lab_true, lab_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = threshold[optimal_idx]\n",
    "    fig = plt.figure(figsize =(4, 4))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    writer.add_figure('ROC', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def show_plot():\n",
    "    #centroids = model.encoder.fuzzy[0].get_centroids().detach().cpu().numpy()\n",
    "    embedings = []\n",
    "    labels_expected = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc='Encoding'):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            fz, mu, rec_x, dfz = model.forward(data)\n",
    "            embedings.append(mu.cpu().numpy())\n",
    "            labels_expected.append((target == mnist_class_anomaly).cpu().numpy())\n",
    "    embedings = np.concatenate(embedings, axis=0)\n",
    "    labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    R, C = 1, 3\n",
    "\n",
    "    plt.subplot(R, C, 1)\n",
    "    plt.title(\"MNIST XY\")\n",
    "    \n",
    "    plt.scatter(embedings[:, 0],      embedings[:,  1], c=labels_expected, cmap=binary_cmap, s=2)\n",
    "    #plt.scatter(centroids[:, 0],      centroids[:, 1], marker='1', c='black', s= 50)\n",
    "\n",
    "    plt.subplot(R, C, 2)\n",
    "    plt.title(\"MNIST XZ\")\n",
    "    plt.scatter(embedings[:, 0],      embedings[:,  2], c=labels_expected, cmap=binary_cmap, s=2)\n",
    "    #plt.scatter(centroids[:, 0],      centroids[:, 2], marker='1', c='black', s= 50)\n",
    "\n",
    "    \n",
    "show_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_item_reconstructio(ind):\n",
    "    for data, trg in iter(test_loader):\n",
    "        data = data.to(device)\n",
    "        fz, mu, rec_x, dfz = model.forward(data)\n",
    "        \n",
    "        plt.figure(figsize=(24, 6))\n",
    "\n",
    "        R, C = 1, 6\n",
    "\n",
    "        plt.subplot(R, C, 1)\n",
    "        plt.imshow(data[ind].cpu().squeeze())\n",
    "        plt.subplot(R, C, 2)\n",
    "        plt.imshow(rec_x[ind].detach().cpu().squeeze())\n",
    "        \n",
    "        plt.subplot(R, C, 3)\n",
    "        plt.imshow((rec_x[ind] - data[ind]).abs().detach().cpu().squeeze())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_item_reconstructio(4)\n",
    "show_item_reconstructio(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = optimal_threshold\n",
    "n = 0\n",
    "fig, ax = plt.subplots(10, 10, figsize=(10, 10))\n",
    "with torch.no_grad():\n",
    "    for data, labels in tqdm(test_loader, desc='EMNIST VIS'):\n",
    "        if n >= 100:\n",
    "            break\n",
    "        data = data.view((-1, 1, 28, 28)).to(device) \n",
    "        \n",
    "        arate = get_arate(data)\n",
    "        \n",
    "        for i in range(data.shape[0]):\n",
    "            if(arate[i] > threshold):\n",
    "                img = data[i]\n",
    "                ax[int(n / 10), int(n % 10)].imshow(img.view(28, 28).cpu().detach().numpy(), cmap='gray')\n",
    "                ax[int(n / 10), int(n % 10)].axis('off')\n",
    "                n = n + 1\n",
    "                    \n",
    "                if n >= 100:\n",
    "                    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
