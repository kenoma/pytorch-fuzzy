{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "from torchfuzzy import FuzzyLayer, DefuzzyLinearLayer, FuzzyBellLayer\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "nz = 3\n",
    "ngf = 16\n",
    "ndf = 32\n",
    "fuzzy_cores = 125\n",
    "\n",
    "niter = 500\n",
    "\n",
    "mnist_dissident = 8\n",
    "\n",
    "prefix = f\"fuzzy_gan_anomaly_detection\"\n",
    "writer = SummaryWriter(f'runs/mnist/{prefix}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(lambda x: x.view(-1, 28, 28) - 0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем обучающую выборку\n",
    "\n",
    "def get_target_and_mask(target_label):\n",
    "    t = target_label\n",
    "    return t \n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=True, \n",
    "    transform = transform,\n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "\n",
    "idx = (train_data.targets != mnist_dissident)\n",
    "train_data.targets = train_data.targets[idx]\n",
    "train_data.data = train_data.data[idx]\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем тестовую выборку\n",
    "test_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=transform, \n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем итераторы датасетов\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngf, nz, fuzzy_cores, nc=1):\n",
    "        super(Generator, self).__init__()\n",
    "        initial_centroids = []\n",
    "        initial_scales = []\n",
    "        sd = 5\n",
    "        exp_k = 3\n",
    "        linex = 0\n",
    "        linez = 0\n",
    "        for x in np.linspace(0.0, 1.0, num = sd):\n",
    "            linex += 1\n",
    "            for y in np.linspace(0.0, 1.0, num = sd):\n",
    "                linez+=1\n",
    "                for z in np.linspace(0.0, 1.0, num = sd):\n",
    "                    initial_centroids.append([exp_k*(x), exp_k*(y + (0.5/sd if linex%2 == 0 else 0)), exp_k*(z+(0.5/sd if linez%2 == 0 else 0))]) #\n",
    "                    initial_scales.append([exp_k, exp_k, exp_k])  \n",
    "\n",
    "        self.fuzzy = nn.Sequential(\n",
    "            FuzzyLayer.from_centers_and_scales(initial_centroids, initial_scales, trainable=False)\n",
    "        )\n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(     fuzzy_cores, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(    ngf,      nc, kernel_size=1, stride=1, padding=2, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        fz = self.fuzzy.forward(input)\n",
    "        output = self.main(fz.reshape((-1, fuzzy_cores, 1, 1)))\n",
    "        return output, fz\n",
    "\n",
    "netG = Generator(ngf, nz, fuzzy_cores).to(device)\n",
    "netG.apply(weights_init)\n",
    "num_params = sum(p.numel() for p in netG.parameters() if p.requires_grad)\n",
    "print(f'Number of parameters: {num_params:,}')\n",
    "\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_centroids = []\n",
    "initial_scales = []\n",
    "sd = 5\n",
    "exp_k = 3\n",
    "linex = 0\n",
    "linez = 0\n",
    "for x in np.linspace(0.0, 1.0, num = sd):\n",
    "    linex += 1\n",
    "    for y in np.linspace(0.0, 1.0, num = sd):\n",
    "        linez+=1\n",
    "        for z in np.linspace(0.0, 1.0, num = sd):\n",
    "            initial_centroids.append([exp_k*(x), exp_k*(y + (0.5/sd if linex%2 == 0 else 0)), exp_k*(z+(0.5/sd if linez%2 == 0 else 0))]) #\n",
    "            initial_scales.append([exp_k, exp_k, exp_k])  \n",
    "        \n",
    "fzl = FuzzyLayer.from_centers_and_scales(initial_centroids, initial_scales, trainable=False).to(device)\n",
    "\n",
    "xmin, xmax = -1, 2\n",
    "ymin, ymax = -1, 2\n",
    "szw = 500\n",
    "mesh = []\n",
    "for x in np.linspace(xmin, xmax, num=szw):\n",
    "    for y in np.linspace(ymin, ymax, num=szw):\n",
    "        mesh.append([x, y, 0.0])\n",
    "\n",
    "x = np.array([a[0] for a in mesh]).reshape((szw,szw))\n",
    "y = np.array([a[1] for a in mesh]).reshape((szw,szw))\n",
    "plt_z = np.array([a[2] for a in mesh]).reshape((szw,szw))\n",
    "inp = torch.FloatTensor(mesh).reshape((-1, 3)).to(device)\n",
    "fz = fzl(inp)\n",
    "\n",
    "z = fz.max(-1).values.squeeze().detach().cpu().numpy().reshape((szw,szw))\n",
    "\n",
    "z_min, z_max = z.min(), z.max()\n",
    "c = plt.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "plt.colorbar(c)\n",
    "plt.show()\n",
    "\n",
    "centroids = fzl.get_centroids().detach().cpu().numpy()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter([a[0] for a in centroids],[a[1] for a in centroids], [a[2] for a in centroids], c= [a[2] for a in centroids], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf, fuzzy_cores):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, ndf, 4, 2, 1, bias=False),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf * 2, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 2, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, ndf * 4, 3, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 3),\n",
    "        )\n",
    "        \n",
    "        initial_centroids = []\n",
    "        initial_scales = []\n",
    "        sd = 5\n",
    "        exp_k = 50\n",
    "        linex = 0\n",
    "        linez = 0\n",
    "        for x in np.linspace(-0.25, 0.25, num = sd):\n",
    "            linex += 1\n",
    "            for y in np.linspace(-0.25, 0.25, num = sd):\n",
    "                linez+=1\n",
    "                for z in np.linspace(-1.0, 1.0, num = sd):\n",
    "                    initial_centroids.append([exp_k*(x), exp_k*(y + (0.5/sd if linex%2 == 0 else 0)), exp_k*(z+(0.5/sd if linez%2 == 0 else 0))]) #\n",
    "                    initial_scales.append([exp_k, exp_k, exp_k])          #scale = 5\n",
    "        #initial_centroids = 5 * np.random.rand(fuzzy_cores, 2)\n",
    "        #initial_scales = 5 * np.ones((fuzzy_cores, 2))\n",
    "                \n",
    "        self.fuzzy = FuzzyLayer.from_centers_and_scales(initial_centroids, initial_scales, trainable=True)\n",
    "        \n",
    "        # initial_centroids_fake = []\n",
    "        # initial_scales_fake = []\n",
    "        # sd = 2\n",
    "        # exp_k = 1\n",
    "        # for x in np.linspace(-2, 2, num = sd):\n",
    "        #     for y in np.linspace(-2, 2, num = sd):\n",
    "        #         for z in np.linspace(-2, 2, num = sd):\n",
    "        #             initial_centroids_fake.append([exp_k*(x), exp_k*(y), exp_k*(z)])\n",
    "        #             initial_scales_fake.append([exp_k, exp_k, exp_k])  \n",
    "                    \n",
    "        # self.fake = FuzzyLayer.from_centers_and_scales(initial_centroids_fake, initial_scales_fake, trainable=True)\n",
    "        # #self.fake_defuzzy = DefuzzyLinearLayer.from_array(np.repeat(0.1, self.fake_size).reshape(1,-1), with_norm=False, trainable=False)\n",
    "        \n",
    "        self.defuzzy = nn.Sequential(\n",
    "            DefuzzyLinearLayer.from_array(np.repeat(1.0, fuzzy_cores).reshape(1,-1), with_norm=False, trainable=True)\n",
    "            \n",
    "            # nn.Linear(fuzzy_cores, fuzzy_cores//2),\n",
    "            # nn.BatchNorm1d(fuzzy_cores//2),\n",
    "            # nn.SiLU(),\n",
    "            # nn.Linear(fuzzy_cores//2, 1),\n",
    "            # nn.Tanh()\n",
    "            \n",
    "            #DefuzzyLinearLayer.from_array(np.repeat(0.5/fuzzy_cores, fuzzy_cores).reshape(1,-1), with_norm=False, trainable=False)#from_dimensions(fuzzy_cores, 3, trainable=True, with_norm=False),\n",
    "            #np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n",
    "            # nn.Linear(fuzzy_cores, 3, bias=False),\n",
    "            # FuzzyLayer.from_dimensions(3, 10),\n",
    "            # nn.BatchNorm1d(10),\n",
    "            #nn.Linear(fuzzy_cores, 1, bias=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        fz = self.fuzzy(output)\n",
    "        #fk = self.fake(output)\n",
    "        \n",
    "        r = self.defuzzy(fz)\n",
    "        #f = self.fake_defuzzy(fk)\n",
    "        \n",
    "        return r.squeeze(), fz, output\n",
    "    \n",
    "\n",
    "netD = Discriminator(ndf, fuzzy_cores).to(device)\n",
    "netD.apply(weights_init)\n",
    "num_params = sum(p.numel() for p in netD.parameters() if p.requires_grad)\n",
    "print(f'Number of parameters: {num_params:,}')\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(2, 1, 28, 28).to(device)\n",
    "dd = Discriminator(ndf, fuzzy_cores).to(device)\n",
    "dd(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=5e-5, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=5e-5, betas=(0.5, 0.999))\n",
    "\n",
    "fixed_noise_for_report = torch.rand(64, nz, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_eigenvals_positive_loss(layer, eps = 1e-15):\n",
    "    ev = layer.get_transformation_matrix_eigenvals().real.min()\n",
    "    ev = torch.clamp(ev, max=eps)\n",
    "    return -ev\n",
    "\n",
    "# def keep_eigenvals_at(layer, val = 1e-1):\n",
    "#     ev = layer.get_transformation_matrix_eigenvals().real.max()\n",
    "#     ev = torch.clamp(ev, min=val)\n",
    "#     return torch.square(val-ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_arate_distr(D):\n",
    "    with torch.no_grad():\n",
    "        firing_levels = []\n",
    "        lab_true = []\n",
    "        lab_pred = []\n",
    "\n",
    "        for data, lab in tqdm(test_loader, desc='Test MNIST', disable=True):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            rates, _, _ = D(data)\n",
    "            rates = rates.detach().cpu().numpy()\n",
    "            for f, l in  zip(rates, lab):\n",
    "                firing_levels.append(f)\n",
    "                lab_pred.append(f)        \n",
    "                if l == mnist_dissident:\n",
    "                    lab_true.append(0)\n",
    "                else:\n",
    "                    lab_true.append(1)\n",
    "                        \n",
    "        fpr, tpr, threshold = metrics.roc_curve(lab_true, lab_pred)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "        firing_levels = np.array(firing_levels)\n",
    "        return firing_levels, roc_auc, threshold\n",
    "\n",
    "def draw_embeddings(netD, netG, epoch):\n",
    "    with torch.no_grad():\n",
    "        centroids_real = netD.fuzzy.get_centroids().detach().cpu().numpy()\n",
    "        #centroids_fake = netD.fake.get_centroids().detach().cpu().numpy()\n",
    "        \n",
    "        embedings_test = []\n",
    "        labels_expected = []\n",
    "            \n",
    "        for data, target in tqdm(test_loader, desc='Encoding', disable=True):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            embeding = netD.main(data)\n",
    "            embedings_test.append(embeding.cpu().numpy())\n",
    "            labels_expected.append(target)\n",
    "        \n",
    "        embedings_test = np.concatenate(embedings_test, axis=0)\n",
    "        labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "        \n",
    "        embedings_fake = []\n",
    "\n",
    "        fixed_noise = torch.rand(49, nz)\n",
    "        if torch.cuda.is_available():\n",
    "            fixed_noise = fixed_noise.cuda()\n",
    "        fake_images, _ = netG(fixed_noise)\n",
    "        embeding = netD.main(fake_images)\n",
    "        embedings_fake.append(embeding.cpu().numpy())\n",
    "\n",
    "        embedings_fake = np.concatenate(embedings_fake, axis=0)    \n",
    "\n",
    "        fig = plt.figure(layout='constrained', figsize=(10, 4))\n",
    "        subfigs = fig.subfigures(1, 3, wspace=0.07)\n",
    "        axsLeft = subfigs[0].subplots(1, 1, sharey=True)\n",
    "\n",
    "        axsLeft.scatter(embedings_test[:, 0], embedings_test[:, 1], cmap='tab10', c=labels_expected, s=1)\n",
    "        axsLeft.scatter(embedings_fake[:, 0], embedings_fake[:, 1], c='black', marker='o', s=2)\n",
    "        axsLeft.scatter(centroids_real[:, 0], centroids_real[:, 1], marker='1', c='black', s= 30)\n",
    "        \n",
    "        axsLeft = subfigs[1].subplots(1, 1, sharey=True)\n",
    "        axsLeft.scatter(embedings_test[:, 0], embedings_test[:, 2], cmap='tab10', c=labels_expected, s=1)\n",
    "        axsLeft.scatter(embedings_fake[:, 0], embedings_fake[:, 2], c='black', marker='o', s=2)\n",
    "        \n",
    "        # ymin, ymax = axsLeft.get_ylim()\n",
    "        # xmin, xmax = axsLeft.get_xlim()\n",
    "        # szw = 100\n",
    "        # mesh = []\n",
    "        # for x in np.linspace(xmin, xmax, num=szw):\n",
    "        #     for y in np.linspace(ymin, ymax, num=szw):\n",
    "        #         mesh.append([x,y])\n",
    "\n",
    "        # x = np.array([a[0] for a in mesh]).reshape((szw,szw))\n",
    "        # y = np.array([a[1] for a in mesh]).reshape((szw,szw))\n",
    "        # inp = torch.FloatTensor(mesh).reshape((-1, 2)).to(device)\n",
    "        # fz = netD.fuzzy(inp)\n",
    "        # #fk = netD.fake(inp)\n",
    "        # r = netD.defuzzy(fz)\n",
    "       \n",
    "        # z = r.squeeze().detach().cpu().numpy().reshape((szw,szw))\n",
    "        \n",
    "        # z_min, z_max = z.min(), z.max()\n",
    "        # c = axsLeft.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "        # c.set_zorder(-1)\n",
    "        # fig.colorbar(c, ax=axsLeft)\n",
    "\n",
    "        fake_images_np = fake_images.cpu().detach().numpy()\n",
    "        fake_images_np = fake_images_np.reshape(fake_images_np.shape[0], 28, 28)\n",
    "        axsRight = subfigs[2].subplots(7, 7, sharey=True)\n",
    "        i = 0\n",
    "        for x in axsRight:\n",
    "            for y in x:\n",
    "                y.imshow(fake_images_np[i], cmap='gray')\n",
    "                y.axis('off')\n",
    "                i += 1\n",
    "        plt.show()\n",
    "        writer.add_figure('Embeddings', fig, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG.train()\n",
    "netD.train()\n",
    "\n",
    "for epoch in range(niter):\n",
    "    report_aver_pos = 0\n",
    "    report_aver_neg = 0\n",
    "    report_loss_G = 0\n",
    "    report_ev = 0\n",
    "    report_fz = 0\n",
    "    local_count = 0\n",
    "    \n",
    "    for i, data in enumerate(tqdm(train_loader, desc='Training', disable=True)):\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        netD.zero_grad()\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        noise = torch.rand(batch_size, nz, device=device)\n",
    "        fake, _ = netG(noise)\n",
    "        \n",
    "        firing_r, _, _ = netD(real_cpu)\n",
    "        limit_real = (1 + 0.1 * (1 - 2*torch.rand(batch_size))).to(device)\n",
    "        errD_real = torch.square(limit_real - firing_r).mean() \n",
    "        errD_real.backward()\n",
    "        \n",
    "        nfiring_r, _, _ = netD(fake.detach())\n",
    "        limit_fake = (0.1 + 0.1 * (1 - 2 * torch.rand(batch_size))).to(device)\n",
    "        errD_fake = torch.square(limit_fake - nfiring_r).mean() \n",
    "        \n",
    "        ev_loss = keep_eigenvals_positive_loss(netD.fuzzy)\n",
    "        errD_fake.backward(retain_graph=True)\n",
    "        ev_loss.backward()\n",
    "        \n",
    "        report_ev = np.maximum(report_ev, ev_loss.item())\n",
    "        \n",
    "        optimizerD.step()\n",
    "        \n",
    "        genr, _, _ = netD(fake)\n",
    "        limit_g = (1 + 0.1 * (1 - 2*torch.rand(batch_size))).to(device)\n",
    "        errG = torch.square(limit_g - genr).mean()\n",
    "        errG.backward()\n",
    "        \n",
    "        optimizerG.step()\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        noise = torch.rand(batch_size, nz, device=device)\n",
    "        fake, fz_g = netG(noise)\n",
    "        _, fz_d, _ = netD(fake)\n",
    "        fz_diff_loss = torch.norm(fz_g - fz_d, dim=-1).mean()\n",
    "        fz_diff_loss.backward()\n",
    "        \n",
    "        optimizerD.step()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        local_count += 1\n",
    "        #report_evmax += ev_max_loss.mean().item()\n",
    "        report_loss_G += errG.item()\n",
    "        report_aver_pos += firing_r.mean().item()\n",
    "        report_aver_neg += nfiring_r.mean().item()\n",
    "        report_fz += fz_diff_loss.item()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        losses = {}\n",
    "        \n",
    "        losses['G'] = report_loss_G / local_count\n",
    "        losses['POS'] = report_aver_pos / local_count\n",
    "        losses['NEG'] = report_aver_neg / local_count\n",
    "        losses['EV'] = report_ev\n",
    "        losses['FZ'] = report_fz / local_count\n",
    "        #losses['EVMAX'] = report_evmax / local_count\n",
    "        \n",
    "        \n",
    "        writer.add_scalars('Loss', losses, epoch)\n",
    "        draw_embeddings(netD, netG, epoch)    \n",
    "        \n",
    "        #mnist_distr, auc, _ = get_test_arate_distr(netD)\n",
    "        #mnist_distr_q = {}\n",
    "        #writer.add_scalars(\"MNIST test  firings\", mnist_distr_q, epoch)\n",
    "        #writer.add_scalar(\"AUC\", auc, epoch)\n",
    "        #losses[\"auc\"] = auc\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{niter} {losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netD.state_dict(), 'weights/netD.pth')\n",
    "torch.save(netG.state_dict(), 'weights/netG.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator(ndf, fuzzy_cores ).to(device)\n",
    "G = Generator(ngf, nz, fuzzy_cores).to(device)\n",
    "D.load_state_dict(torch.load('weights/netD.pth'))\n",
    "G.load_state_dict(torch.load('weights/netG.pth'))\n",
    "#DO NOT USE EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_embeddings(D, G, niter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firings, auc, threshold = get_test_arate_distr(D)\n",
    "print(F\"Average firing {firings.mean()}\")\n",
    "print(F\"AUC {auc} Threshold {threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     R, C = 10, 10\n",
    "#     x = np.linspace(0.0, 1.0, R*C)\n",
    "#     y = 0.3*x+0.7\n",
    "\n",
    "#     noise = torch.FloatTensor(np.dstack((x,y))).reshape((-1,nz)).to(device)\n",
    "#     fake_images, _ = G(noise)\n",
    "#     fake_images_np = fake_images.cpu().detach().numpy()\n",
    "#     fake_images_np = fake_images_np.reshape(fake_images_np.shape[0], 28, 28)\n",
    "    \n",
    "#     embedings = D.main(fake_images).cpu().numpy()\n",
    "        \n",
    "    \n",
    "#     for i in range(R*C):\n",
    "#         plt.subplot(R, C, i + 1)\n",
    "#         plt.imshow(fake_images_np[i], cmap='gray')\n",
    "#     plt.show()\n",
    "#     plt.scatter(embedings[:,0],embedings[:,1], s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fixed_noise = torch.rand(49, nz)\n",
    "    if torch.cuda.is_available():\n",
    "        fixed_noise = fixed_noise.cuda()\n",
    "    fake_images,_ = G(fixed_noise)\n",
    "\n",
    "    fake_images_np = fake_images.cpu().detach().numpy()\n",
    "    fake_images_np = fake_images_np.reshape(fake_images_np.shape[0], 28, 28)\n",
    "    R, C = 7, 7\n",
    "    for i in range(49):\n",
    "        plt.subplot(R, C, i + 1)\n",
    "        plt.imshow(fake_images_np[i], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arate(inp):\n",
    "    rd, fz_d, embd1 = D(inp)\n",
    "    generated_image = G.main(fz_d.reshape((-1, fuzzy_cores, 1, 1)))\n",
    "    rg, fz_g, embd2 = D(generated_image)\n",
    "    \n",
    "    \n",
    "    return nn.functional.cosine_similarity(fz_d, fz_g, dim=-1).detach().cpu().numpy() #torch.norm(fz_d - fz_g, dim=-1).detach().cpu().numpy()\n",
    "\n",
    "inp = torch.rand((2,1,28,28))\n",
    "get_arate(inp.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_r = D.fuzzy.get_centroids().detach().cpu().numpy()\n",
    "centroids_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings_fake = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_size = 256\n",
    "    latent_size = nz\n",
    "    \n",
    "    fixed_noise = torch.rand(batch_size, latent_size)\n",
    "    if torch.cuda.is_available():\n",
    "        fixed_noise = fixed_noise.cuda()\n",
    "    fake_images, _ = G(fixed_noise)\n",
    "    embeding = D.main(fake_images)\n",
    "    embedings_fake.append(embeding.cpu().numpy())\n",
    "\n",
    "embedings_fake = np.concatenate(embedings_fake, axis=0)    \n",
    "\n",
    "embedings = []\n",
    "labels_expected = []\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader, desc='Encoding'):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        embeding = D.main(data)\n",
    "        embedings.append(embeding.cpu().numpy())\n",
    "        labels_expected.append(target.cpu().numpy())\n",
    "embedings = np.concatenate(embedings, axis=0)\n",
    "labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "R, C = 1, 2\n",
    "\n",
    "plt.subplot(R, C, 1)\n",
    "plt.title(\"MNIST\")\n",
    "plt.scatter(embedings_fake[:, 0], embedings_fake[:, 1], c='red', marker='+', s=4)\n",
    "plt.scatter(embedings[:, 0],      embedings[:,  1], c=labels_expected, cmap='tab10', s=2)\n",
    "plt.scatter(centroids_r[:, 0],    centroids_r[:,1], marker='1', c='blue', s= 120)\n",
    "xmin, xmax = plt.xlim()\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.subplot(R, C, 2)\n",
    "plt.title(\"EMNIST\")\n",
    "plt.scatter(embedings_fake[:, 0], embedings_fake[:, 1], c='red', marker='+', s=4)\n",
    "plt.scatter(centroids_r[:, 0], centroids_r[:, 1], marker='1', c='blue', s= 120)\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G.fuzzy.get_centroids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.rand(2, nz)\n",
    "if torch.cuda.is_available():\n",
    "    fixed_noise = fixed_noise.cuda()\n",
    "fake_images,fzg = G(fixed_noise)\n",
    "\n",
    "_, fzd,_ = D(fake_images)\n",
    "fake_images_np = fake_images.cpu().detach().numpy()\n",
    "fake_images_np = fake_images_np.reshape(fake_images_np.shape[0], 28, 28)\n",
    "plt.imshow(fake_images_np[0], cmap='gray')\n",
    "plt.show()\n",
    "plt.plot(fzd[0].detach().cpu().numpy(), c=\"red\")\n",
    "plt.plot(fzg[0].detach().cpu().numpy(), c=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_loader.dataset[4][0].view((-1, 1, 28, 28)).to(device)\n",
    "img  = torch.cat((img,img), dim=0)\n",
    "print(img.shape)\n",
    "plt.imshow(img[0][0].detach().cpu().numpy())\n",
    "plt.show()\n",
    "r, fzd, _ = D(img)\n",
    "\n",
    "gimg = G.main(fzd.reshape((2, fuzzy_cores, 1, 1)))\n",
    "_, fzg, _ = D(gimg)\n",
    "\n",
    "plt.imshow(gimg[0][0].detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.plot(fzd[0].detach().cpu().numpy(), c=\"red\")\n",
    "plt.plot(fzg[0].detach().cpu().numpy(), c=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firings_mnist = {}\n",
    "firings_mnist['MNIST'] = []\n",
    "firings_mnist['DISSIDENT'] = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader, desc='MNIST HIST'):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        rates = get_arate(data)\n",
    "        for f, l in  zip(rates, target):\n",
    "            if l == mnist_dissident:\n",
    "                firings_mnist['MNIST'].append(f)\n",
    "            else:\n",
    "                firings_mnist['DISSIDENT'].append(f)\n",
    "        \n",
    "\n",
    "labels, data = firings_mnist.keys(), firings_mnist.values()\n",
    "\n",
    "fig = plt.figure(figsize =(12, 2))\n",
    "plt.boxplot(data, notch=True, showfliers=False)\n",
    "plt.xticks(range(1, len(labels) + 1), labels)\n",
    "plt.show()\n",
    "\n",
    "writer.add_figure('Anomaly Detection', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    firing_levels = []\n",
    "    lab_true = []\n",
    "    lab_pred = []\n",
    "\n",
    "    for data, lab in tqdm(test_loader, desc='Test MNIST', disable=True):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        rates = get_arate(data)\n",
    "        \n",
    "        for f, l in  zip(rates, lab):\n",
    "            firing_levels.append(f)\n",
    "            lab_pred.append(f)        \n",
    "            if l == mnist_dissident:\n",
    "                lab_true.append(0)\n",
    "            else:\n",
    "                lab_true.append(1)\n",
    "                    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(lab_true, lab_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    fig = plt.figure(figsize =(4, 4))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    writer.add_figure('ROC', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.0\n",
    "n = 0\n",
    "fig, ax = plt.subplots(10, 10, figsize=(10, 10))\n",
    "with torch.no_grad():\n",
    "    for data, labels in tqdm(test_loader, desc='EMNIST VIS'):\n",
    "        data = data.view((-1, 1, 28, 28)).to(device) \n",
    "        \n",
    "        arate = get_arate(data)\n",
    "        \n",
    "        winner = arate.argmax()\n",
    "        if(arate[winner] > threshold):\n",
    "            img = data[winner]\n",
    "            ax[int(n / 10), int(n % 10)].imshow(img.view(28, 28).cpu().detach().numpy(), cmap='gray')\n",
    "            ax[int(n / 10), int(n % 10)].axis('off')\n",
    "            n = n + 1\n",
    "                \n",
    "            if n == 100:\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
