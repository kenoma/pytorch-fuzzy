{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Детектор аномалии на принципе многократной прогонки реконструкции входного образца до сходимости латентного вектора.\n",
    "Критерий аномальности - расстояние от первоначального латентного вектора до сошедшего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "import piqa\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd() + '/..')\n",
    "from torchfuzzy import FuzzyLayer, DefuzzyLinearLayer, FuzzyBellLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 2e-3\n",
    "num_epochs = 300\n",
    "\n",
    "latent_dim = 16\n",
    "#terms = 10\n",
    "kernels = 2\n",
    "\n",
    "prefix = f\"fuzzy_classifier\"\n",
    "writer = SummaryWriter(f'runs/mnist/{prefix}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ssim = piqa.SSIM(window_size = 11, n_channels=1, reduction='none').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Датасет\n",
    "\n",
    "1. Исключаем класс аномалии `mnist_class_anomaly` из общей выборк\n",
    "2. Убираем метки с остальных классов\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_and_transform(x):\n",
    "    nimg = 2.0*(x.view(-1, 28, 28) - 0.5)\n",
    "    nimg = torch.clamp(nimg, -1, 1)\n",
    "    return nimg\n",
    "\n",
    "def clamp(x):\n",
    "    #nimg = 2.0*(x.view(-1, 28, 28) - 0.5)\n",
    "    nimg = torch.clamp(x, -1, 1)\n",
    "    return nimg\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    #transforms.RandomCrop(size=25),\n",
    "    #transforms.Resize(size=(28, 28)),\n",
    "    #transforms.Lambda(norm_and_transform)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Lambda(norm_and_transform)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target_and_mask(target_label):\n",
    "    t = F.one_hot(torch.LongTensor([target_label]), 10)\n",
    "    return t.squeeze().to(torch.float)\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=True, \n",
    "    transform = train_transform,\n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "загружаем тестовую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=test_transform, \n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем итераторы датасетов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    \n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data, t in iter(train_loader):\n",
    "    plt.imshow(data[0].squeeze().cpu())\n",
    "    print(t.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0139, 0.0089, 0.0092, 0.0081, 0.0120, 0.0443, 0.0266, 0.0049, 0.0078,\n",
       "         0.0066],\n",
       "        [0.0173, 0.0059, 0.0109, 0.0097, 0.0162, 0.0323, 0.0293, 0.0063, 0.0068,\n",
       "         0.0123],\n",
       "        [0.0095, 0.0025, 0.0144, 0.0100, 0.0110, 0.0289, 0.0311, 0.0080, 0.0056,\n",
       "         0.0104],\n",
       "        [0.0177, 0.0085, 0.0106, 0.0075, 0.0152, 0.0432, 0.0286, 0.0060, 0.0082,\n",
       "         0.0076],\n",
       "        [0.0259, 0.0065, 0.0118, 0.0108, 0.0186, 0.0335, 0.0332, 0.0051, 0.0086,\n",
       "         0.0075]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FuzzyClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        latent_dim (int): Размер латентного вектора.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, terms):\n",
    "        super(FuzzyClassifier, self).__init__()\n",
    "                \n",
    "        self.input = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels, kernel_size=7, padding=6, stride=1),\n",
    "            nn.BatchNorm2d(kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "        )\n",
    "\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(kernels, 2*kernels, kernel_size=2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(2*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(2*kernels, 2*kernels, kernel_size=2, stride=1, padding =0),\n",
    "            nn.BatchNorm2d(2*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            nn.MaxPool2d(2, stride = 2),\n",
    "        )\n",
    "\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv2d(2*kernels, 4*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(4*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(4*kernels, 4*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(4*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            #nn.MaxPool2d(2, stride = 2),\n",
    "        )\n",
    "\n",
    "        self.block_4 = nn.Sequential(\n",
    "            nn.Conv2d(4*kernels, 8*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(8*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(8*kernels, 8*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(8*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            #nn.MaxPool2d(2, stride = 2),\n",
    "        )\n",
    "\n",
    "        self.block_5 = nn.Sequential(\n",
    "            nn.Conv2d(8*kernels, 16*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(16*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(16*kernels, 16*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(16*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            #nn.MaxPool2d(2, stride = 2),\n",
    "        )\n",
    "\n",
    "        self.block_6 = nn.Sequential(\n",
    "            nn.Conv2d(16*kernels, 32*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(32*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(32*kernels, 32*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(32*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            #nn.MaxPool2d(2, stride = 2),\n",
    "        )\n",
    "\n",
    "        self.block_7 = nn.Sequential(\n",
    "            nn.Conv2d(32*kernels, 64*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(64*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(64*kernels, 64*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(64*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            #nn.MaxPool2d(2, stride = 2),\n",
    "        )\n",
    "\n",
    "        self.block_8 = nn.Sequential(\n",
    "            nn.Conv2d(64*kernels, 128*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(128*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(128*kernels, 128*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(128*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            #nn.MaxPool2d(2, stride = 2),\n",
    "        )\n",
    "\n",
    "        self.block_9 = nn.Sequential(\n",
    "            nn.Conv2d(128*kernels, 256*kernels, kernel_size = 2, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(256*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            nn.Conv2d(256*kernels, 256*kernels, kernel_size = 3, stride=1, padding = 0),\n",
    "            nn.BatchNorm2d(256*kernels, track_running_stats=False),\n",
    "            nn.SiLU(),  \n",
    "            #nn.MaxPool2d(2, stride = 2),\n",
    "        )\n",
    "\n",
    "        self.prefuzzy = nn.Sequential(\n",
    "            #nn.AvgPool2d(kernel_size = 3, stride = 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*kernels, latent_dim),\n",
    "        )\n",
    "\n",
    "        #terms_list = []\n",
    "        #for d in range(10):\n",
    "        #    terms_list.append(nn.Sequential(\n",
    "        #        FuzzyBellLayer.from_dimensions(latent_dim, terms, trainable=True)))\n",
    "\n",
    "        self.fuzzy = nn.Sequential(\n",
    "            FuzzyLayer.from_dimensions(latent_dim, 10, trainable=True)) #nn.ParameterList(terms_list)\n",
    "\n",
    "        # self.out = nn.Sequential(\n",
    "        #     nn.Linear(10*terms, 10)\n",
    "        # )\n",
    "        \n",
    "        self.downscale_1 = nn.Sequential(nn.Conv2d(  kernels, 2*kernels, kernel_size=19))\n",
    "        self.downscale_2 = nn.Sequential(nn.Conv2d(2*kernels, 4*kernels, kernel_size=3))\n",
    "        self.downscale_3 = nn.Sequential(nn.Conv2d(4*kernels, 8*kernels, kernel_size=3))\n",
    "        self.downscale_4 = nn.Sequential(nn.Conv2d(8*kernels, 16*kernels, kernel_size=3))\n",
    "        self.downscale_5 = nn.Sequential(nn.Conv2d(16*kernels, 32*kernels, kernel_size=3))\n",
    "        self.downscale_6 = nn.Sequential(nn.Conv2d(32*kernels, 64*kernels, kernel_size=3))\n",
    "        self.downscale_7 = nn.Sequential(nn.Conv2d(64*kernels, 128*kernels, kernel_size=3))\n",
    "        self.downscale_8 = nn.Sequential(nn.Conv2d(128*kernels, 256*kernels, kernel_size=4))\n",
    "        self.after_sum = nn.SiLU()\n",
    "        \n",
    "\n",
    "         \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Выход энкодера для чистого VAE.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Входной вектор.\n",
    "            eps (float): Небольшая поправка к скейлу для лучшей сходимости и устойчивости.\n",
    "        \n",
    "        Returns:\n",
    "            mu, logvar, z, dist\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.input(x)\n",
    "        #print(x.shape)\n",
    "        res_1 = self.downscale_1(x)\n",
    "        #print(res_1.shape)\n",
    "        \n",
    "\n",
    "        x = self.block_2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.after_sum(x+res_1)\n",
    "        res_2 = self.downscale_2(x)\n",
    "        #print(res_2.shape)\n",
    "        \n",
    "        x = self.block_3(x)\n",
    "        #print(x.shape)\n",
    "        x = self.after_sum(x+res_2)\n",
    "        res_3 = self.downscale_3(x)\n",
    "        #print(res_3.shape)\n",
    "        \n",
    "        x = self.block_4(x)\n",
    "        #print(x.shape)\n",
    "        x = self.after_sum(x+res_3)\n",
    "        res_4 = self.downscale_4(x)\n",
    "        #print(res_4.shape)\n",
    "\n",
    "        x = self.block_5(x)\n",
    "        #print(x.shape)\n",
    "        x = self.after_sum(x + res_4)\n",
    "        res_5 = self.downscale_5(x)\n",
    "        #print(res_5.shape)\n",
    "\n",
    "        x = self.block_6(x)\n",
    "        #print(x.shape)\n",
    "        x = self.after_sum(x + res_5)\n",
    "        res_6 = self.downscale_6(x)\n",
    "        #print(res_6.shape)\n",
    "\n",
    "        x = self.block_7(x)\n",
    "        #print(x.shape)\n",
    "        x = self.after_sum(x + res_6)\n",
    "        res_7 = self.downscale_7(x)\n",
    "        #print(res_7.shape)\n",
    "\n",
    "        x = self.block_8(x)\n",
    "        #print(x.shape)\n",
    "        x = self.after_sum(x + res_7)\n",
    "        res_8 = self.downscale_8(x)\n",
    "        #print(res_8.shape)\n",
    "\n",
    "        x = self.block_9(x)\n",
    "        #print(x.shape)\n",
    "        x = self.after_sum(x + res_8)\n",
    "\n",
    "        x = self.prefuzzy(x)# 7\n",
    "        #chunks = x.split(latent_dim, dim=-1)\n",
    "        #acts = []\n",
    "\n",
    "        #for d in range(10):\n",
    "        #    acts.append(self.fuzzy[d](chunks[d]))\n",
    "\n",
    "        #x = torch.stack(acts, 1).squeeze().flatten(1)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.fuzzy(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "inp = torch.rand(5, 1, 28, 28)\n",
    "m = FuzzyClassifier(latent_dim, 10)\n",
    "mu = m.forward(inp)\n",
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5,917,972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FuzzyClassifier(\n",
       "  (input): Sequential(\n",
       "    (0): Conv2d(1, 2, kernel_size=(7, 7), stride=(1, 1), padding=(6, 6))\n",
       "    (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): SiLU()\n",
       "  )\n",
       "  (block_2): Sequential(\n",
       "    (0): Conv2d(2, 4, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): SiLU()\n",
       "    (3): Conv2d(4, 4, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (5): SiLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block_3): Sequential(\n",
       "    (0): Conv2d(4, 8, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): SiLU()\n",
       "    (3): Conv2d(8, 8, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (5): SiLU()\n",
       "  )\n",
       "  (block_4): Sequential(\n",
       "    (0): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): SiLU()\n",
       "    (3): Conv2d(16, 16, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (5): SiLU()\n",
       "  )\n",
       "  (block_5): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): SiLU()\n",
       "    (3): Conv2d(32, 32, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (5): SiLU()\n",
       "  )\n",
       "  (block_6): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): SiLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (5): SiLU()\n",
       "  )\n",
       "  (block_7): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): SiLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (5): SiLU()\n",
       "  )\n",
       "  (block_8): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): SiLU()\n",
       "    (3): Conv2d(256, 256, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (5): SiLU()\n",
       "  )\n",
       "  (block_9): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (2): SiLU()\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (5): SiLU()\n",
       "  )\n",
       "  (prefuzzy): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=512, out_features=16, bias=True)\n",
       "  )\n",
       "  (fuzzy): Sequential(\n",
       "    (0): FuzzyLayer(\n",
       "      (rots): ParameterList(\n",
       "          (0): Parameter containing: [torch.float32 of size 10x15 (cuda:0)]\n",
       "          (1): Parameter containing: [torch.float32 of size 10x14 (cuda:0)]\n",
       "          (2): Parameter containing: [torch.float32 of size 10x13 (cuda:0)]\n",
       "          (3): Parameter containing: [torch.float32 of size 10x12 (cuda:0)]\n",
       "          (4): Parameter containing: [torch.float32 of size 10x11 (cuda:0)]\n",
       "          (5): Parameter containing: [torch.float32 of size 10x10 (cuda:0)]\n",
       "          (6): Parameter containing: [torch.float32 of size 10x9 (cuda:0)]\n",
       "          (7): Parameter containing: [torch.float32 of size 10x8 (cuda:0)]\n",
       "          (8): Parameter containing: [torch.float32 of size 10x7 (cuda:0)]\n",
       "          (9): Parameter containing: [torch.float32 of size 10x6 (cuda:0)]\n",
       "          (10): Parameter containing: [torch.float32 of size 10x5 (cuda:0)]\n",
       "          (11): Parameter containing: [torch.float32 of size 10x4 (cuda:0)]\n",
       "          (12): Parameter containing: [torch.float32 of size 10x3 (cuda:0)]\n",
       "          (13): Parameter containing: [torch.float32 of size 10x2 (cuda:0)]\n",
       "          (14): Parameter containing: [torch.float32 of size 10x1 (cuda:0)]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (downscale_1): Sequential(\n",
       "    (0): Conv2d(2, 4, kernel_size=(19, 19), stride=(1, 1))\n",
       "  )\n",
       "  (downscale_2): Sequential(\n",
       "    (0): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (downscale_3): Sequential(\n",
       "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (downscale_4): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (downscale_5): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (downscale_6): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (downscale_7): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (downscale_8): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "  )\n",
       "  (after_sum): SiLU()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FuzzyClassifier(latent_dim=latent_dim, terms=10).to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of parameters: {num_params:,}')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss(reduction='sum')\n",
    "def compute_classification_loss(fz, target):\n",
    "    return criterion(fz, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_eigenvals_positive_loss(layer, eps = 1e-15):\n",
    "    ev = layer.get_transformation_matrix_eigenvals().real.min()\n",
    "    ev = torch.clamp(ev, max=eps)\n",
    "    return -ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fshape_loss(fuzzy_layer):\n",
    "    \n",
    "    eigens = fuzzy_layer.get_transformation_matrix_eigenvals().real\n",
    "    fz_volume = (0.1 - (eigens.min(-1).values/eigens.max(-1).values).clamp(max=0.1)).square()\n",
    "    fz_volume = fz_volume.mean()\n",
    "\n",
    "    return fz_volume\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, prev_updates, epoch, writer=None):\n",
    "    model.train()  \n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(tqdm(dataloader, disable=True)):\n",
    "        n_upd = prev_updates + batch_idx\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "\n",
    "        pred = model.forward(data)  \n",
    "\n",
    "        loss = compute_classification_loss(pred, target)\n",
    "\n",
    "        ev_loss = keep_eigenvals_positive_loss(model.fuzzy[0])\n",
    "        if ev_loss.item() > 0:\n",
    "            ev_loss.backward(retain_graph=True)\n",
    "        #fzloss =  compute_fshape_loss(model.fuzzy[0])\n",
    "        #fzloss.backward(retain_graph=True)\n",
    "        \n",
    "        # for fd in range(10):\n",
    "        #     fzloss =  compute_fshape_loss(model.fuzzy[fd][0])\n",
    "        #     fzloss.backward(retain_graph=True)\n",
    "        #     ev_loss = keep_eigenvals_positive_loss(model.fuzzy[fd][0])\n",
    "        #     if ev_loss.item() > 0:\n",
    "        #         ev_loss.backward(retain_graph=True)\n",
    "        \n",
    "        loss.backward() \n",
    "        \n",
    "        optimizer.step()  \n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)    \n",
    "        \n",
    "        \n",
    "    return prev_updates + len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, cur_step, epoch, writer=None):\n",
    "    model.eval() \n",
    "    loss_accuracy = 0\n",
    "    loss_count = 0\n",
    "    total_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(dataloader, desc='Testing', disable=True):\n",
    "            target = target.to(device)\n",
    "            data = data.to(device)\n",
    "            pred = model.forward(data)  \n",
    "            pred_lab = pred.max(-1).indices.squeeze().cpu()\n",
    "            exp_lab = target.max(-1).indices.squeeze().cpu()\n",
    "\n",
    "            loss_accuracy += (pred_lab == exp_lab).float().sum()\n",
    "            loss_count += pred_lab.shape[0]\n",
    "\n",
    "            total_loss.append(compute_classification_loss(pred, target).item())\n",
    "\n",
    "    loss_accuracy /= loss_count\n",
    "    print(f'[{cur_step}] Accuracy : {loss_accuracy:.4f} Loss {np.mean(total_loss):4f} ')\n",
    "    \n",
    "    if writer is not None:\n",
    "        writer.add_scalar('FCL/Accuracy', loss_accuracy, global_step=cur_step)\n",
    "        writer.add_scalar('FCL/Loss', np.mean(total_loss), global_step=cur_step)\n",
    "    return loss_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_updates = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[469] Accuracy : 0.9828 Loss 48.634751 \n",
      "[938] Accuracy : 0.9882 Loss 29.288891 \n",
      "[1407] Accuracy : 0.9866 Loss 30.906555 \n",
      "[1876] Accuracy : 0.9900 Loss 25.748421 \n",
      "[2345] Accuracy : 0.9901 Loss 23.237551 \n",
      "[2814] Accuracy : 0.9910 Loss 24.446680 \n",
      "[3283] Accuracy : 0.9912 Loss 19.265895 \n",
      "[3752] Accuracy : 0.9894 Loss 24.202596 \n",
      "[4221] Accuracy : 0.9909 Loss 22.167453 \n",
      "[4690] Accuracy : 0.9905 Loss 19.322405 \n",
      "[5159] Accuracy : 0.9908 Loss 21.763817 \n",
      "[5628] Accuracy : 0.9921 Loss 18.941500 \n",
      "[6097] Accuracy : 0.9933 Loss 13.757289 \n",
      "[6566] Accuracy : 0.9922 Loss 14.191890 \n",
      "[7035] Accuracy : 0.9905 Loss 17.693182 \n",
      "[7504] Accuracy : 0.9907 Loss 19.947558 \n",
      "[7973] Accuracy : 0.9914 Loss 14.342822 \n",
      "[8442] Accuracy : 0.9917 Loss 14.560203 \n",
      "[8911] Accuracy : 0.9916 Loss 13.866758 \n",
      "[9380] Accuracy : 0.9905 Loss 16.040753 \n",
      "[9849] Accuracy : 0.9919 Loss 16.716242 \n",
      "[10318] Accuracy : 0.9896 Loss 15.554173 \n",
      "[10787] Accuracy : 0.9929 Loss 13.755700 \n",
      "[11256] Accuracy : 0.9936 Loss 12.783533 \n",
      "[11725] Accuracy : 0.9917 Loss 13.776212 \n",
      "[12194] Accuracy : 0.9927 Loss 12.954484 \n",
      "[12663] Accuracy : 0.9930 Loss 11.244510 \n",
      "[13132] Accuracy : 0.9929 Loss 11.561419 \n",
      "[13601] Accuracy : 0.9943 Loss 10.604402 \n",
      "[14070] Accuracy : 0.9926 Loss 13.227363 \n",
      "[14539] Accuracy : 0.9908 Loss 14.072824 \n",
      "[15008] Accuracy : 0.9892 Loss 15.567134 \n",
      "[15477] Accuracy : 0.9937 Loss 12.066995 \n",
      "[15946] Accuracy : 0.9899 Loss 15.201450 \n",
      "[16415] Accuracy : 0.9943 Loss 10.217238 \n",
      "[16884] Accuracy : 0.9932 Loss 10.749881 \n",
      "[17353] Accuracy : 0.9941 Loss 9.526084 \n",
      "[17822] Accuracy : 0.9931 Loss 10.896857 \n",
      "[18291] Accuracy : 0.9940 Loss 10.733886 \n",
      "[18760] Accuracy : 0.9912 Loss 11.665804 \n",
      "[19229] Accuracy : 0.9913 Loss 13.785959 \n",
      "[19698] Accuracy : 0.9902 Loss 14.537840 \n",
      "[20167] Accuracy : 0.9947 Loss 9.415347 \n",
      "[20636] Accuracy : 0.9927 Loss 10.098099 \n",
      "[21105] Accuracy : 0.9913 Loss 12.709834 \n",
      "[21574] Accuracy : 0.9932 Loss 10.189394 \n",
      "[22043] Accuracy : 0.9931 Loss 10.300670 \n",
      "[22512] Accuracy : 0.9934 Loss 9.542193 \n",
      "[22981] Accuracy : 0.9940 Loss 9.651953 \n",
      "[23450] Accuracy : 0.9927 Loss 11.138183 \n",
      "[23919] Accuracy : 0.9935 Loss 10.068576 \n",
      "[24388] Accuracy : 0.9942 Loss 9.043249 \n",
      "[24857] Accuracy : 0.9941 Loss 8.844383 \n",
      "[25326] Accuracy : 0.9933 Loss 9.655519 \n",
      "[25795] Accuracy : 0.9911 Loss 16.719824 \n",
      "[26264] Accuracy : 0.9929 Loss 11.225372 \n",
      "[26733] Accuracy : 0.9928 Loss 10.199684 \n",
      "[27202] Accuracy : 0.9927 Loss 10.383521 \n",
      "[27671] Accuracy : 0.9932 Loss 9.799185 \n",
      "[28140] Accuracy : 0.9933 Loss 10.050024 \n",
      "[28609] Accuracy : 0.9933 Loss 9.685496 \n",
      "[29078] Accuracy : 0.9934 Loss 9.706596 \n",
      "[29547] Accuracy : 0.9935 Loss 8.756158 \n",
      "[30016] Accuracy : 0.9941 Loss 9.092811 \n",
      "[30485] Accuracy : 0.9919 Loss 10.613894 \n",
      "[30954] Accuracy : 0.9928 Loss 9.782351 \n",
      "[31423] Accuracy : 0.9926 Loss 10.701508 \n",
      "[31892] Accuracy : 0.9938 Loss 9.802064 \n",
      "[32361] Accuracy : 0.9932 Loss 9.156716 \n",
      "[32830] Accuracy : 0.9938 Loss 8.808834 \n",
      "[33299] Accuracy : 0.9936 Loss 9.087553 \n",
      "[33768] Accuracy : 0.9936 Loss 9.131367 \n",
      "[34237] Accuracy : 0.9923 Loss 10.184973 \n",
      "[34706] Accuracy : 0.9923 Loss 10.183102 \n",
      "[35175] Accuracy : 0.9927 Loss 9.989107 \n",
      "[35644] Accuracy : 0.9927 Loss 9.764349 \n",
      "[36113] Accuracy : 0.9924 Loss 9.451888 \n",
      "[36582] Accuracy : 0.9930 Loss 9.835796 \n",
      "[37051] Accuracy : 0.9932 Loss 9.101860 \n",
      "[37520] Accuracy : 0.9933 Loss 8.555315 \n",
      "[37989] Accuracy : 0.9934 Loss 8.907390 \n",
      "[38458] Accuracy : 0.9933 Loss 9.045273 \n",
      "[38927] Accuracy : 0.9926 Loss 10.060435 \n",
      "[39396] Accuracy : 0.9934 Loss 9.698176 \n",
      "[39865] Accuracy : 0.9931 Loss 9.560382 \n",
      "[40334] Accuracy : 0.9932 Loss 9.477846 \n",
      "[40803] Accuracy : 0.9932 Loss 9.399194 \n",
      "[41272] Accuracy : 0.9939 Loss 8.700732 \n",
      "[41741] Accuracy : 0.9937 Loss 8.843820 \n",
      "[42210] Accuracy : 0.9935 Loss 8.706607 \n",
      "[42679] Accuracy : 0.9936 Loss 8.661053 \n",
      "[43148] Accuracy : 0.9937 Loss 8.769657 \n",
      "[43617] Accuracy : 0.9935 Loss 8.858982 \n",
      "[44086] Accuracy : 0.9936 Loss 9.189206 \n",
      "[44555] Accuracy : 0.9938 Loss 9.048623 \n",
      "[45024] Accuracy : 0.9932 Loss 8.763657 \n",
      "[45493] Accuracy : 0.9937 Loss 8.650624 \n",
      "[45962] Accuracy : 0.9939 Loss 8.731679 \n",
      "[46431] Accuracy : 0.9940 Loss 8.683645 \n",
      "[46900] Accuracy : 0.9939 Loss 9.047240 \n",
      "[47369] Accuracy : 0.9939 Loss 8.769320 \n",
      "[47838] Accuracy : 0.9942 Loss 8.673251 \n",
      "[48307] Accuracy : 0.9937 Loss 8.829393 \n",
      "[48776] Accuracy : 0.9940 Loss 8.668952 \n",
      "[49245] Accuracy : 0.9937 Loss 9.005272 \n",
      "[49714] Accuracy : 0.9934 Loss 9.021669 \n",
      "[50183] Accuracy : 0.9932 Loss 8.990463 \n",
      "[50652] Accuracy : 0.9936 Loss 8.628532 \n",
      "[51121] Accuracy : 0.9933 Loss 9.069582 \n",
      "[51590] Accuracy : 0.9938 Loss 8.919902 \n",
      "[52059] Accuracy : 0.9937 Loss 9.168220 \n",
      "[52528] Accuracy : 0.9934 Loss 9.111848 \n",
      "[52997] Accuracy : 0.9939 Loss 8.912628 \n",
      "[53466] Accuracy : 0.9938 Loss 8.747880 \n",
      "[53935] Accuracy : 0.9939 Loss 9.100104 \n",
      "[54404] Accuracy : 0.9936 Loss 9.018841 \n",
      "[54873] Accuracy : 0.9938 Loss 8.845878 \n",
      "[55342] Accuracy : 0.9944 Loss 9.012772 \n",
      "[55811] Accuracy : 0.9938 Loss 9.061449 \n",
      "[56280] Accuracy : 0.9937 Loss 9.037497 \n",
      "[56749] Accuracy : 0.9937 Loss 9.020100 \n",
      "[57218] Accuracy : 0.9937 Loss 8.616871 \n",
      "[57687] Accuracy : 0.9935 Loss 9.025536 \n",
      "[58156] Accuracy : 0.9940 Loss 8.832651 \n",
      "[58625] Accuracy : 0.9937 Loss 9.285134 \n",
      "[59094] Accuracy : 0.9936 Loss 8.949024 \n",
      "[59563] Accuracy : 0.9940 Loss 8.962648 \n",
      "[60032] Accuracy : 0.9937 Loss 9.137063 \n",
      "[60501] Accuracy : 0.9936 Loss 8.989888 \n",
      "[60970] Accuracy : 0.9935 Loss 9.003348 \n",
      "[61439] Accuracy : 0.9937 Loss 9.110337 \n",
      "[61908] Accuracy : 0.9936 Loss 9.509530 \n",
      "[62377] Accuracy : 0.9935 Loss 9.266070 \n",
      "[62846] Accuracy : 0.9934 Loss 9.267882 \n",
      "[63315] Accuracy : 0.9935 Loss 9.412277 \n",
      "[63784] Accuracy : 0.9934 Loss 9.382361 \n",
      "[64253] Accuracy : 0.9933 Loss 9.309936 \n",
      "[64722] Accuracy : 0.9932 Loss 9.583833 \n",
      "[65191] Accuracy : 0.9936 Loss 9.626956 \n",
      "[65660] Accuracy : 0.9938 Loss 9.488370 \n",
      "[66129] Accuracy : 0.9935 Loss 9.646903 \n",
      "[66598] Accuracy : 0.9935 Loss 9.705071 \n",
      "[67067] Accuracy : 0.9934 Loss 9.785817 \n",
      "[67536] Accuracy : 0.9933 Loss 9.584519 \n",
      "[68005] Accuracy : 0.9937 Loss 9.386849 \n",
      "[68474] Accuracy : 0.9937 Loss 9.470641 \n",
      "[68943] Accuracy : 0.9938 Loss 9.689813 \n",
      "[69412] Accuracy : 0.9937 Loss 9.563128 \n",
      "[69881] Accuracy : 0.9935 Loss 9.736422 \n",
      "[70350] Accuracy : 0.9934 Loss 9.672589 \n",
      "[70819] Accuracy : 0.9936 Loss 9.686141 \n",
      "[71288] Accuracy : 0.9934 Loss 9.885796 \n",
      "[71757] Accuracy : 0.9935 Loss 9.929101 \n",
      "[72226] Accuracy : 0.9936 Loss 10.015226 \n",
      "[72695] Accuracy : 0.9934 Loss 9.697213 \n",
      "[73164] Accuracy : 0.9936 Loss 9.786562 \n",
      "[73633] Accuracy : 0.9936 Loss 9.894844 \n",
      "[74102] Accuracy : 0.9934 Loss 9.880361 \n",
      "[74571] Accuracy : 0.9935 Loss 10.007730 \n",
      "[75040] Accuracy : 0.9934 Loss 9.939319 \n",
      "[75509] Accuracy : 0.9935 Loss 10.082738 \n",
      "[75978] Accuracy : 0.9934 Loss 9.959879 \n",
      "[76447] Accuracy : 0.9936 Loss 10.004047 \n",
      "[76916] Accuracy : 0.9936 Loss 10.002986 \n",
      "[77385] Accuracy : 0.9937 Loss 9.971428 \n",
      "[77854] Accuracy : 0.9934 Loss 10.100729 \n",
      "[78323] Accuracy : 0.9935 Loss 10.104896 \n",
      "[78792] Accuracy : 0.9932 Loss 10.041239 \n",
      "[79261] Accuracy : 0.9931 Loss 10.089963 \n",
      "[79730] Accuracy : 0.9933 Loss 10.029440 \n",
      "[80199] Accuracy : 0.9934 Loss 9.956124 \n",
      "[80668] Accuracy : 0.9933 Loss 10.094244 \n",
      "[81137] Accuracy : 0.9931 Loss 10.292260 \n",
      "[81606] Accuracy : 0.9932 Loss 10.392259 \n",
      "[82075] Accuracy : 0.9932 Loss 10.438265 \n",
      "[82544] Accuracy : 0.9933 Loss 10.399602 \n",
      "[83013] Accuracy : 0.9934 Loss 10.249031 \n",
      "[83482] Accuracy : 0.9934 Loss 10.361850 \n",
      "[83951] Accuracy : 0.9936 Loss 10.147607 \n",
      "[84420] Accuracy : 0.9934 Loss 10.245644 \n",
      "[84889] Accuracy : 0.9935 Loss 10.145116 \n",
      "[85358] Accuracy : 0.9931 Loss 10.403083 \n",
      "[85827] Accuracy : 0.9934 Loss 10.400758 \n",
      "[86296] Accuracy : 0.9933 Loss 10.350430 \n",
      "[86765] Accuracy : 0.9934 Loss 10.218498 \n",
      "[87234] Accuracy : 0.9935 Loss 10.424685 \n",
      "[87703] Accuracy : 0.9935 Loss 10.396235 \n",
      "[88172] Accuracy : 0.9933 Loss 10.473152 \n",
      "[88641] Accuracy : 0.9933 Loss 10.403159 \n",
      "[89110] Accuracy : 0.9934 Loss 10.345641 \n",
      "[89579] Accuracy : 0.9934 Loss 10.341568 \n",
      "[90048] Accuracy : 0.9935 Loss 10.457176 \n",
      "[90517] Accuracy : 0.9935 Loss 10.364846 \n",
      "[90986] Accuracy : 0.9935 Loss 10.408266 \n",
      "[91455] Accuracy : 0.9934 Loss 10.374515 \n",
      "[91924] Accuracy : 0.9935 Loss 10.259740 \n",
      "[92393] Accuracy : 0.9935 Loss 10.414648 \n",
      "[92862] Accuracy : 0.9935 Loss 10.309229 \n",
      "[93331] Accuracy : 0.9933 Loss 10.314860 \n",
      "[93800] Accuracy : 0.9935 Loss 10.349859 \n",
      "[94269] Accuracy : 0.9934 Loss 10.567048 \n",
      "[94738] Accuracy : 0.9932 Loss 10.673254 \n",
      "[95207] Accuracy : 0.9933 Loss 10.611224 \n",
      "[95676] Accuracy : 0.9932 Loss 10.567628 \n",
      "[96145] Accuracy : 0.9934 Loss 10.500704 \n",
      "[96614] Accuracy : 0.9935 Loss 10.514913 \n",
      "[97083] Accuracy : 0.9935 Loss 10.429194 \n",
      "[97552] Accuracy : 0.9935 Loss 10.507973 \n",
      "[98021] Accuracy : 0.9931 Loss 10.504378 \n",
      "[98490] Accuracy : 0.9934 Loss 10.546088 \n",
      "[98959] Accuracy : 0.9935 Loss 10.519657 \n",
      "[99428] Accuracy : 0.9933 Loss 10.625764 \n",
      "[99897] Accuracy : 0.9933 Loss 10.477904 \n",
      "[100366] Accuracy : 0.9935 Loss 10.480323 \n",
      "[100835] Accuracy : 0.9932 Loss 10.510571 \n",
      "[101304] Accuracy : 0.9934 Loss 10.547105 \n",
      "[101773] Accuracy : 0.9933 Loss 10.614470 \n",
      "[102242] Accuracy : 0.9935 Loss 10.492878 \n",
      "[102711] Accuracy : 0.9932 Loss 10.500207 \n",
      "[103180] Accuracy : 0.9933 Loss 10.556016 \n",
      "[103649] Accuracy : 0.9934 Loss 10.457191 \n",
      "[104118] Accuracy : 0.9932 Loss 10.445053 \n",
      "[104587] Accuracy : 0.9933 Loss 10.471242 \n",
      "[105056] Accuracy : 0.9932 Loss 10.517028 \n",
      "[105525] Accuracy : 0.9932 Loss 10.528470 \n",
      "[105994] Accuracy : 0.9933 Loss 10.523404 \n",
      "[106463] Accuracy : 0.9934 Loss 10.484870 \n",
      "[106932] Accuracy : 0.9936 Loss 10.463872 \n",
      "[107401] Accuracy : 0.9935 Loss 10.459538 \n",
      "[107870] Accuracy : 0.9936 Loss 10.444947 \n",
      "[108339] Accuracy : 0.9936 Loss 10.528535 \n",
      "[108808] Accuracy : 0.9936 Loss 10.423618 \n",
      "[109277] Accuracy : 0.9936 Loss 10.447422 \n",
      "[109746] Accuracy : 0.9935 Loss 10.482397 \n",
      "[110215] Accuracy : 0.9934 Loss 10.489615 \n",
      "[110684] Accuracy : 0.9937 Loss 10.512342 \n",
      "[111153] Accuracy : 0.9936 Loss 10.587872 \n",
      "[111622] Accuracy : 0.9936 Loss 10.471282 \n",
      "[112091] Accuracy : 0.9934 Loss 10.512155 \n",
      "[112560] Accuracy : 0.9933 Loss 10.610141 \n",
      "[113029] Accuracy : 0.9934 Loss 10.573206 \n",
      "[113498] Accuracy : 0.9932 Loss 10.595560 \n",
      "[113967] Accuracy : 0.9936 Loss 10.566685 \n",
      "[114436] Accuracy : 0.9933 Loss 10.566833 \n",
      "[114905] Accuracy : 0.9933 Loss 10.517985 \n",
      "[115374] Accuracy : 0.9935 Loss 10.532882 \n",
      "[115843] Accuracy : 0.9934 Loss 10.559630 \n",
      "[116312] Accuracy : 0.9934 Loss 10.629255 \n",
      "[116781] Accuracy : 0.9934 Loss 10.593870 \n",
      "[117250] Accuracy : 0.9934 Loss 10.586324 \n",
      "[117719] Accuracy : 0.9934 Loss 10.571048 \n",
      "[118188] Accuracy : 0.9933 Loss 10.576202 \n",
      "[118657] Accuracy : 0.9934 Loss 10.607163 \n",
      "[119126] Accuracy : 0.9934 Loss 10.604217 \n",
      "[119595] Accuracy : 0.9934 Loss 10.656862 \n",
      "[120064] Accuracy : 0.9934 Loss 10.589581 \n",
      "[120533] Accuracy : 0.9934 Loss 10.622438 \n",
      "[121002] Accuracy : 0.9934 Loss 10.565495 \n",
      "[121471] Accuracy : 0.9934 Loss 10.587442 \n",
      "[121940] Accuracy : 0.9935 Loss 10.606687 \n",
      "[122409] Accuracy : 0.9934 Loss 10.642845 \n",
      "[122878] Accuracy : 0.9935 Loss 10.612271 \n",
      "[123347] Accuracy : 0.9935 Loss 10.622526 \n",
      "[123816] Accuracy : 0.9935 Loss 10.587964 \n",
      "[124285] Accuracy : 0.9933 Loss 10.719504 \n",
      "[124754] Accuracy : 0.9934 Loss 10.747947 \n",
      "[125223] Accuracy : 0.9934 Loss 10.748259 \n",
      "[125692] Accuracy : 0.9934 Loss 10.730861 \n",
      "[126161] Accuracy : 0.9934 Loss 10.669668 \n",
      "[126630] Accuracy : 0.9934 Loss 10.692746 \n",
      "[127099] Accuracy : 0.9934 Loss 10.642939 \n",
      "[127568] Accuracy : 0.9934 Loss 10.707935 \n",
      "[128037] Accuracy : 0.9935 Loss 10.687058 \n",
      "[128506] Accuracy : 0.9934 Loss 10.729862 \n",
      "[128975] Accuracy : 0.9934 Loss 10.698480 \n",
      "[129444] Accuracy : 0.9934 Loss 10.705379 \n",
      "[129913] Accuracy : 0.9934 Loss 10.747262 \n",
      "[130382] Accuracy : 0.9934 Loss 10.805002 \n",
      "[130851] Accuracy : 0.9934 Loss 10.820295 \n",
      "[131320] Accuracy : 0.9933 Loss 10.793516 \n",
      "[131789] Accuracy : 0.9934 Loss 10.784005 \n",
      "[132258] Accuracy : 0.9934 Loss 10.811049 \n",
      "[132727] Accuracy : 0.9934 Loss 10.820613 \n",
      "[133196] Accuracy : 0.9935 Loss 10.800931 \n",
      "[133665] Accuracy : 0.9935 Loss 10.829148 \n",
      "[134134] Accuracy : 0.9935 Loss 10.782453 \n",
      "[134603] Accuracy : 0.9935 Loss 10.822777 \n",
      "[135072] Accuracy : 0.9935 Loss 10.828407 \n",
      "[135541] Accuracy : 0.9935 Loss 10.813071 \n",
      "[136010] Accuracy : 0.9935 Loss 10.839659 \n",
      "[136479] Accuracy : 0.9935 Loss 10.812622 \n",
      "[136948] Accuracy : 0.9935 Loss 10.813609 \n",
      "[137417] Accuracy : 0.9934 Loss 10.841683 \n",
      "[137886] Accuracy : 0.9935 Loss 10.837661 \n",
      "[138355] Accuracy : 0.9935 Loss 10.855863 \n",
      "[138824] Accuracy : 0.9935 Loss 10.841521 \n",
      "[139293] Accuracy : 0.9936 Loss 10.840173 \n",
      "[139762] Accuracy : 0.9936 Loss 10.829085 \n",
      "[140231] Accuracy : 0.9934 Loss 10.840130 \n",
      "[140700] Accuracy : 0.9934 Loss 10.880459 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    prev_updates = train(model, train_loader, optimizer, prev_updates, epoch, writer=writer)\n",
    "    acc = test(model, test_loader, prev_updates, epoch, writer=writer)\n",
    "    scheduler.step(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0072e-07, device='cuda:0', grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_eigenvals_positive_loss(model.fuzzy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализируем результаты"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
