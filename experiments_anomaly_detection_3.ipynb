{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "from torchfuzzy import FuzzyLayer, DefuzzyLinearLayer, FuzzyBellLayer\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "nz = 100 \n",
    "ngf = 32 \n",
    "ngpu = 1\n",
    "\n",
    "niter = 120\n",
    "\n",
    "prefix = f\"fuzzy_gan_anomaly_detection\"\n",
    "writer = SummaryWriter(f'runs/mnist/{prefix}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(lambda x: x.view(-1, 28, 28) - 0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_and_mask(target_label):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    t = torch.zeros(10)\n",
    "    t[target_label] = 1.0\n",
    "    \n",
    "    return t.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем обучающую выборку\n",
    "train_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=True, \n",
    "    transform = transform,\n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем тестовую выборку\n",
    "test_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=transform, \n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем итераторы датасетов\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(lambda x: x.view(-1, 28, 28) - 0.5),\n",
    "])\n",
    "\n",
    "emnist_test = pd.read_csv(\"./data/EMNIST/emnist-letters.zip\")\n",
    "emnist_y = emnist_test[\"label\"]\n",
    "emnist_x = emnist_test.drop(labels = [\"label\"], axis = 1) \n",
    "del emnist_test \n",
    "\n",
    "emnist_x = emnist_x / 255.0\n",
    "emnist_x = emnist_x.values.reshape(-1, 28, 28)\n",
    "emnist_x = [torch.tensor(emnist_transform(a), dtype=torch.float32, device=device) for a in emnist_x]\n",
    "\n",
    "len(emnist_x)\n",
    "\n",
    "emnist_mapping = pd.read_csv(\"./data/EMNIST/emnist-letters-mapping.txt\", sep=' ', header=None)\n",
    "emnist_mapping.columns=(\"EMNIST\",\"UP\",\"LO\")\n",
    "emnist_mapping[\"Letter\"] = emnist_mapping.apply(lambda row: chr(row[\"UP\"])+chr(row[\"LO\"]), axis=1)\n",
    "emnist_mapping = dict(zip(emnist_mapping[\"EMNIST\"], emnist_mapping[\"Letter\"]))\n",
    "\n",
    "emnist_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.stack(emnist_x), torch.Tensor(np.array(emnist_y))), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu, nc=1, nz=100, ngf=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(    ngf,      nc, kernel_size=1, stride=1, padding=2, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "num_params = sum(p.numel() for p in netG.parameters() if p.requires_grad)\n",
    "print(f'Number of parameters: {num_params:,}')\n",
    "#netG.load_state_dict(torch.load('weights/netG_epoch_99.pth'))\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, 1, 2, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16, 2),\n",
    "        )\n",
    "        self.latent_dim = 2\n",
    "        self.fuzzlets = 1\n",
    "        \n",
    "        #theta = np.linspace(0, 2*np.pi, self.fuzzlets+1)\n",
    "        #a, b = 0.1 * np.cos(theta), 0.1 * np.sin(theta)\n",
    "\n",
    "        self.core = nn.Sequential(\n",
    "            FuzzyLayer.from_dimensions(self.latent_dim, self.fuzzlets),#from_centers_and_scales([[x[0],x[1]] for x in zip(a,b)][:-1], [[1, 1] for x in zip(a,b)][:-1]),\n",
    "            #nn.BatchNorm1d(self.fuzzlets),\n",
    "            #nn.Linear(self.fuzzlets, 1, bias=False)\n",
    "            #DefuzzyLinearLayer.from_dimensions(self.fuzzlets, 1, with_norm=False)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, input):\n",
    "        #batch_size = input.shape[0]\n",
    "        output = self.main(input)\n",
    "        c = self.core(output)\n",
    "        \n",
    "        #centroids_c = self.core[0].get_centroids()\n",
    "        #average_cenroid_c = centroids_c.mean(0).reshape((1,-1))\n",
    "        #dists = torch.cdist(output, average_cenroid_c, p=2).squeeze()\n",
    "        #torch.cdist(centroids_c, output, p=2)\n",
    "        # winner_fake = f.max(-1).values\n",
    "        # centroids_f = self.fake[0].get_centroids()\n",
    "        # average_cenroid_f = centroids_f.mean(0)\n",
    "\n",
    "        # с_winners = torch.randint(0, self.fuzzlets, (batch_size, )).to(device)\n",
    "        # random_centroids = centroids_c[с_winners]\n",
    "        # firing_of_random = c.gather(1, с_winners.reshape((batch_size, 1))).squeeze()\n",
    "\n",
    "        return c.squeeze() #c.median(-1).values# torch.quantile(c, 0.8, -1)\n",
    "    \n",
    "    # def forward2(self, input):\n",
    "    #     batch_size = input.shape[0]\n",
    "    #     output = self.main(input)\n",
    "    #     c = self.core(output)\n",
    "        \n",
    "    #     centroids_c = self.core[0].centroids.squeeze(-1)\n",
    "\n",
    "    #     с_winners = torch.randint(0, self.fuzzlets, (batch_size, )).to(device)\n",
    "    #     centroids_c = centroids_c[с_winners]\n",
    "        \n",
    "    #     c = c.gather(1, с_winners.reshape((batch_size, 1))).squeeze()    \n",
    "    #     return c, (output - centroids_c).abs().sum(-1)\n",
    "    \n",
    "    def freeze_encoder(self):\n",
    "        #self.core[0].set_requires_grad_rot(False)\n",
    "        self.main.requires_grad_(False)\n",
    "    \n",
    "    def arate(self, input):\n",
    "        #output = self.main(input)\n",
    "        #c = self.core(output)\n",
    "        return self.forward(input) #c.max(-1).values\n",
    "    \n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "num_params = sum(p.numel() for p in netD.parameters() if p.requires_grad)\n",
    "print(f'Number of parameters: {num_params:,}')\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(4, 1, 28, 28).to(device)\n",
    "dd = Discriminator(1).to(device)\n",
    "dd(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=0.00005, betas=(0.9, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=0.00005, betas=(0.9, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_eigenvals_positive_loss(d, eps = 1e-3):\n",
    "    ev = d.core[0].get_transformation_matrix_eigenvals().real.min()\n",
    "    ev = torch.where(ev > eps, eps, ev)\n",
    "    return -ev\n",
    "\n",
    "def gas_temperature(d, target_min, target_max):\n",
    "    c = d.get_centroids()\n",
    "    #dists = torch.cdist(c, c, p = 1)\n",
    "    #nonzero = dists[torch.triu(torch.ones_like(dists), diagonal=1) == 1]\n",
    "    #diff_max = (nonzero - target_max).max()\n",
    "    #diff_min = (target_min - nonzero).max()\n",
    "    \n",
    "    average_cenroid_c = c.mean(0)\n",
    "    nonzero = torch.cdist(c, average_cenroid_c.reshape((1,-1)), p = 2)\n",
    "    diff_max = (nonzero - target_max).max()\n",
    "    diff_min = (target_min - nonzero).max()\n",
    "    discrep = torch.maximum(diff_max, diff_min)\n",
    "    \n",
    "    return discrep.where(discrep > 0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_arate_distr(D):\n",
    "    firing_levels = []\n",
    "    lab_true = []\n",
    "    lab_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, _ in tqdm(test_loader, desc='Test MNIST', disable=True):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            rates = D.arate(data)\n",
    "            firing_levels.append(rates.cpu().numpy())\n",
    "            \n",
    "\n",
    "    firing_levels = np.concatenate(firing_levels, axis=0)\n",
    "    for p in firing_levels:\n",
    "        lab_true.append(1)\n",
    "        lab_pred.append(p)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in tqdm(emnist_loader, desc='Test EMNIST', disable=True):\n",
    "            data = data.view((-1, 1, 28, 28)).to(device) \n",
    "            arate = D.arate(data)\n",
    "            \n",
    "            for p in arate.cpu().numpy():\n",
    "                lab_true.append(0)\n",
    "                lab_pred.append(p)\n",
    "                \n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(lab_true, lab_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return firing_levels, roc_auc\n",
    "\n",
    "def draw_embeddings(epoch):\n",
    "    centroids_core = netD.core[0].get_centroids().detach().cpu().numpy()\n",
    "    \n",
    "    embedings = []\n",
    "    embedings_test = []\n",
    "    labels_expected = []\n",
    "    \n",
    "    cnt = 3\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(train_loader, desc='Encoding', disable=True):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            embeding = netD.main(data)\n",
    "            embedings.append(embeding.cpu().numpy())\n",
    "            labels_expected.append(np.argmax(target.squeeze(1).cpu().numpy(), axis=1))\n",
    "            cnt -= 1\n",
    "            if cnt < 0:\n",
    "                break\n",
    "            \n",
    "    embedings = np.concatenate(embedings, axis=0)\n",
    "    labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "\n",
    "    cnt = 3\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc='Encoding', disable=True):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            embeding = netD.main(data)\n",
    "            embedings_test.append(embeding.cpu().numpy())\n",
    "            cnt -= 1\n",
    "            if cnt < 0:\n",
    "                break\n",
    "    embedings_test = np.concatenate(embedings_test, axis=0)\n",
    "\n",
    "    embedings_fake = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_size = 256\n",
    "        latent_size = 100\n",
    "        \n",
    "        fixed_noise = torch.randn(batch_size, latent_size, 1, 1)\n",
    "        if torch.cuda.is_available():\n",
    "            fixed_noise = fixed_noise.cuda()\n",
    "        fake_images = netG(fixed_noise)\n",
    "        embeding = netD.main(fake_images)\n",
    "        embedings_fake.append(embeding.cpu().numpy())\n",
    "\n",
    "    embedings_fake = np.concatenate(embedings_fake, axis=0)    \n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    R, C = 1, 2\n",
    "\n",
    "    plt.subplot(R, C, 1)\n",
    "    plt.scatter(embedings[:, 0], embedings[:, 1], c=labels_expected, cmap='tab10', s=16)\n",
    "    plt.scatter(embedings_test[:, 0], embedings_test[:, 1], marker='+', c='red', s = 1)\n",
    "    \n",
    "    plt.scatter(embedings_fake[:, 0], embedings_fake[:, 1], c='black', marker='o', s=16)\n",
    "    plt.scatter(centroids_core[:, 0], centroids_core[:,1], marker='1', c='blue', s= 120)\n",
    "    plt.subplot(R, C, 2)\n",
    "    plt.scatter(embedings_test[:, 0], embedings_test[:, 1], c='red', s=20)\n",
    "    plt.scatter(embedings_fake[:, 0], embedings_fake[:, 1], c='black', marker='o', s=20)\n",
    "    plt.scatter(centroids_core[:,0], centroids_core[:, 1], marker='1', c='blue', s= 120)\n",
    "\n",
    "    plt.show()\n",
    "    #writer.add_figure('Embeddings', fig, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(niter):\n",
    "\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    report_aver_pos = 0\n",
    "    report_aver_neg = 0\n",
    "    report_loss_G = 0\n",
    "    local_count = 0\n",
    "    \n",
    "    for i, data in enumerate(tqdm(train_loader, desc='Training', disable=True)):\n",
    "        \n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        \n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        \n",
    "        #_ = netD(real_cpu)\n",
    "        #temp_loss = gas_temperature(netD.core[0], 0.01, 0.02)\n",
    "        #temp_loss.backward()\n",
    "        \n",
    "        firing_c = netD(real_cpu)\n",
    "        #firing_c = torch.clamp(firing_c,0, 8)\n",
    "        errD_real = torch.square(firing_c - 1).mean()\n",
    "        errD_real.backward()\n",
    "        \n",
    "        neg_firing_c = netD(fake.detach())\n",
    "        errD_fake = torch.square(neg_firing_c-0.99).mean()\n",
    "        errD_fake.backward()\n",
    "        \n",
    "        optimizerD.step()\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        netG.zero_grad()\n",
    "        gcore = netD(fake)\n",
    "        errG = torch.square(gcore - 1).mean() \n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        local_count += 1\n",
    "        report_loss_G += errG.item()\n",
    "        report_aver_pos += firing_c.mean().item()\n",
    "        report_aver_neg += neg_firing_c.mean().item()\n",
    "        \n",
    "    netG.eval()\n",
    "    netD.eval()\n",
    "\n",
    "    losses = {}\n",
    "    \n",
    "    losses['G'] = report_loss_G / local_count\n",
    "    losses['POS'] = report_aver_pos / local_count\n",
    "    losses['NEG'] = report_aver_neg / local_count\n",
    "    \n",
    "    writer.add_scalars('Loss', losses, epoch)\n",
    "    fake = netG(fixed_noise)\n",
    "    writer.add_images('Generated images', fake.detach(), epoch)\n",
    "    draw_embeddings(epoch)    \n",
    "    \n",
    "    mnist_distr, auc = get_test_arate_distr(netD)\n",
    "    mnist_distr_q = {}\n",
    "    mnist_distr_q[\"q20\"] = np.quantile(mnist_distr, 0.2)\n",
    "    mnist_distr_q[\"q80\"] = np.quantile(mnist_distr, 0.8)\n",
    "    writer.add_scalars(\"MNIST test  firings\", mnist_distr_q, epoch)\n",
    "    writer.add_scalar(\"AUC\", auc, epoch)\n",
    "    mnist_distr_q[\"auc\"] = auc\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{niter}\")\n",
    "    print(losses | mnist_distr_q)\n",
    "#draw_embeddings(epoch)\n",
    "#num_gpu = 1 if torch.cuda.is_available() else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD.core[0](torch.tensor([[-6.0, -5.0]]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD.core[0].get_centroids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD.core[0].get_transformation_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = netD\n",
    "G = netG\n",
    "D.eval()\n",
    "G.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_size = 49\n",
    "    latent_size = 100\n",
    "    \n",
    "    fixed_noise = torch.randn(batch_size, latent_size, 1, 1)\n",
    "    if torch.cuda.is_available():\n",
    "        fixed_noise = fixed_noise.cuda()\n",
    "    fake_images = G(fixed_noise)\n",
    "\n",
    "    fake_images_np = fake_images.cpu().detach().numpy()\n",
    "    fake_images_np = fake_images_np.reshape(fake_images_np.shape[0], 28, 28)\n",
    "    R, C = 7, 7\n",
    "    for i in range(batch_size):\n",
    "        plt.subplot(R, C, i + 1)\n",
    "        plt.imshow(fake_images_np[i], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arate(inp):\n",
    "    #output = D.main(inp)\n",
    "    #c = D.core(output)\n",
    "    return D.arate(inp)#c.sum(-1)\n",
    "    \n",
    "get_arate(inp.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_r = D.core[0].get_centroids().detach().cpu().numpy()\n",
    "#centroids_f = D.fake[0].centroids.squeeze(-1).cpu().detach().numpy()\n",
    "#centroids_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings_fake = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_size = 256\n",
    "    latent_size = 100\n",
    "    \n",
    "    fixed_noise = torch.randn(batch_size, latent_size, 1, 1)\n",
    "    if torch.cuda.is_available():\n",
    "        fixed_noise = fixed_noise.cuda()\n",
    "    fake_images = G(fixed_noise)\n",
    "    embeding = D.main(fake_images)\n",
    "    embedings_fake.append(embeding.cpu().numpy())\n",
    "\n",
    "embedings_fake = np.concatenate(embedings_fake, axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings = []\n",
    "labels_expected = []\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader, desc='Encoding'):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        embeding = D.main(data)\n",
    "        embedings.append(embeding.cpu().numpy())\n",
    "        labels_expected.append(np.argmax(target.squeeze(1).cpu().numpy(), axis=1))\n",
    "embedings = np.concatenate(embedings, axis=0)\n",
    "labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "R, C = 1, 1\n",
    "cnt = 1\n",
    "for i in range(1):\n",
    "    plt.subplot(R, C, cnt)\n",
    "    cnt += 1\n",
    "    plt.scatter(embedings_fake[:, 2*i], embedings_fake[:, 2*i+1], c='black', marker='+', s=30)\n",
    "    plt.scatter(embedings[:, 2*i], embedings[:, 2*i+1], c=labels_expected, cmap='tab10', s=2)\n",
    "    plt.scatter(centroids_r[:,2*i], centroids_r[:,2*i+1], marker='x', c='black', s= 100)\n",
    "    #plt.scatter(centroids_f[:,2*i], centroids_f[:,2*i+1], marker='o', c='red', s= 20)\n",
    "    #plt.xlim(-0.5, 0.5)\n",
    "    #plt.ylim(-0.5, 0.5)\n",
    "#plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings = []\n",
    "labels_expected = []\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(emnist_loader, desc='Encoding'):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        embeding = D.main(data)\n",
    "        embedings.append(embeding.cpu().numpy())\n",
    "        labels_expected.append(target.cpu().numpy())\n",
    "embedings = np.concatenate(embedings, axis=0)\n",
    "labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "R, C = 1, 2\n",
    "\n",
    "plt.subplot(R, C, 1)\n",
    "plt.scatter(embedings_fake[:, 2*i], embedings_fake[:, 2*i+1], c='black', marker='+', s=30)\n",
    "plt.scatter(embedings[:, 2*i], embedings[:, 2*i+1], c=labels_expected, cmap='tab10', s=2)\n",
    "plt.scatter(centroids_r[:,2*i], centroids_r[:,2*i+1], marker='x', c='green', s= 20)\n",
    "plt.subplot(R, C, 2)\n",
    "plt.scatter(embedings_fake[:, 2*i], embedings_fake[:, 2*i+1], c='black', marker='+', s=30)\n",
    "plt.scatter(centroids_r[:,2*i], centroids_r[:,2*i+1], marker='x', c='black', s= 200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firing_levels = []\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader, desc='Encoding'):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        rates = get_arate(data)\n",
    "        #output = D.main(data)\n",
    "        #r = D.real(output)\n",
    "        #rates = r\n",
    "\n",
    "        firing_levels.append(rates.cpu().numpy())\n",
    "        \n",
    "firing_levels = np.concatenate(firing_levels, axis=0)\n",
    "fig = plt.figure(figsize =(10, 5))\n",
    "plt.boxplot(firing_levels, notch=True, showfliers=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firings_emnist = {}\n",
    "for m in emnist_mapping:\n",
    "    firings_emnist[emnist_mapping[m]] = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in tqdm(emnist_loader, desc='Encoding'):\n",
    "        data = data.view((-1, 1, 28, 28)).to(device) \n",
    "        arate = get_arate(data)\n",
    "        #output = D.main(data)\n",
    "        #r = D.real(output)\n",
    "        #arate = 1 - r.sum(dim = 1)\n",
    "        \n",
    "        for label, flabel in zip(labels, arate.cpu().numpy()):\n",
    "            firings_emnist[emnist_mapping[label.item()]].append(flabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, data = firings_emnist.keys(), firings_emnist.values()\n",
    "fig = plt.figure(figsize =(33, 5))\n",
    "plt.boxplot(data, notch=True, showfliers=False)\n",
    "plt.xticks(range(1, len(labels) + 1), labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firings_mnist = {}\n",
    "firings_mnist['MNIST'] = firing_levels\n",
    "firings = {**firings_mnist, **firings_emnist} \n",
    "labels, data = firings.keys(), firings.values()\n",
    "\n",
    "fig = plt.figure(figsize =(12, 2))\n",
    "#plt.ylim(ymin=0)\n",
    "plt.boxplot(data, notch=True, showfliers=False)\n",
    "plt.xticks(range(1, len(labels) + 1), labels)\n",
    "plt.show()\n",
    "\n",
    "writer.add_figure('Anomaly Detection', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_true = []\n",
    "lab_pred = []\n",
    "for k,v in firings_mnist.items():\n",
    "    for p in v:\n",
    "        lab_true.append(1)\n",
    "        lab_pred.append(p)\n",
    "for k,v in firings_emnist.items():\n",
    "    if k == 'Oo':\n",
    "        continue\n",
    "    for p in v:\n",
    "        lab_true.append(0)\n",
    "        lab_pred.append(p)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(lab_true, lab_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "fig = plt.figure(figsize =(4, 4))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "writer.add_figure('ROC', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
