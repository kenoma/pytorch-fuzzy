{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "from torchfuzzy import FuzzyLayer, DefuzzyLinearLayer\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "nz = 100 \n",
    "ngf = 32 \n",
    "ngpu = 1\n",
    "\n",
    "niter = 50\n",
    "\n",
    "prefix = f\"fuzzy_gan_anomaly_detection\"\n",
    "writer = SummaryWriter(f'runs/mnist/{prefix}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(lambda x: x.view(-1, 28, 28) - 0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_and_mask(target_label):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    t = torch.zeros(10)\n",
    "    t[target_label] = 1.0\n",
    "    \n",
    "    return t.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем обучающую выборку\n",
    "train_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=True, \n",
    "    transform = transform,\n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем тестовую выборку\n",
    "test_data = datasets.MNIST(\n",
    "    '~/.pytorch/MNIST_data/', \n",
    "    download=True, \n",
    "    train=False, \n",
    "    transform=transform, \n",
    "    target_transform = transforms.Lambda(lambda x: get_target_and_mask(x))\n",
    ")\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем итераторы датасетов\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(lambda x: x.view(-1, 28, 28) - 0.5),\n",
    "])\n",
    "\n",
    "emnist_test = pd.read_csv(\"./data/EMNIST/emnist-letters.zip\")\n",
    "emnist_y = emnist_test[\"label\"]\n",
    "emnist_x = emnist_test.drop(labels = [\"label\"], axis = 1) \n",
    "del emnist_test \n",
    "\n",
    "emnist_x = emnist_x / 255.0\n",
    "emnist_x = emnist_x.values.reshape(-1, 28, 28)\n",
    "emnist_x = [torch.tensor(emnist_transform(a), dtype=torch.float32, device=device) for a in emnist_x]\n",
    "\n",
    "len(emnist_x)\n",
    "\n",
    "emnist_mapping = pd.read_csv(\"./data/EMNIST/emnist-letters-mapping.txt\", sep=' ', header=None)\n",
    "emnist_mapping.columns=(\"EMNIST\",\"UP\",\"LO\")\n",
    "emnist_mapping[\"Letter\"] = emnist_mapping.apply(lambda row: chr(row[\"UP\"])+chr(row[\"LO\"]), axis=1)\n",
    "emnist_mapping = dict(zip(emnist_mapping[\"EMNIST\"], emnist_mapping[\"Letter\"]))\n",
    "\n",
    "emnist_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(torch.stack(emnist_x), torch.Tensor(np.array(emnist_y))), \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu, nc=1, nz=100, ngf=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(    ngf,      nc, kernel_size=1, stride=1, padding=2, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "\n",
    "netG = Generator(ngpu).to(device)\n",
    "netG.apply(weights_init)\n",
    "num_params = sum(p.numel() for p in netG.parameters() if p.requires_grad)\n",
    "print(f'Number of parameters: {num_params:,}')\n",
    "#netG.load_state_dict(torch.load('weights/netG_epoch_99.pth'))\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, 1, 2, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(1),\n",
    "            # nn.Conv2d(1, 1, 2, 2, 0, bias=False),\n",
    "            # nn.BatchNorm2d(1),\n",
    "            # nn.LeakyReLU(0.2, inplace=True),\n",
    "            # nn.Conv2d(1, 1, 2, 2, 0, bias=False),\n",
    "            # nn.BatchNorm2d(1),\n",
    "            nn.Flatten(),\n",
    "            #FuzzyLayer.from_dimensions(9, 10),\n",
    "            nn.Linear(16, 2),\n",
    "            #nn.BatchNorm1d(2),\n",
    "            #DefuzzyLinearLayer.from_dimensions(100, 1, with_norm=False)\n",
    "            \n",
    "            #DefuzzyLinearLayer.from_dimensions(20, 1, with_norm=False)\n",
    "            #nn.Conv2d(ndf * 4, 1, 4, 2, 1, bias=False),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "        self.latent_dim = 2\n",
    "        self.fuzzlets = 10\n",
    "        theta = np.linspace(0, 2*np.pi, self.fuzzlets+1)\n",
    "        a, b = 0.5 * np.cos(theta), 0.5 * np.sin(theta)\n",
    "\n",
    "        self.core = nn.Sequential(\n",
    "            FuzzyLayer.from_centers_and_scales([[x[0],x[1]] for x in zip(a,b)][:-1], [[0.1, 0.1] for x in zip(a,b)][:-1]),\n",
    "            #nn.BatchNorm1d(self.fuzzlets),\n",
    "            #nn.Linear(self.fuzzlets, 1, bias=False)\n",
    "            #DefuzzyLinearLayer.from_dimensions(self.fuzzlets, 1, with_norm=False)\n",
    "        )\n",
    "        #self.real = nn.Sequential(\n",
    "        #    FuzzyLayer.from_dimensions(self.latent_dim, self.fuzzlets)\n",
    "        #)\n",
    "        #self.core[0].set_requires_grad_rot(False)\n",
    "        #self.core[0].set_requires_grad_rot(False)\n",
    "        #self.fake[0].set_requires_grad_rot(False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.shape[0]\n",
    "        output = self.main(input)\n",
    "        c = self.core(output)\n",
    "        centroids_c = self.core[0].centroids.squeeze(-1)\n",
    "        average_cenroid = centroids_c.mean(0)\n",
    "\n",
    "        с_winners = torch.randint(0, self.fuzzlets, (batch_size, )).to(device)\n",
    "        random_centroids = centroids_c[с_winners]\n",
    "        firing_of_random = c.gather(1, с_winners.reshape((batch_size, 1))).squeeze()\n",
    "\n",
    "        return c.sum(-1), (output - average_cenroid).abs().sum(-1), (random_centroids - output).abs().sum(-1)\n",
    "    \n",
    "    # def forward2(self, input):\n",
    "    #     batch_size = input.shape[0]\n",
    "    #     output = self.main(input)\n",
    "    #     c = self.core(output)\n",
    "        \n",
    "    #     centroids_c = self.core[0].centroids.squeeze(-1)\n",
    "\n",
    "    #     с_winners = torch.randint(0, self.fuzzlets, (batch_size, )).to(device)\n",
    "    #     centroids_c = centroids_c[с_winners]\n",
    "        \n",
    "    #     c = c.gather(1, с_winners.reshape((batch_size, 1))).squeeze()    \n",
    "    #     return c, (output - centroids_c).abs().sum(-1)\n",
    "    \n",
    "    def freeze_encoder(self):\n",
    "        #self.core[0].set_requires_grad_rot(False)\n",
    "        self.main.requires_grad_(False)\n",
    "    \n",
    "    def arate(self, input):\n",
    "        return self.forward(input)[0]\n",
    "    \n",
    "\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "num_params = sum(p.numel() for p in netD.parameters() if p.requires_grad)\n",
    "print(f'Number of parameters: {num_params:,}')\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(13, 1, 28, 28).to(device)\n",
    "dd = Discriminator(1).to(device)\n",
    "dd(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_eigenvals_positive_loss(d, eps = 1e-3):\n",
    "    ev = d.core[0].get_transformation_matrix_eigenvals().real.min()\n",
    "    ev = torch.where(ev > eps, eps, ev)\n",
    "    return -ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_arate_distr(D):\n",
    "    firing_levels = []\n",
    "    lab_true = []\n",
    "    lab_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, _ in tqdm(test_loader, desc='Test MNIST', disable=True):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            rates = D.arate(data)\n",
    "            firing_levels.append(rates.cpu().numpy())\n",
    "            \n",
    "\n",
    "    firing_levels = np.concatenate(firing_levels, axis=0)\n",
    "    for p in firing_levels:\n",
    "        lab_true.append(1)\n",
    "        lab_pred.append(p)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in tqdm(emnist_loader, desc='Test EMNIST', disable=True):\n",
    "            data = data.view((-1, 1, 28, 28)).to(device) \n",
    "            arate = D.arate(data)\n",
    "            \n",
    "            for p in arate.cpu().numpy():\n",
    "                lab_true.append(0)\n",
    "                lab_pred.append(p)\n",
    "                \n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(lab_true, lab_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    return firing_levels, roc_auc\n",
    "\n",
    "def draw_embeddings(epoch):\n",
    "    #centroids_r = netD.real[0].centroids.squeeze(-1).cpu().detach().numpy()\n",
    "    #centroids_f = netD.fake[0].centroids.squeeze(-1).cpu().detach().numpy()\n",
    "    centroids_core = netD.core[0].centroids.squeeze(-1).cpu().detach().numpy()\n",
    "    \n",
    "    embedings = []\n",
    "    labels_expected = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc='Encoding', disable=True):\n",
    "            data = data.view((-1,1,28,28)).to(device)\n",
    "            embeding = netD.main(data)\n",
    "            embedings.append(embeding.cpu().numpy())\n",
    "            labels_expected.append(np.argmax(target.squeeze(1).cpu().numpy(), axis=1))\n",
    "    embedings = np.concatenate(embedings, axis=0)\n",
    "    labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "\n",
    "    embedings_fake = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_size = 256\n",
    "        latent_size = 100\n",
    "        \n",
    "        fixed_noise = torch.randn(batch_size, latent_size, 1, 1)\n",
    "        if torch.cuda.is_available():\n",
    "            fixed_noise = fixed_noise.cuda()\n",
    "        fake_images = netG(fixed_noise)\n",
    "        embeding = netD.main(fake_images)\n",
    "        embedings_fake.append(embeding.cpu().numpy())\n",
    "\n",
    "    embedings_fake = np.concatenate(embedings_fake, axis=0)    \n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    R, C = 1, 1\n",
    "    cnt = 1\n",
    "    for i in range(1):\n",
    "        plt.subplot(R, C, cnt)\n",
    "        cnt += 1\n",
    "        plt.scatter(embedings_fake[:, 2*i], embedings_fake[:, 2*i+1], c='black', marker='+', s=16)\n",
    "        plt.scatter(embedings[:, 2*i], embedings[:, 2*i+1], c=labels_expected, cmap='tab10', s=2)\n",
    "        #plt.scatter(centroids_r[:,2*i], centroids_r[:,2*i+1], marker='X', c='green', s= 64)\n",
    "        #plt.scatter(centroids_f[:,2*i], centroids_f[:,2*i+1], marker='o', c='red', s= 16)\n",
    "        plt.scatter(centroids_core[:,2*i], centroids_core[:,2*i+1], marker='*', c='blue', s= 128)\n",
    "        \n",
    "        #plt.xlim(-2, 2)\n",
    "        #plt.ylim(-2, 2)\n",
    "\n",
    "    A = netD.core[0].get_transformation_matrix().detach().cpu().numpy()\n",
    "\n",
    "    theta = np.linspace(0, 2*np.pi, 20)\n",
    "    a, b = 1 * np.cos(theta), 1 * np.sin(theta)\n",
    "\n",
    "    for i in range(A.shape[0]):\n",
    "        h = A[i]\n",
    "        t = [np.matmul(h, [x[0], x[1], 1]) for x in zip(a,b)]\n",
    "        t = np.array(t)\n",
    "        plt.plot(t[:, 0], t[:, 1])\n",
    "\n",
    "    #plt.colorbar()\n",
    "    plt.show()\n",
    "    writer.add_figure('Embeddings', fig, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(niter):\n",
    "\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    report_loss_G = 0\n",
    "    report_loss_D = 0\n",
    "    report_loss_EV = 0\n",
    "    local_count = 0\n",
    "    for i, data in enumerate(tqdm(train_loader, desc='Training', disable=True)):\n",
    "        \n",
    "        # train with real\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        \n",
    "        # netD.zero_grad()\n",
    "        # core, dists_c, _, _ = netD(real_cpu)\n",
    "        # dists_c.mean().backward()\n",
    "        # optimizerD.step()\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        _, center_mass_dist, rand_dist = netD(real_cpu)\n",
    "        rand_dist.mean().backward(retain_graph=True)\n",
    "        center_mass_dist.mean().backward(retain_graph=True)\n",
    "        ev_loss_r = keep_eigenvals_positive_loss(netD)\n",
    "        ev_loss_r.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        netD.zero_grad()\n",
    "        core, _, _ = netD(real_cpu)\n",
    "        errD_real = torch.square(core - 1).mean()\n",
    "        errD_real.backward(retain_graph=True)\n",
    "        ev_loss_r = keep_eigenvals_positive_loss(netD)\n",
    "        ev_loss_r.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        #train with fake\n",
    "        netD.zero_grad()\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        core, _, _ = netD(fake.detach())\n",
    "        errD_fake = torch.square(core).mean()\n",
    "        errD_fake.backward(retain_graph=True)\n",
    "        ev_loss_f = keep_eigenvals_positive_loss(netD)\n",
    "        ev_loss_f.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        #adjust sattelite\n",
    "        #netD.zero_grad()\n",
    "        # _, _, p, dp = netD(real_cpu)\n",
    "        # dp.mean().backward()\n",
    "        # optimizerD.step()\n",
    "        \n",
    "        #inp = torch.cat((fake.detach(), real_cpu))\n",
    "        #target = torch.cat((torch.zeros(batch_size), torch.ones(batch_size))).to(device)\n",
    "        \n",
    "        # netD.zero_grad()\n",
    "        # _, _, p, _ = netD(inp)\n",
    "        # (torch.square(target - p).mean()).backward()\n",
    "        # optimizerD.step()\n",
    "        \n",
    "        errD = errD_real + errD_fake\n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        core,_,_ = netD(fake)\n",
    "        errG = torch.square(core - 1).mean()\n",
    "        errG.backward()\n",
    "        \n",
    "        optimizerG.step()\n",
    "\n",
    "        report_loss_G += errG.item()\n",
    "\n",
    "        local_count += 1\n",
    "        report_loss_D += errD.item()\n",
    "        report_loss_EV += torch.max(ev_loss_f, ev_loss_r).item()\n",
    "        \n",
    "    netG.eval()\n",
    "    netD.eval()\n",
    "\n",
    "    losses = {}\n",
    "    \n",
    "    losses['EV'] = report_loss_EV / local_count\n",
    "    losses['D'] = report_loss_D / local_count\n",
    "    losses['G'] = report_loss_G / local_count\n",
    "    writer.add_scalars('Loss', losses, epoch)\n",
    "    fake = netG(fixed_noise)\n",
    "    writer.add_images('Generated images', fake.detach(), epoch)\n",
    "    draw_embeddings(epoch)    \n",
    "    #mnist_distr, auc = get_test_arate_distr(netD)\n",
    "    #mnist_distr_q = {}\n",
    "    #mnist_distr_q[\"q20\"] = np.quantile(mnist_distr, 0.2)\n",
    "    #mnist_distr_q[\"q80\"] = np.quantile(mnist_distr, 0.8)\n",
    "    #writer.add_scalars(\"MNIST test  firings\", mnist_distr_q, epoch)\n",
    "    #writer.add_scalar(\"AUC\", auc, epoch)\n",
    "    print(f\"Epoch {epoch}/{niter}\")\n",
    "    print(losses)\n",
    "\n",
    "draw_embeddings(epoch)\n",
    "num_gpu = 1 if torch.cuda.is_available() else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD.core[0].get_transformation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD.core[0].get_transformation_matrix_eigenvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizerD = torch.optim.Adam(netD.parameters(), lr=0.002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netG.eval()\n",
    "# netD.freeze_encoder()\n",
    "\n",
    "\n",
    "# for epoch in range(niter):\n",
    "   \n",
    "#     netD.train()\n",
    "#     report_loss_D = 0\n",
    "#     report_loss_Rad = 0\n",
    "#     report_loss_EV = 0.0\n",
    "#     local_count = 0\n",
    "#     for i, data in enumerate(tqdm(train_loader, desc='Training', disable=True)):\n",
    "        \n",
    "#         # train with real\n",
    "#         real_cpu = data[0].to(device)\n",
    "#         batch_size = real_cpu.size(0)\n",
    "        \n",
    "#         #netD.zero_grad()\n",
    "#         #_, dist = netD.forward2(real_cpu)\n",
    "#         #dist.abs().mean().backward()\n",
    "#         #optimizerD.step()\n",
    "        \n",
    "#         noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "#         fake = netG(noise)\n",
    "        \n",
    "#         inp = torch.cat((fake.detach(), real_cpu))\n",
    "#         target = torch.cat((torch.zeros(batch_size), torch.ones(batch_size))).to(device)\n",
    "        \n",
    "#         netD.zero_grad()\n",
    "#         firing, dist = netD(inp)\n",
    "#         # _, _, p, _ = netD(inp)\n",
    "#         #d = torch.where(dist>1, dist, 0.1)\n",
    "#         errD = (torch.square(target - firing).mean()) #+ d.mean()\n",
    "#         errD.backward(retain_graph=True)\n",
    "#         ev_loss = keep_eigenvals_positive_loss(netD)\n",
    "#         ev_loss.backward()\n",
    "#         optimizerD.step()\n",
    "        \n",
    "#         # netD.zero_grad()\n",
    "#         # firing, dist = netD.forward2(real_cpu)\n",
    "#         # errD_real = torch.square(firing - 1).mean()\n",
    "#         # errD_real.backward()\n",
    "#         # optimizerD.step()\n",
    "        \n",
    "#         # #train with fake\n",
    "#         # netD.zero_grad()\n",
    "        \n",
    "#         # firing, _ = netD.forward2(fake.detach())\n",
    "#         # errD_fake = torch.square(firing).mean()\n",
    "#         # errD_fake.backward()\n",
    "#         # optimizerD.step()\n",
    "        \n",
    "#         #errD = errD_real + errD_fake\n",
    "        \n",
    "#         local_count += 1\n",
    "#         #report_loss_Rad += dist.mean().item()\n",
    "#         report_loss_D += errD.item()\n",
    "#         if ev_loss.item() > report_loss_EV:\n",
    "#             report_loss_EV = ev_loss.item()\n",
    "        \n",
    "#     netG.eval()\n",
    "#     netD.eval()\n",
    "\n",
    "#     losses = {}\n",
    "    \n",
    "#     losses['Adjusting D'] = report_loss_D / local_count\n",
    "#     losses['Adjusting Rad'] = report_loss_Rad / local_count\n",
    "#     losses['EV'] = report_loss_EV\n",
    "    \n",
    "    \n",
    "#     writer.add_scalars('Loss', losses, epoch)\n",
    "#     draw_embeddings(epoch)    \n",
    "#     #mnist_distr, auc = get_test_arate_distr(netD)\n",
    "#     #mnist_distr_q = {}\n",
    "#     #mnist_distr_q[\"q20\"] = np.quantile(mnist_distr, 0.2)\n",
    "#     #mnist_distr_q[\"q80\"] = np.quantile(mnist_distr, 0.8)\n",
    "#     #writer.add_scalars(\"MNIST test  firings\", mnist_distr_q, epoch)\n",
    "#     #writer.add_scalar(\"AUC\", auc, epoch)\n",
    "#     print(f\"Epoch {epoch}/{niter}\")\n",
    "#     print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = netD\n",
    "G = netG\n",
    "D.eval()\n",
    "G.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_size = 49\n",
    "    latent_size = 100\n",
    "    \n",
    "    fixed_noise = torch.randn(batch_size, latent_size, 1, 1)\n",
    "    if torch.cuda.is_available():\n",
    "        fixed_noise = fixed_noise.cuda()\n",
    "    fake_images = G(fixed_noise)\n",
    "\n",
    "    fake_images_np = fake_images.cpu().detach().numpy()\n",
    "    fake_images_np = fake_images_np.reshape(fake_images_np.shape[0], 28, 28)\n",
    "    R, C = 7, 7\n",
    "    for i in range(batch_size):\n",
    "        plt.subplot(R, C, i + 1)\n",
    "        plt.imshow(fake_images_np[i], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = netD.core[0].get_transformation_matrix().detach().cpu().numpy()\n",
    "\n",
    "theta = np.linspace(0, 2*np.pi, 20)\n",
    "a, b = 1 * np.cos(theta), 1 * np.sin(theta)\n",
    "\n",
    "for i in range(A.shape[0]):\n",
    "    h = A[i]\n",
    "    t = [np.matmul(h, [x[0], x[1], 1]) for x in zip(a,b)]\n",
    "    t = np.array(t)\n",
    "    plt.plot(t[:, 0], t[:, 1])\n",
    "    \n",
    "plt.show()\n",
    "#c_real = lambda x: D.real(torch.FloatTensor([x]).to(device)).max(1).values.detach().cpu().item()\n",
    "#c_core = lambda x: D.core(torch.FloatTensor([x]).to(device)).max(1).values.detach().cpu().item()\n",
    "#f = lambda x: [[c_core(a), 0, 0] for a in x]\n",
    "\n",
    "#xy = np.mgrid[-2:2:200j, -2:2:200j].reshape(2,-1).T\n",
    "\n",
    "#img = np.reshape(f(xy),(200,200,3))\n",
    "#plt.figure(figsize=(6, 6))\n",
    "#plt.imshow(img, extent=[-2,2,-2,2])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arate(inp):\n",
    "    return D(inp)[1]\n",
    "get_arate(inp.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_r = D.core[0].centroids.squeeze(-1).cpu().detach().numpy()\n",
    "#centroids_f = D.fake[0].centroids.squeeze(-1).cpu().detach().numpy()\n",
    "#centroids_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.core[0].get_transformation_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.core[0].get_transformation_matrix_eigenvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings_fake = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_size = 256\n",
    "    latent_size = 100\n",
    "    \n",
    "    fixed_noise = torch.randn(batch_size, latent_size, 1, 1)\n",
    "    if torch.cuda.is_available():\n",
    "        fixed_noise = fixed_noise.cuda()\n",
    "    fake_images = G(fixed_noise)\n",
    "    embeding = D.main(fake_images)\n",
    "    embedings_fake.append(embeding.cpu().numpy())\n",
    "\n",
    "embedings_fake = np.concatenate(embedings_fake, axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings = []\n",
    "labels_expected = []\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader, desc='Encoding'):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        embeding = D.main(data)\n",
    "        embedings.append(embeding.cpu().numpy())\n",
    "        labels_expected.append(np.argmax(target.squeeze(1).cpu().numpy(), axis=1))\n",
    "embedings = np.concatenate(embedings, axis=0)\n",
    "labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "R, C = 1, 1\n",
    "cnt = 1\n",
    "for i in range(1):\n",
    "    plt.subplot(R, C, cnt)\n",
    "    cnt += 1\n",
    "    plt.scatter(embedings_fake[:, 2*i], embedings_fake[:, 2*i+1], c='black', marker='+', s=30)\n",
    "    plt.scatter(embedings[:, 2*i], embedings[:, 2*i+1], c=labels_expected, cmap='tab10', s=2)\n",
    "    plt.scatter(centroids_r[:,2*i], centroids_r[:,2*i+1], marker='x', c='green', s= 100)\n",
    "    #plt.scatter(centroids_f[:,2*i], centroids_f[:,2*i+1], marker='o', c='red', s= 20)\n",
    "    #plt.xlim(-0.5, 0.5)\n",
    "    #plt.ylim(-0.5, 0.5)\n",
    "#plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedings = []\n",
    "labels_expected = []\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(emnist_loader, desc='Encoding'):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        embeding = D.main(data)\n",
    "        embedings.append(embeding.cpu().numpy())\n",
    "        labels_expected.append(target.cpu().numpy())\n",
    "embedings = np.concatenate(embedings, axis=0)\n",
    "labels_expected = np.concatenate(labels_expected, axis=0)\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "R, C = 1, 1\n",
    "cnt = 1\n",
    "for i in range(1):\n",
    "    plt.subplot(R, C, cnt)\n",
    "    cnt += 1\n",
    "    plt.scatter(embedings_fake[:, 2*i], embedings_fake[:, 2*i+1], c='black', marker='+', s=30)\n",
    "    plt.scatter(embedings[:, 2*i], embedings[:, 2*i+1], c=labels_expected, cmap='tab10', s=2)\n",
    "    plt.scatter(centroids_r[:,2*i], centroids_r[:,2*i+1], marker='x', c='green', s= 20)\n",
    "    #plt.scatter(centroids_f[:,2*i], centroids_f[:,2*i+1], marker='o', c='red', s= 20)\n",
    "    #plt.xlim(-0.5, 0.5)\n",
    "    #plt.ylim(-0.5, 0.5)\n",
    "#plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firing_levels = []\n",
    "with torch.no_grad():\n",
    "    for data, target in tqdm(test_loader, desc='Encoding'):\n",
    "        data = data.view((-1,1,28,28)).to(device)\n",
    "        rates = get_arate(data)\n",
    "        #output = D.main(data)\n",
    "        #r = D.real(output)\n",
    "        #rates = r\n",
    "\n",
    "        firing_levels.append(rates.cpu().numpy())\n",
    "        \n",
    "firing_levels = np.concatenate(firing_levels, axis=0)\n",
    "fig = plt.figure(figsize =(10, 5))\n",
    "plt.boxplot(firing_levels, notch=True, showfliers=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firings_emnist = {}\n",
    "for m in emnist_mapping:\n",
    "    firings_emnist[emnist_mapping[m]] = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in tqdm(emnist_loader, desc='Encoding'):\n",
    "        data = data.view((-1, 1, 28, 28)).to(device) \n",
    "        arate = get_arate(data)\n",
    "        #output = D.main(data)\n",
    "        #r = D.real(output)\n",
    "        #arate = 1 - r.sum(dim = 1)\n",
    "        \n",
    "        for label, flabel in zip(labels, arate.cpu().numpy()):\n",
    "            firings_emnist[emnist_mapping[label.item()]].append(flabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, data = firings_emnist.keys(), firings_emnist.values()\n",
    "fig = plt.figure(figsize =(33, 5))\n",
    "plt.boxplot(data, notch=True, showfliers=False)\n",
    "plt.xticks(range(1, len(labels) + 1), labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firings_mnist = {}\n",
    "firings_mnist['MNIST'] = firing_levels\n",
    "firings = {**firings_mnist, **firings_emnist} \n",
    "labels, data = firings.keys(), firings.values()\n",
    "\n",
    "fig = plt.figure(figsize =(12, 2))\n",
    "#plt.ylim(ymin=0)\n",
    "plt.boxplot(data, notch=True, showfliers=False)\n",
    "plt.xticks(range(1, len(labels) + 1), labels)\n",
    "plt.show()\n",
    "\n",
    "writer.add_figure('Anomaly Detection', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_true = []\n",
    "lab_pred = []\n",
    "for k,v in firings_mnist.items():\n",
    "    for p in v:\n",
    "        lab_true.append(1)\n",
    "        lab_pred.append(p)\n",
    "for k,v in firings_emnist.items():\n",
    "    if k == 'Oo':\n",
    "        continue\n",
    "    for p in v:\n",
    "        lab_true.append(0)\n",
    "        lab_pred.append(p)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(lab_true, lab_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "fig = plt.figure(figsize =(4, 4))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "writer.add_figure('ROC', fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
