@article{DECAMPOSSOUZA2020106275,
title = {Fuzzy neural networks and neuro-fuzzy networks: A review the main techniques and applications used in the literature},
journal = {Applied Soft Computing},
volume = {92},
pages = {106275},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106275},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620302155},
author = {Paulo Vitor {de Campos Souza}},
keywords = {Fuzzy neural network, Neuro-fuzzy network, Hybrid models},
abstract = {This paper presents a review of the central theories involved in hybrid models based on fuzzy systems and artificial neural networks, mainly focused on supervised methods for training hybrid models. The basic concepts regarding the history of hybrid models, from the first proposed model to the current advances, the composition and the functionalities in their architecture, the data treatment and the training methods of these intelligent models are presented to the reader so that the evolution of this category of intelligent systems can be evidenced. Finally, the features of the leading models and their applications are presented to the reader. We conclude that the fuzzy neural network models and their derivations are efficient in constructing a system with a high degree of accuracy and an appropriate level of interpretability working in a wide range of areas of economics and science.}
}

@misc{kingma2022autoencoding,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2022},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      doi={https://doi.org/10.48550/arXiv.1312.6114}
}

@article{Kingma_2019,
   title={An Introduction to Variational Autoencoders},
   volume={12},
   ISSN={1935-8245},
   url={http://dx.doi.org/10.1561/2200000056},
   DOI={10.1561/2200000056},
   number={4},
   journal={Foundations and Trends® in Machine Learning},
   publisher={Now Publishers},
   author={Kingma, Diederik P. and Welling, Max},
   year={2019},
   pages={307–392}
   }

@misc{debbagh2023learning,
      title={Learning Structured Output Representations from Attributes using Deep Conditional Generative Models}, 
      author={Mohamed Debbagh},
      year={2023},
      eprint={2305.00980},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      doi={https://doi.org/10.48550/arXiv.2305.00980}
}


@INPROCEEDINGS{Bolat2020,
  author={Bölat, Kutay and Kumbasar, Tufan},
  booktitle={2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
  title={Interpreting Variational Autoencoders with Fuzzy Logic: A step towards interpretable deep learning based fuzzy classifiers}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
  abstract={The emerging success of Deep Learning (DL) in various application areas comes also with the questions starting with "How"s and "Why"s. These questions can be answered if the DL methods are interpretable and thus provide a certain a degree of explanation. In this paper, we propose a DL framework that leverages the advantages of β-Variational Autoencoder (VAE) and Fuzzy Sets (FSs), which are disentanglement and linguistic representation, for the design of a novel DL based Fuzzy Classifier (FC). We first present a step-by-step design approach to construct the DL-FC which is composed of the encoder layer of β-VAE and a Fuzzy Logic System (FLS) followed by a softmax layer. The β-VAE is trained so that the semantic information of the high dimensional data is captured. The latent space of the β-VAE is clustered to extract FSs. The FSs are then used to define antecedents of the FLS that is trained with DL methods. We present results conducted on the MNIST dataset and showed that DL-FC is quite competitive with its deep neural network counterpart. We then try to provide an interpretation to the antecedents of FLS by examining the FSs, the latent traversals and heat-maps of each latent dimension. The results show that the antecedents of FLS can be defined with linguistic interpretations. Thus, for the first time in the literature, we showed that linguistic interpretations can be defined for the latent space of β-VAE with FSs.},
  keywords={Frequency selective surfaces;Fuzzy logic;Training;Linguistics;Feature extraction;Machine learning;Clustering algorithms;Variational autoencoder;fuzzy sets;fuzzy cmeans clustering;classification;interpretation},
  doi={10.1109/FUZZ48607.2020.9177631},
  ISSN={1558-4739},
  month={July},}

@misc{burgess2018understanding,
      title={Understanding disentangling in $\beta$-VAE}, 
      author={Christopher P. Burgess and Irina Higgins and Arka Pal and Loic Matthey and Nick Watters and Guillaume Desjardins and Alexander Lerchner},
      year={2018},
      eprint={1804.03599},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      doi={10.48550/arXiv.1804.03599}
}

@article{RAMCHANDRAN2024110113,
title = {Learning conditional variational autoencoders with missing covariates},
journal = {Pattern Recognition},
volume = {147},
pages = {110113},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.110113},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323008105},
author = {Siddharth Ramchandran and Gleb Tikhonov and Otto Lönnroth and Pekka Tiikkainen and Harri Lähdesmäki},
keywords = {Variational autoencoders, Gaussian process, Conditional VAEs, Missing value imputation},
abstract = {Conditional variational autoencoders (CVAEs) are versatile deep latent variable models that extend the standard VAE framework by conditioning the generative model with auxiliary covariates. The original CVAE model assumes that the data samples are independent, whereas more recent conditional VAE models, such as the Gaussian process (GP) prior VAEs, can account for complex correlation structures across all data samples. While several methods have been proposed to learn standard VAEs from partially observed datasets, these methods fall short for conditional VAEs. In this work, we propose a method to learn conditional VAEs from datasets in which auxiliary covariates can contain missing values as well. The proposed method augments the conditional VAEs with a prior distribution for the missing covariates and estimates their posterior using amortised variational inference. At training time, our method accounts for the uncertainty associated with the missing covariates while simultaneously maximising the evidence lower bound. We develop computationally efficient methods to learn CVAEs and GP prior VAEs that are compatible with mini-batching. Our experiments on simulated datasets as well as on real-world biomedical datasets show that the proposed method outperforms previous methods in learning conditional VAEs from non-temporal, temporal, and longitudinal datasets.}
}

@ARTICLE{Jang633847,
  author={Jang, J.S.R. and Sun, C.T. and Mizutani, E.},
  journal={IEEE Transactions on Automatic Control}, 
  title={Neuro-Fuzzy and Soft Computing-A Computational Approach to Learning and Machine Intelligence [Book Review]}, 
  year={1997},
  volume={42},
  number={10},
  pages={1482-1484},
  keywords={Machine learning;Machine intelligence;Books;Computational intelligence;Neural networks;Fuzzy logic;Contracts;Optimal control;Stochastic processes;Learning systems},
  doi={10.1109/TAC.1997.633847}}

@misc{ramchandran2021longitudinal,
      title={Longitudinal Variational Autoencoder}, 
      author={Siddharth Ramchandran and Gleb Tikhonov and Kalle Kujanpää and Miika Koskinen and Harri Lähdesmäki},
      year={2021},
      eprint={2006.09763},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}